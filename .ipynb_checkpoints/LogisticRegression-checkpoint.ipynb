{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55035abd-0f92-4b00-90a5-90048fe5149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import skorch\n",
    "device = \"cpu\"#\"mps\" if torch.has_mps else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b18eb46-9cb5-4f79-bd67-59faf8336e78",
   "metadata": {},
   "source": [
    "### Logistic Regression PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d5cd46b-8998-4508-91e7-d08c09b4d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "def train_loop(dataloader, model, loss_fn, optimizer, print_nth_batch=4):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(\"yay training\")\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # print(y)\n",
    "        # Compute prediction and loss\n",
    "        pred = torch.squeeze(model(X))\n",
    "        # print(pred)\n",
    "        # regularization, computing largest singular value\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % print_nth_batch == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    # we don't want to track gradients here because we're just doing\n",
    "    # a forward pass to evaluate predictions\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = torch.squeeze(model(X))\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # round predicted probs to get label prediction, compute n correct\n",
    "            correct += (pred.round() == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "def test_auc_score(dataset, model):\n",
    "    X, y = dataset[:]\n",
    "    pred = model(X)\n",
    "    pred, y = pred.detach().numpy(), y.detach().numpy()\n",
    "    score = roc_auc_score(y_true=y, y_score=pred)\n",
    "    print(\"AUC:\", score)\n",
    "    return score\n",
    "    \n",
    "    \n",
    "    \n",
    "class LogisticRegressionTorch(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, output_dim, bias=True),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # logits = torch.sigmoid(self.linear(x))\n",
    "        probs = self.linear(x)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88a7ed46-6f7e-494b-9f74-1571075c8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "X_1 = torch.rand(1000, 128) + .2\n",
    "y_1 = torch.zeros(1000,)\n",
    "X_2 = torch.rand(1000, 128) - .1\n",
    "y_2 = torch.ones(1000,)\n",
    "\n",
    "X = torch.cat((X_1, X_2))\n",
    "y = torch.cat((y_1, y_2))\n",
    "\n",
    "dataset = SimpleDataset(X, y)\n",
    "\n",
    "train_prop = .8\n",
    "train_num = int(train_prop * len(dataset))\n",
    "test_num = len(X) - train_num\n",
    "\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset,\n",
    "                                                    [train_num, test_num])\n",
    "\n",
    "\n",
    "target = train_set.dataset.y[train_set.indices]\n",
    "cls_weights = torch.from_numpy(\n",
    "    compute_class_weight(class_weight='balanced',\n",
    "                         classes=np.unique(target.numpy()),\n",
    "                         y=target.numpy())\n",
    ")\n",
    "weights = cls_weights[target.numpy()]\n",
    "sampler = WeightedRandomSampler(weights, len(target.numpy()), replacement=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=100, sampler=sampler)\n",
    "test_dataloader = DataLoader(test_set, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "29b0b7b5-d1b1-44fd-b644-842d55fdd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionTorch(128)\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "weight_decay = 0\n",
    "lr = 5e-1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr,weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d62a1cb0-0e75-4aed-8389-5f90c13bb18c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.448199  [    0/12672]\n",
      "loss: 19.501102  [  400/12672]\n",
      "loss: 11.889750  [  800/12672]\n",
      "loss: 10.050867  [ 1200/12672]\n",
      "loss: 4.415318  [ 1600/12672]\n",
      "loss: 20.606892  [ 2000/12672]\n",
      "loss: 23.693605  [ 2400/12672]\n",
      "loss: 6.622545  [ 2800/12672]\n",
      "loss: 22.063299  [ 3200/12672]\n",
      "loss: 9.026217  [ 3600/12672]\n",
      "loss: 12.450096  [ 4000/12672]\n",
      "loss: 13.314853  [ 4400/12672]\n",
      "loss: 10.686407  [ 4800/12672]\n",
      "loss: 17.673269  [ 5200/12672]\n",
      "loss: 13.940762  [ 5600/12672]\n",
      "loss: 16.124735  [ 6000/12672]\n",
      "loss: 14.201858  [ 6400/12672]\n",
      "loss: 16.571611  [ 6800/12672]\n",
      "loss: 10.381608  [ 7200/12672]\n",
      "loss: 7.627161  [ 7600/12672]\n",
      "loss: 9.125181  [ 8000/12672]\n",
      "loss: 8.847769  [ 8400/12672]\n",
      "loss: 13.405208  [ 8800/12672]\n",
      "loss: 19.638165  [ 9200/12672]\n",
      "loss: 14.617030  [ 9600/12672]\n",
      "loss: 7.883918  [10000/12672]\n",
      "loss: 15.535515  [10400/12672]\n",
      "loss: 13.479076  [10800/12672]\n",
      "loss: 19.351795  [11200/12672]\n",
      "loss: 16.983923  [11600/12672]\n",
      "loss: 5.988626  [12000/12672]\n",
      "loss: 7.569260  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 18.016468 \n",
      "\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.649567  [    0/12672]\n",
      "loss: 13.509387  [  400/12672]\n",
      "loss: 9.397038  [  800/12672]\n",
      "loss: 16.511402  [ 1200/12672]\n",
      "loss: 13.522353  [ 1600/12672]\n",
      "loss: 10.554843  [ 2000/12672]\n",
      "loss: 20.753071  [ 2400/12672]\n",
      "loss: 8.883151  [ 2800/12672]\n",
      "loss: 20.038437  [ 3200/12672]\n",
      "loss: 11.116176  [ 3600/12672]\n",
      "loss: 13.765134  [ 4000/12672]\n",
      "loss: 25.880190  [ 4400/12672]\n",
      "loss: 11.455889  [ 4800/12672]\n",
      "loss: 31.708776  [ 5200/12672]\n",
      "loss: 13.507155  [ 5600/12672]\n",
      "loss: 12.163624  [ 6000/12672]\n",
      "loss: 11.731881  [ 6400/12672]\n",
      "loss: 17.309862  [ 6800/12672]\n",
      "loss: 17.024408  [ 7200/12672]\n",
      "loss: 12.313538  [ 7600/12672]\n",
      "loss: 22.971514  [ 8000/12672]\n",
      "loss: 24.267168  [ 8400/12672]\n",
      "loss: 17.090376  [ 8800/12672]\n",
      "loss: 11.761583  [ 9200/12672]\n",
      "loss: 12.240818  [ 9600/12672]\n",
      "loss: 14.493343  [10000/12672]\n",
      "loss: 7.649945  [10400/12672]\n",
      "loss: 12.533072  [10800/12672]\n",
      "loss: 14.956245  [11200/12672]\n",
      "loss: 10.299414  [11600/12672]\n",
      "loss: 19.823963  [12000/12672]\n",
      "loss: 7.478351  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 16.509207 \n",
      "\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.266824  [    0/12672]\n",
      "loss: 10.595530  [  400/12672]\n",
      "loss: 12.792263  [  800/12672]\n",
      "loss: 12.264021  [ 1200/12672]\n",
      "loss: 16.692595  [ 1600/12672]\n",
      "loss: 14.937693  [ 2000/12672]\n",
      "loss: 14.701788  [ 2400/12672]\n",
      "loss: 9.137777  [ 2800/12672]\n",
      "loss: 11.993700  [ 3200/12672]\n",
      "loss: 11.910131  [ 3600/12672]\n",
      "loss: 18.288343  [ 4000/12672]\n",
      "loss: 12.494102  [ 4400/12672]\n",
      "loss: 20.259302  [ 4800/12672]\n",
      "loss: 14.861511  [ 5200/12672]\n",
      "loss: 17.148876  [ 5600/12672]\n",
      "loss: 11.321703  [ 6000/12672]\n",
      "loss: 15.139669  [ 6400/12672]\n",
      "loss: 6.948258  [ 6800/12672]\n",
      "loss: 10.369998  [ 7200/12672]\n",
      "loss: 14.284306  [ 7600/12672]\n",
      "loss: 18.567276  [ 8000/12672]\n",
      "loss: 12.247922  [ 8400/12672]\n",
      "loss: 12.163288  [ 8800/12672]\n",
      "loss: 8.944297  [ 9200/12672]\n",
      "loss: 17.060509  [ 9600/12672]\n",
      "loss: 11.167281  [10000/12672]\n",
      "loss: 8.378854  [10400/12672]\n",
      "loss: 15.077158  [10800/12672]\n",
      "loss: 12.965886  [11200/12672]\n",
      "loss: 10.021579  [11600/12672]\n",
      "loss: 18.610182  [12000/12672]\n",
      "loss: 16.985973  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 12.171034 \n",
      "\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.183802  [    0/12672]\n",
      "loss: 15.160548  [  400/12672]\n",
      "loss: 10.933621  [  800/12672]\n",
      "loss: 12.116112  [ 1200/12672]\n",
      "loss: 14.420932  [ 1600/12672]\n",
      "loss: 14.012866  [ 2000/12672]\n",
      "loss: 18.241829  [ 2400/12672]\n",
      "loss: 10.419930  [ 2800/12672]\n",
      "loss: 12.127881  [ 3200/12672]\n",
      "loss: 14.237103  [ 3600/12672]\n",
      "loss: 12.597299  [ 4000/12672]\n",
      "loss: 15.388819  [ 4400/12672]\n",
      "loss: 13.214099  [ 4800/12672]\n",
      "loss: 11.429836  [ 5200/12672]\n",
      "loss: 11.230478  [ 5600/12672]\n",
      "loss: 12.669957  [ 6000/12672]\n",
      "loss: 13.378399  [ 6400/12672]\n",
      "loss: 6.739186  [ 6800/12672]\n",
      "loss: 9.320815  [ 7200/12672]\n",
      "loss: 12.317379  [ 7600/12672]\n",
      "loss: 11.085850  [ 8000/12672]\n",
      "loss: 8.723577  [ 8400/12672]\n",
      "loss: 15.511212  [ 8800/12672]\n",
      "loss: 7.381125  [ 9200/12672]\n",
      "loss: 16.983154  [ 9600/12672]\n",
      "loss: 16.036646  [10000/12672]\n",
      "loss: 14.431424  [10400/12672]\n",
      "loss: 15.025547  [10800/12672]\n",
      "loss: 24.832644  [11200/12672]\n",
      "loss: 18.524570  [11600/12672]\n",
      "loss: 15.385938  [12000/12672]\n",
      "loss: 9.860323  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 12.374711 \n",
      "\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.690251  [    0/12672]\n",
      "loss: 11.427903  [  400/12672]\n",
      "loss: 15.970511  [  800/12672]\n",
      "loss: 17.453661  [ 1200/12672]\n",
      "loss: 10.413849  [ 1600/12672]\n",
      "loss: 20.263115  [ 2000/12672]\n",
      "loss: 17.261705  [ 2400/12672]\n",
      "loss: 19.123299  [ 2800/12672]\n",
      "loss: 8.567987  [ 3200/12672]\n",
      "loss: 18.131485  [ 3600/12672]\n",
      "loss: 16.139654  [ 4000/12672]\n",
      "loss: 11.765730  [ 4400/12672]\n",
      "loss: 8.238675  [ 4800/12672]\n",
      "loss: 14.039829  [ 5200/12672]\n",
      "loss: 14.159582  [ 5600/12672]\n",
      "loss: 7.805453  [ 6000/12672]\n",
      "loss: 7.981040  [ 6400/12672]\n",
      "loss: 14.287803  [ 6800/12672]\n",
      "loss: 13.123188  [ 7200/12672]\n",
      "loss: 12.064352  [ 7600/12672]\n",
      "loss: 16.017061  [ 8000/12672]\n",
      "loss: 22.324369  [ 8400/12672]\n",
      "loss: 9.934460  [ 8800/12672]\n",
      "loss: 4.155950  [ 9200/12672]\n",
      "loss: 19.120373  [ 9600/12672]\n",
      "loss: 12.057743  [10000/12672]\n",
      "loss: 10.811903  [10400/12672]\n",
      "loss: 7.482214  [10800/12672]\n",
      "loss: 19.884293  [11200/12672]\n",
      "loss: 15.217796  [11600/12672]\n",
      "loss: 16.182228  [12000/12672]\n",
      "loss: 9.579833  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 23.999401 \n",
      "\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.469971  [    0/12672]\n",
      "loss: 8.185697  [  400/12672]\n",
      "loss: 10.853790  [  800/12672]\n",
      "loss: 12.540036  [ 1200/12672]\n",
      "loss: 15.900827  [ 1600/12672]\n",
      "loss: 17.563251  [ 2000/12672]\n",
      "loss: 12.235682  [ 2400/12672]\n",
      "loss: 16.347824  [ 2800/12672]\n",
      "loss: 7.042830  [ 3200/12672]\n",
      "loss: 8.950843  [ 3600/12672]\n",
      "loss: 13.864532  [ 4000/12672]\n",
      "loss: 14.991700  [ 4400/12672]\n",
      "loss: 10.055911  [ 4800/12672]\n",
      "loss: 17.945389  [ 5200/12672]\n",
      "loss: 13.379511  [ 5600/12672]\n",
      "loss: 13.228340  [ 6000/12672]\n",
      "loss: 14.096674  [ 6400/12672]\n",
      "loss: 15.256887  [ 6800/12672]\n",
      "loss: 12.325086  [ 7200/12672]\n",
      "loss: 16.655842  [ 7600/12672]\n",
      "loss: 6.907852  [ 8000/12672]\n",
      "loss: 10.289501  [ 8400/12672]\n",
      "loss: 15.458017  [ 8800/12672]\n",
      "loss: 13.156487  [ 9200/12672]\n",
      "loss: 9.132521  [ 9600/12672]\n",
      "loss: 18.210533  [10000/12672]\n",
      "loss: 10.921343  [10400/12672]\n",
      "loss: 16.609474  [10800/12672]\n",
      "loss: 25.303988  [11200/12672]\n",
      "loss: 9.111837  [11600/12672]\n",
      "loss: 12.410911  [12000/12672]\n",
      "loss: 17.120338  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 11.639173 \n",
      "\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.995352  [    0/12672]\n",
      "loss: 20.579924  [  400/12672]\n",
      "loss: 12.873269  [  800/12672]\n",
      "loss: 12.389066  [ 1200/12672]\n",
      "loss: 17.971207  [ 1600/12672]\n",
      "loss: 12.104944  [ 2000/12672]\n",
      "loss: 12.234894  [ 2400/12672]\n",
      "loss: 13.213955  [ 2800/12672]\n",
      "loss: 31.964170  [ 3200/12672]\n",
      "loss: 18.850887  [ 3600/12672]\n",
      "loss: 9.032213  [ 4000/12672]\n",
      "loss: 12.687379  [ 4400/12672]\n",
      "loss: 11.031095  [ 4800/12672]\n",
      "loss: 13.497263  [ 5200/12672]\n",
      "loss: 13.893620  [ 5600/12672]\n",
      "loss: 13.919647  [ 6000/12672]\n",
      "loss: 19.508148  [ 6400/12672]\n",
      "loss: 24.821772  [ 6800/12672]\n",
      "loss: 22.795626  [ 7200/12672]\n",
      "loss: 10.880245  [ 7600/12672]\n",
      "loss: 9.302649  [ 8000/12672]\n",
      "loss: 19.752058  [ 8400/12672]\n",
      "loss: 13.427460  [ 8800/12672]\n",
      "loss: 12.387313  [ 9200/12672]\n",
      "loss: 13.814828  [ 9600/12672]\n",
      "loss: 14.291482  [10000/12672]\n",
      "loss: 17.464123  [10400/12672]\n",
      "loss: 12.016816  [10800/12672]\n",
      "loss: 21.615026  [11200/12672]\n",
      "loss: 10.352825  [11600/12672]\n",
      "loss: 8.616590  [12000/12672]\n",
      "loss: 16.116655  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 12.515601 \n",
      "\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.303319  [    0/12672]\n",
      "loss: 8.861413  [  400/12672]\n",
      "loss: 12.084609  [  800/12672]\n",
      "loss: 14.408398  [ 1200/12672]\n",
      "loss: 26.308914  [ 1600/12672]\n",
      "loss: 9.421227  [ 2000/12672]\n",
      "loss: 14.875638  [ 2400/12672]\n",
      "loss: 15.288671  [ 2800/12672]\n",
      "loss: 16.106947  [ 3200/12672]\n",
      "loss: 12.155720  [ 3600/12672]\n",
      "loss: 10.166431  [ 4000/12672]\n",
      "loss: 6.269804  [ 4400/12672]\n",
      "loss: 9.990419  [ 4800/12672]\n",
      "loss: 11.910367  [ 5200/12672]\n",
      "loss: 23.577917  [ 5600/12672]\n",
      "loss: 20.741671  [ 6000/12672]\n",
      "loss: 15.915788  [ 6400/12672]\n",
      "loss: 17.503172  [ 6800/12672]\n",
      "loss: 12.012377  [ 7200/12672]\n",
      "loss: 14.923263  [ 7600/12672]\n",
      "loss: 18.974543  [ 8000/12672]\n",
      "loss: 15.195709  [ 8400/12672]\n",
      "loss: 10.174289  [ 8800/12672]\n",
      "loss: 6.401777  [ 9200/12672]\n",
      "loss: 21.268303  [ 9600/12672]\n",
      "loss: 16.403627  [10000/12672]\n",
      "loss: 16.186075  [10400/12672]\n",
      "loss: 11.771916  [10800/12672]\n",
      "loss: 8.495223  [11200/12672]\n",
      "loss: 13.724661  [11600/12672]\n",
      "loss: 16.110214  [12000/12672]\n",
      "loss: 15.796165  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 17.372392 \n",
      "\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.784058  [    0/12672]\n",
      "loss: 15.786152  [  400/12672]\n",
      "loss: 16.158751  [  800/12672]\n",
      "loss: 19.234642  [ 1200/12672]\n",
      "loss: 17.091730  [ 1600/12672]\n",
      "loss: 13.708818  [ 2000/12672]\n",
      "loss: 6.746881  [ 2400/12672]\n",
      "loss: 8.083761  [ 2800/12672]\n",
      "loss: 12.905032  [ 3200/12672]\n",
      "loss: 16.447067  [ 3600/12672]\n",
      "loss: 17.348621  [ 4000/12672]\n",
      "loss: 20.821470  [ 4400/12672]\n",
      "loss: 8.115767  [ 4800/12672]\n",
      "loss: 10.315418  [ 5200/12672]\n",
      "loss: 15.641731  [ 5600/12672]\n",
      "loss: 10.003778  [ 6000/12672]\n",
      "loss: 17.689465  [ 6400/12672]\n",
      "loss: 11.771424  [ 6800/12672]\n",
      "loss: 9.236159  [ 7200/12672]\n",
      "loss: 13.616966  [ 7600/12672]\n",
      "loss: 13.914340  [ 8000/12672]\n",
      "loss: 24.049797  [ 8400/12672]\n",
      "loss: 12.333526  [ 8800/12672]\n",
      "loss: 9.009104  [ 9200/12672]\n",
      "loss: 9.193885  [ 9600/12672]\n",
      "loss: 12.357063  [10000/12672]\n",
      "loss: 8.423096  [10400/12672]\n",
      "loss: 22.022852  [10800/12672]\n",
      "loss: 13.279924  [11200/12672]\n",
      "loss: 18.116661  [11600/12672]\n",
      "loss: 12.920464  [12000/12672]\n",
      "loss: 10.147761  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 13.401397 \n",
      "\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.949485  [    0/12672]\n",
      "loss: 8.294992  [  400/12672]\n",
      "loss: 17.950354  [  800/12672]\n",
      "loss: 7.952792  [ 1200/12672]\n",
      "loss: 21.010412  [ 1600/12672]\n",
      "loss: 12.966318  [ 2000/12672]\n",
      "loss: 14.722090  [ 2400/12672]\n",
      "loss: 13.706103  [ 2800/12672]\n",
      "loss: 13.002720  [ 3200/12672]\n",
      "loss: 20.948454  [ 3600/12672]\n",
      "loss: 12.959817  [ 4000/12672]\n",
      "loss: 12.776285  [ 4400/12672]\n",
      "loss: 9.594580  [ 4800/12672]\n",
      "loss: 6.740043  [ 5200/12672]\n",
      "loss: 10.858948  [ 5600/12672]\n",
      "loss: 12.081985  [ 6000/12672]\n",
      "loss: 12.982862  [ 6400/12672]\n",
      "loss: 11.654093  [ 6800/12672]\n",
      "loss: 16.540211  [ 7200/12672]\n",
      "loss: 8.533106  [ 7600/12672]\n",
      "loss: 18.562454  [ 8000/12672]\n",
      "loss: 19.952366  [ 8400/12672]\n",
      "loss: 13.183201  [ 8800/12672]\n",
      "loss: 13.489519  [ 9200/12672]\n",
      "loss: 15.693146  [ 9600/12672]\n",
      "loss: 14.538270  [10000/12672]\n",
      "loss: 18.448578  [10400/12672]\n",
      "loss: 9.776441  [10800/12672]\n",
      "loss: 14.023926  [11200/12672]\n",
      "loss: 6.790817  [11600/12672]\n",
      "loss: 5.435196  [12000/12672]\n",
      "loss: 11.927088  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 9.422605 \n",
      "\n",
      "------------------------------\n",
      "Epoch 11\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.481699  [    0/12672]\n",
      "loss: 17.628441  [  400/12672]\n",
      "loss: 15.440799  [  800/12672]\n",
      "loss: 13.526672  [ 1200/12672]\n",
      "loss: 8.630747  [ 1600/12672]\n",
      "loss: 17.374836  [ 2000/12672]\n",
      "loss: 20.244081  [ 2400/12672]\n",
      "loss: 8.898341  [ 2800/12672]\n",
      "loss: 9.706555  [ 3200/12672]\n",
      "loss: 12.080648  [ 3600/12672]\n",
      "loss: 6.360147  [ 4000/12672]\n",
      "loss: 16.686150  [ 4400/12672]\n",
      "loss: 16.738291  [ 4800/12672]\n",
      "loss: 9.030946  [ 5200/12672]\n",
      "loss: 14.754167  [ 5600/12672]\n",
      "loss: 14.072790  [ 6000/12672]\n",
      "loss: 15.118283  [ 6400/12672]\n",
      "loss: 21.216120  [ 6800/12672]\n",
      "loss: 17.123138  [ 7200/12672]\n",
      "loss: 9.326149  [ 7600/12672]\n",
      "loss: 23.808596  [ 8000/12672]\n",
      "loss: 12.617850  [ 8400/12672]\n",
      "loss: 15.786592  [ 8800/12672]\n",
      "loss: 13.495055  [ 9200/12672]\n",
      "loss: 11.627166  [ 9600/12672]\n",
      "loss: 13.130214  [10000/12672]\n",
      "loss: 13.557033  [10400/12672]\n",
      "loss: 28.438143  [10800/12672]\n",
      "loss: 12.197895  [11200/12672]\n",
      "loss: 8.645572  [11600/12672]\n",
      "loss: 17.772137  [12000/12672]\n",
      "loss: 9.375681  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 14.795683 \n",
      "\n",
      "------------------------------\n",
      "Epoch 12\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.015186  [    0/12672]\n",
      "loss: 18.378218  [  400/12672]\n",
      "loss: 10.905426  [  800/12672]\n",
      "loss: 14.407141  [ 1200/12672]\n",
      "loss: 17.095621  [ 1600/12672]\n",
      "loss: 24.367382  [ 2000/12672]\n",
      "loss: 13.952176  [ 2400/12672]\n",
      "loss: 14.862652  [ 2800/12672]\n",
      "loss: 14.088938  [ 3200/12672]\n",
      "loss: 8.695137  [ 3600/12672]\n",
      "loss: 17.837742  [ 4000/12672]\n",
      "loss: 10.056675  [ 4400/12672]\n",
      "loss: 15.008612  [ 4800/12672]\n",
      "loss: 13.683960  [ 5200/12672]\n",
      "loss: 8.771029  [ 5600/12672]\n",
      "loss: 24.330189  [ 6000/12672]\n",
      "loss: 10.008524  [ 6400/12672]\n",
      "loss: 13.807898  [ 6800/12672]\n",
      "loss: 9.575724  [ 7200/12672]\n",
      "loss: 6.328605  [ 7600/12672]\n",
      "loss: 8.506556  [ 8000/12672]\n",
      "loss: 13.222223  [ 8400/12672]\n",
      "loss: 11.756744  [ 8800/12672]\n",
      "loss: 19.167419  [ 9200/12672]\n",
      "loss: 14.429133  [ 9600/12672]\n",
      "loss: 13.435739  [10000/12672]\n",
      "loss: 11.294412  [10400/12672]\n",
      "loss: 13.987287  [10800/12672]\n",
      "loss: 7.798900  [11200/12672]\n",
      "loss: 16.432383  [11600/12672]\n",
      "loss: 14.301639  [12000/12672]\n",
      "loss: 17.070663  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 24.183126 \n",
      "\n",
      "------------------------------\n",
      "Epoch 13\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 24.290478  [    0/12672]\n",
      "loss: 12.562439  [  400/12672]\n",
      "loss: 20.123627  [  800/12672]\n",
      "loss: 16.138966  [ 1200/12672]\n",
      "loss: 12.933722  [ 1600/12672]\n",
      "loss: 14.098883  [ 2000/12672]\n",
      "loss: 14.329038  [ 2400/12672]\n",
      "loss: 19.382090  [ 2800/12672]\n",
      "loss: 14.456590  [ 3200/12672]\n",
      "loss: 9.724588  [ 3600/12672]\n",
      "loss: 21.644138  [ 4000/12672]\n",
      "loss: 5.628462  [ 4400/12672]\n",
      "loss: 13.378937  [ 4800/12672]\n",
      "loss: 11.573664  [ 5200/12672]\n",
      "loss: 20.805998  [ 5600/12672]\n",
      "loss: 12.693164  [ 6000/12672]\n",
      "loss: 10.590127  [ 6400/12672]\n",
      "loss: 11.371552  [ 6800/12672]\n",
      "loss: 9.033211  [ 7200/12672]\n",
      "loss: 10.337649  [ 7600/12672]\n",
      "loss: 16.199575  [ 8000/12672]\n",
      "loss: 9.331702  [ 8400/12672]\n",
      "loss: 6.877408  [ 8800/12672]\n",
      "loss: 11.971825  [ 9200/12672]\n",
      "loss: 6.992371  [ 9600/12672]\n",
      "loss: 10.481929  [10000/12672]\n",
      "loss: 16.138960  [10400/12672]\n",
      "loss: 12.549189  [10800/12672]\n",
      "loss: 11.699098  [11200/12672]\n",
      "loss: 6.820444  [11600/12672]\n",
      "loss: 10.163265  [12000/12672]\n",
      "loss: 7.893771  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 14.835085 \n",
      "\n",
      "------------------------------\n",
      "Epoch 14\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.221777  [    0/12672]\n",
      "loss: 18.183083  [  400/12672]\n",
      "loss: 12.206839  [  800/12672]\n",
      "loss: 8.189983  [ 1200/12672]\n",
      "loss: 7.791025  [ 1600/12672]\n",
      "loss: 15.121408  [ 2000/12672]\n",
      "loss: 10.265409  [ 2400/12672]\n",
      "loss: 11.727020  [ 2800/12672]\n",
      "loss: 10.553771  [ 3200/12672]\n",
      "loss: 11.364791  [ 3600/12672]\n",
      "loss: 18.377714  [ 4000/12672]\n",
      "loss: 12.504489  [ 4400/12672]\n",
      "loss: 11.490378  [ 4800/12672]\n",
      "loss: 13.047885  [ 5200/12672]\n",
      "loss: 12.386039  [ 5600/12672]\n",
      "loss: 12.195374  [ 6000/12672]\n",
      "loss: 15.164351  [ 6400/12672]\n",
      "loss: 20.665071  [ 6800/12672]\n",
      "loss: 17.923084  [ 7200/12672]\n",
      "loss: 14.874378  [ 7600/12672]\n",
      "loss: 18.247149  [ 8000/12672]\n",
      "loss: 9.630839  [ 8400/12672]\n",
      "loss: 15.790625  [ 8800/12672]\n",
      "loss: 10.902757  [ 9200/12672]\n",
      "loss: 14.080828  [ 9600/12672]\n",
      "loss: 8.631372  [10000/12672]\n",
      "loss: 9.469222  [10400/12672]\n",
      "loss: 11.847634  [10800/12672]\n",
      "loss: 11.521795  [11200/12672]\n",
      "loss: 9.161792  [11600/12672]\n",
      "loss: 12.765641  [12000/12672]\n",
      "loss: 19.950039  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 11.334845 \n",
      "\n",
      "------------------------------\n",
      "Epoch 15\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.034005  [    0/12672]\n",
      "loss: 24.054441  [  400/12672]\n",
      "loss: 18.953207  [  800/12672]\n",
      "loss: 17.820242  [ 1200/12672]\n",
      "loss: 15.984304  [ 1600/12672]\n",
      "loss: 9.668208  [ 2000/12672]\n",
      "loss: 18.507774  [ 2400/12672]\n",
      "loss: 14.505383  [ 2800/12672]\n",
      "loss: 13.899640  [ 3200/12672]\n",
      "loss: 15.674949  [ 3600/12672]\n",
      "loss: 14.519579  [ 4000/12672]\n",
      "loss: 9.527836  [ 4400/12672]\n",
      "loss: 9.200832  [ 4800/12672]\n",
      "loss: 11.880459  [ 5200/12672]\n",
      "loss: 13.229123  [ 5600/12672]\n",
      "loss: 15.025188  [ 6000/12672]\n",
      "loss: 14.049607  [ 6400/12672]\n",
      "loss: 12.471589  [ 6800/12672]\n",
      "loss: 11.801819  [ 7200/12672]\n",
      "loss: 7.602612  [ 7600/12672]\n",
      "loss: 7.913931  [ 8000/12672]\n",
      "loss: 7.411782  [ 8400/12672]\n",
      "loss: 20.745857  [ 8800/12672]\n",
      "loss: 16.161396  [ 9200/12672]\n",
      "loss: 10.264732  [ 9600/12672]\n",
      "loss: 10.733447  [10000/12672]\n",
      "loss: 10.308656  [10400/12672]\n",
      "loss: 9.667546  [10800/12672]\n",
      "loss: 16.173214  [11200/12672]\n",
      "loss: 6.567777  [11600/12672]\n",
      "loss: 17.029745  [12000/12672]\n",
      "loss: 16.983877  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 12.249934 \n",
      "\n",
      "Done!\n",
      "CPU times: user 1.88 s, sys: 182 ms, total: 2.06 s\n",
      "Wall time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 15\n",
    "for t in range(epochs):\n",
    "    print(f\"{'-'*30}\\nEpoch {t+1}\\n{'-'*30}\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(train_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "fa642a60-58bb-4586-846f-ab3748391f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2556009 , -0.11014569, -0.34770104, -0.05363564, -0.32045805,\n",
       "        -0.20047373,  0.04354731, -0.07021297, -0.2549383 , -0.08638977,\n",
       "        -0.12272437, -0.26145813, -0.132995  , -0.5590581 , -0.13800876,\n",
       "        -0.39361808, -0.18007791, -0.14691398, -0.11113048,  0.02109414,\n",
       "        -0.02971033, -0.06245779, -0.23700692, -0.0312883 , -0.05845731,\n",
       "         0.14545608, -0.22800045,  0.13870333, -0.2055197 , -0.5122873 ,\n",
       "        -0.02531125,  0.07469609,  0.00397547, -0.01759478, -0.16063288,\n",
       "        -0.04735122, -0.12375901, -0.19227822, -0.05999327, -0.04843619,\n",
       "        -0.2967028 ,  0.00551004,  0.03163636, -0.06197273,  0.01000183,\n",
       "        -0.21084203, -0.10228419, -0.07310296,  0.11168385, -0.23657438,\n",
       "        -0.2252271 , -0.30507943, -0.02275092, -0.31863666, -0.14079633,\n",
       "        -0.06563782,  0.15927301, -0.18031234,  0.01695587, -0.09139688,\n",
       "        -0.18292908, -0.11702715, -0.5671819 ,  0.15823328, -0.11645681,\n",
       "         0.01410398, -0.08147889, -0.09058848, -0.22371317, -0.10227858,\n",
       "        -0.07714693, -0.1457704 , -0.22057438, -0.20044486,  0.01164881,\n",
       "        -0.33475474, -0.00665838, -0.0153681 , -0.18155982, -0.24766387,\n",
       "        -0.22887512, -0.16248038, -0.16140792, -0.24882337, -0.21806884,\n",
       "        -0.01768024, -0.05802217,  0.03899268, -0.1101112 , -0.02605398,\n",
       "         0.005957  , -0.12131958, -0.03348484, -0.08030755, -0.18201545,\n",
       "        -0.22871569,  0.05486714, -0.02430068, -0.13328029, -0.23076108,\n",
       "        -0.13126285, -0.0348118 , -0.25455052,  0.01738402, -0.08646844,\n",
       "        -0.13921174, -0.01897634, -0.18064915,  0.03457617, -0.2869255 ,\n",
       "         0.08858623, -0.23258016, -0.15405336, -0.05003629,  0.04085288,\n",
       "        -0.26723537,  0.03861602, -0.3729332 ,  0.02717902, -0.2995317 ,\n",
       "        -0.17331788, -0.04101282, -0.07718829, -0.08079364, -0.07807177,\n",
       "        -0.27141556, -0.29208753,  0.01224815]], dtype=float32)"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p for p in model.parameters()][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ce08d-e451-45d9-b3fc-a28c2bedf854",
   "metadata": {},
   "source": [
    "## Compare to sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23c67416-86b6-4434-a7e7-1e8c1a238572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "523260ef-dec1-468c-ab02-fe635822b437",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimpleDataset' object has no attribute 'indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SimpleDataset' object has no attribute 'indices'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sk_model = LogisticRegression(penalty='l2', C=1., class_weight='balanced', fit_intercept=True)\n",
    "\n",
    "train_idx = train_set.indices\n",
    "test_idx = test_set.indices\n",
    "sk_train_X = train_set.dataset.X[train_idx].detach().numpy()\n",
    "sk_train_y = train_set.dataset.y[train_idx].detach().numpy()\n",
    "sk_test_X = test_set.dataset.X[test_idx].detach().numpy()\n",
    "sk_test_y = test_set.dataset.y[test_idx].detach().numpy()\n",
    "fit = sk_model.fit(sk_train_X, sk_train_y)\n",
    "\n",
    "sk_pred = fit.predict(sk_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "5f6c6a99-81a3-4ea5-9786-93a96a6a24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "58151025-efbe-4cce-afff-9a5c7e397716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(sk_test_y, sk_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1afe66-561c-4162-bf56-5b951363d036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.37388629, -0.32047521, -0.4095046 , -0.32130418, -0.39439804,\n",
       "        -0.35823253, -0.33484078, -0.33933441, -0.37543385, -0.36355242,\n",
       "        -0.34292282, -0.37185879, -0.3706311 , -0.44209721, -0.36840507,\n",
       "        -0.34789623, -0.41266399, -0.36281521, -0.36685014, -0.3846179 ,\n",
       "        -0.31708437, -0.31090994, -0.37258387, -0.37406865, -0.36461087,\n",
       "        -0.28830832, -0.41198973, -0.3104652 , -0.35061659, -0.40379479,\n",
       "        -0.33055622, -0.33311162, -0.35796505, -0.35008685, -0.35916163,\n",
       "        -0.35937264, -0.39998844, -0.3649274 , -0.37894635, -0.35347019,\n",
       "        -0.37401855, -0.3663856 , -0.30936565, -0.37154138, -0.32723201,\n",
       "        -0.36783979, -0.39499198, -0.3303809 , -0.33748294, -0.36837342,\n",
       "        -0.39150569, -0.36852701, -0.3572353 , -0.37949994, -0.33189788,\n",
       "        -0.40627196, -0.35212517, -0.37479089, -0.33094589, -0.37000063,\n",
       "        -0.3751616 , -0.33788822, -0.37261937, -0.31014997, -0.34313105,\n",
       "        -0.35801088, -0.36816738, -0.34082888, -0.3879882 , -0.3457667 ,\n",
       "        -0.36235728, -0.32733371, -0.39650571, -0.39248756, -0.32821235,\n",
       "        -0.3990739 , -0.30894712, -0.34339028, -0.34547438, -0.35086806,\n",
       "        -0.40395907, -0.37280974, -0.31477501, -0.42060536, -0.38097354,\n",
       "        -0.3758629 , -0.35815151, -0.34744084, -0.32995486, -0.35151104,\n",
       "        -0.32959605, -0.37197039, -0.3986891 , -0.3753245 , -0.39787029,\n",
       "        -0.39702534, -0.35969944, -0.32618243, -0.37155474, -0.40618902,\n",
       "        -0.38696675, -0.34085002, -0.37607757, -0.38614888, -0.39346724,\n",
       "        -0.38347606, -0.33018366, -0.37635166, -0.3128696 , -0.39317084,\n",
       "        -0.31900011, -0.37878595, -0.37034107, -0.33262278, -0.28697747,\n",
       "        -0.34756017, -0.35172575, -0.38714172, -0.33827419, -0.41180401,\n",
       "        -0.31481693, -0.33421305, -0.31779403, -0.30730132, -0.3548753 ,\n",
       "        -0.42106957, -0.38216169, -0.32547378]])"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a23101-0bd3-4961-b112-e36be91814f4",
   "metadata": {},
   "source": [
    "## Using lab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c55e7286-4c5d-48d6-85ac-6bc3d166cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptsa.data.timeseries import TimeSeries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be80b2f1-34c7-458e-88a0-a1c8ae83f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeSeries.from_hdf(\n",
    "    \"/Users/jrudoler/rhino_mount/scratch/jrudoler/scalp_features/LTP093_feats.h5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3edface1-79d9-4147-87e1-79869a0c0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_npy = ts.data\n",
    "y_npy = ts.recalled.data\n",
    "X = torch.tensor(ts.data).float()\n",
    "y = torch.tensor(ts.recalled.data).float()\n",
    "dataset = SimpleDataset(X, y)\n",
    "sessions = ts.session.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e42c29f-5d38-4187-88cb-36414f76b390",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "SESSION 0\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.737422  [    0/12672]\n",
      "loss: 10.383088  [ 1600/12672]\n",
      "loss: 3.770948  [ 3200/12672]\n",
      "loss: 15.264870  [ 4800/12672]\n",
      "loss: 21.662603  [ 6400/12672]\n",
      "loss: 22.210598  [ 8000/12672]\n",
      "loss: 5.121501  [ 9600/12672]\n",
      "loss: 7.469790  [11200/12672]\n",
      "AUC: 0.5446420261898265\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.271780  [    0/12672]\n",
      "loss: 8.584652  [ 1600/12672]\n",
      "loss: 13.140459  [ 3200/12672]\n",
      "loss: 14.204865  [ 4800/12672]\n",
      "loss: 4.776558  [ 6400/12672]\n",
      "loss: 11.014359  [ 8000/12672]\n",
      "loss: 20.743042  [ 9600/12672]\n",
      "loss: 7.611734  [11200/12672]\n",
      "AUC: 0.604839311813765\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 5.005864  [    0/12672]\n",
      "loss: 17.197273  [ 1600/12672]\n",
      "loss: 20.821140  [ 3200/12672]\n",
      "loss: 17.445971  [ 4800/12672]\n",
      "loss: 11.184290  [ 6400/12672]\n",
      "loss: 18.471264  [ 8000/12672]\n",
      "loss: 19.241203  [ 9600/12672]\n",
      "loss: 11.208179  [11200/12672]\n",
      "AUC: 0.5808669722005418\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.630642  [    0/12672]\n",
      "loss: 18.710932  [ 1600/12672]\n",
      "loss: 12.972134  [ 3200/12672]\n",
      "loss: 19.169201  [ 4800/12672]\n",
      "loss: 5.468853  [ 6400/12672]\n",
      "loss: 16.444416  [ 8000/12672]\n",
      "loss: 14.662804  [ 9600/12672]\n",
      "loss: 25.706545  [11200/12672]\n",
      "AUC: 0.5919554643924541\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.618052  [    0/12672]\n",
      "loss: 4.721966  [ 1600/12672]\n",
      "loss: 23.899298  [ 3200/12672]\n",
      "loss: 24.523899  [ 4800/12672]\n",
      "loss: 22.084925  [ 6400/12672]\n",
      "loss: 8.903074  [ 8000/12672]\n",
      "loss: 15.941516  [ 9600/12672]\n",
      "loss: 11.089851  [11200/12672]\n",
      "AUC: 0.5437812117289473\n",
      "Done!\n",
      "##############################\n",
      "SESSION 1\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.711888  [    0/12672]\n",
      "loss: 12.359042  [ 1600/12672]\n",
      "loss: 12.989155  [ 3200/12672]\n",
      "loss: 15.020239  [ 4800/12672]\n",
      "loss: 21.760071  [ 6400/12672]\n",
      "loss: 14.394426  [ 8000/12672]\n",
      "loss: 6.855078  [ 9600/12672]\n",
      "loss: 6.002698  [11200/12672]\n",
      "AUC: 0.5322110010162602\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 20.650576  [    0/12672]\n",
      "loss: 4.901742  [ 1600/12672]\n",
      "loss: 16.624340  [ 3200/12672]\n",
      "loss: 15.264400  [ 4800/12672]\n",
      "loss: 28.131216  [ 6400/12672]\n",
      "loss: 12.218087  [ 8000/12672]\n",
      "loss: 27.742258  [ 9600/12672]\n",
      "loss: 14.287005  [11200/12672]\n",
      "AUC: 0.5946015545604675\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.606502  [    0/12672]\n",
      "loss: 7.176241  [ 1600/12672]\n",
      "loss: 9.124722  [ 3200/12672]\n",
      "loss: 8.837542  [ 4800/12672]\n",
      "loss: 10.587821  [ 6400/12672]\n",
      "loss: 9.362284  [ 8000/12672]\n",
      "loss: 27.351089  [ 9600/12672]\n",
      "loss: 25.541252  [11200/12672]\n",
      "AUC: 0.5261986841548103\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.691029  [    0/12672]\n",
      "loss: 18.512764  [ 1600/12672]\n",
      "loss: 20.734739  [ 3200/12672]\n",
      "loss: 20.309952  [ 4800/12672]\n",
      "loss: 19.617535  [ 6400/12672]\n",
      "loss: 5.014045  [ 8000/12672]\n",
      "loss: 23.738987  [ 9600/12672]\n",
      "loss: 10.756639  [11200/12672]\n",
      "AUC: 0.5742129276761518\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.452726  [    0/12672]\n",
      "loss: 10.358311  [ 1600/12672]\n",
      "loss: 13.615909  [ 3200/12672]\n",
      "loss: 20.037922  [ 4800/12672]\n",
      "loss: 11.295886  [ 6400/12672]\n",
      "loss: 7.909841  [ 8000/12672]\n",
      "loss: 7.605312  [ 9600/12672]\n",
      "loss: 12.190151  [11200/12672]\n",
      "AUC: 0.5814145997417006\n",
      "Done!\n",
      "##############################\n",
      "SESSION 2\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.768238  [    0/12672]\n",
      "loss: 20.579218  [ 1600/12672]\n",
      "loss: 5.475163  [ 3200/12672]\n",
      "loss: 11.752067  [ 4800/12672]\n",
      "loss: 16.628252  [ 6400/12672]\n",
      "loss: 3.027725  [ 8000/12672]\n",
      "loss: 17.065857  [ 9600/12672]\n",
      "loss: 25.297451  [11200/12672]\n",
      "AUC: 0.578615050577764\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 6.161941  [    0/12672]\n",
      "loss: 11.299352  [ 1600/12672]\n",
      "loss: 14.087561  [ 3200/12672]\n",
      "loss: 15.211898  [ 4800/12672]\n",
      "loss: 5.847706  [ 6400/12672]\n",
      "loss: 11.295163  [ 8000/12672]\n",
      "loss: 13.786308  [ 9600/12672]\n",
      "loss: 12.105211  [11200/12672]\n",
      "AUC: 0.5348164858564461\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 16.359638  [    0/12672]\n",
      "loss: 27.207569  [ 1600/12672]\n",
      "loss: 15.032549  [ 3200/12672]\n",
      "loss: 26.912109  [ 4800/12672]\n",
      "loss: 16.625454  [ 6400/12672]\n",
      "loss: 16.418720  [ 8000/12672]\n",
      "loss: 22.668493  [ 9600/12672]\n",
      "loss: 7.318204  [11200/12672]\n",
      "AUC: 0.5861952099055961\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 8.709901  [    0/12672]\n",
      "loss: 17.779448  [ 1600/12672]\n",
      "loss: 18.978687  [ 3200/12672]\n",
      "loss: 6.184202  [ 4800/12672]\n",
      "loss: 21.171608  [ 6400/12672]\n",
      "loss: 15.478992  [ 8000/12672]\n",
      "loss: 29.271326  [ 9600/12672]\n",
      "loss: 6.577135  [11200/12672]\n",
      "AUC: 0.6040778056163838\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.225354  [    0/12672]\n",
      "loss: 11.655666  [ 1600/12672]\n",
      "loss: 17.502806  [ 3200/12672]\n",
      "loss: 14.524054  [ 4800/12672]\n",
      "loss: 9.186088  [ 6400/12672]\n",
      "loss: 22.148491  [ 8000/12672]\n",
      "loss: 11.247519  [ 9600/12672]\n",
      "loss: 14.508182  [11200/12672]\n",
      "AUC: 0.5437338844838097\n",
      "Done!\n",
      "##############################\n",
      "SESSION 3\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.696899  [    0/12672]\n",
      "loss: 11.661193  [ 1600/12672]\n",
      "loss: 18.142853  [ 3200/12672]\n",
      "loss: 19.595301  [ 4800/12672]\n",
      "loss: 21.314150  [ 6400/12672]\n",
      "loss: 15.489676  [ 8000/12672]\n",
      "loss: 19.998533  [ 9600/12672]\n",
      "loss: 17.150213  [11200/12672]\n",
      "AUC: 0.5232480603510175\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.513584  [    0/12672]\n",
      "loss: 15.911542  [ 1600/12672]\n",
      "loss: 23.321674  [ 3200/12672]\n",
      "loss: 7.307704  [ 4800/12672]\n",
      "loss: 5.972665  [ 6400/12672]\n",
      "loss: 4.153691  [ 8000/12672]\n",
      "loss: 17.262035  [ 9600/12672]\n",
      "loss: 15.440261  [11200/12672]\n",
      "AUC: 0.5588634462952484\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 23.847673  [    0/12672]\n",
      "loss: 14.309379  [ 1600/12672]\n",
      "loss: 11.212944  [ 3200/12672]\n",
      "loss: 15.174956  [ 4800/12672]\n",
      "loss: 14.301064  [ 6400/12672]\n",
      "loss: 21.493385  [ 8000/12672]\n",
      "loss: 10.629948  [ 9600/12672]\n",
      "loss: 11.254156  [11200/12672]\n",
      "AUC: 0.6003163127216407\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.926312  [    0/12672]\n",
      "loss: 16.242809  [ 1600/12672]\n",
      "loss: 14.744219  [ 3200/12672]\n",
      "loss: 12.254446  [ 4800/12672]\n",
      "loss: 23.028677  [ 6400/12672]\n",
      "loss: 11.702184  [ 8000/12672]\n",
      "loss: 21.227058  [ 9600/12672]\n",
      "loss: 5.917282  [11200/12672]\n",
      "AUC: 0.531132178253866\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 22.534773  [    0/12672]\n",
      "loss: 15.801152  [ 1600/12672]\n",
      "loss: 19.950909  [ 3200/12672]\n",
      "loss: 17.506519  [ 4800/12672]\n",
      "loss: 10.543183  [ 6400/12672]\n",
      "loss: 13.422499  [ 8000/12672]\n",
      "loss: 13.219631  [ 9600/12672]\n",
      "loss: 13.912650  [11200/12672]\n",
      "AUC: 0.6268614875430802\n",
      "Done!\n",
      "##############################\n",
      "SESSION 4\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.715754  [    0/12672]\n",
      "loss: 6.607387  [ 1600/12672]\n",
      "loss: 9.226167  [ 3200/12672]\n",
      "loss: 13.327212  [ 4800/12672]\n",
      "loss: 13.823853  [ 6400/12672]\n",
      "loss: 11.669647  [ 8000/12672]\n",
      "loss: 15.439668  [ 9600/12672]\n",
      "loss: 13.581940  [11200/12672]\n",
      "AUC: 0.5314500216340415\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 26.118704  [    0/12672]\n",
      "loss: 10.807270  [ 1600/12672]\n",
      "loss: 24.513887  [ 3200/12672]\n",
      "loss: 9.293546  [ 4800/12672]\n",
      "loss: 19.926479  [ 6400/12672]\n",
      "loss: 12.963556  [ 8000/12672]\n",
      "loss: 14.175673  [ 9600/12672]\n",
      "loss: 6.367430  [11200/12672]\n",
      "AUC: 0.59597256473161\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.748791  [    0/12672]\n",
      "loss: 7.314189  [ 1600/12672]\n",
      "loss: 15.904921  [ 3200/12672]\n",
      "loss: 7.110794  [ 4800/12672]\n",
      "loss: 7.108387  [ 6400/12672]\n",
      "loss: 31.383080  [ 8000/12672]\n",
      "loss: 14.261230  [ 9600/12672]\n",
      "loss: 5.940573  [11200/12672]\n",
      "AUC: 0.5470710918050676\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.619684  [    0/12672]\n",
      "loss: 12.696946  [ 1600/12672]\n",
      "loss: 21.360691  [ 3200/12672]\n",
      "loss: 21.476719  [ 4800/12672]\n",
      "loss: 17.946554  [ 6400/12672]\n",
      "loss: 20.552397  [ 8000/12672]\n",
      "loss: 7.180481  [ 9600/12672]\n",
      "loss: 16.203453  [11200/12672]\n",
      "AUC: 0.5606967946449886\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.117962  [    0/12672]\n",
      "loss: 9.538943  [ 1600/12672]\n",
      "loss: 26.496964  [ 3200/12672]\n",
      "loss: 19.641485  [ 4800/12672]\n",
      "loss: 12.577559  [ 6400/12672]\n",
      "loss: 15.989140  [ 8000/12672]\n",
      "loss: 21.046623  [ 9600/12672]\n",
      "loss: 9.641997  [11200/12672]\n",
      "AUC: 0.6480413998823449\n",
      "Done!\n",
      "##############################\n",
      "SESSION 5\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.698180  [    0/12672]\n",
      "loss: 21.934515  [ 1600/12672]\n",
      "loss: 6.303351  [ 3200/12672]\n",
      "loss: 10.266965  [ 4800/12672]\n",
      "loss: 12.037312  [ 6400/12672]\n",
      "loss: 5.247633  [ 8000/12672]\n",
      "loss: 19.813042  [ 9600/12672]\n",
      "loss: 13.812937  [11200/12672]\n",
      "AUC: 0.5395737388982079\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.101062  [    0/12672]\n",
      "loss: 35.929493  [ 1600/12672]\n",
      "loss: 8.616073  [ 3200/12672]\n",
      "loss: 4.532659  [ 4800/12672]\n",
      "loss: 6.137300  [ 6400/12672]\n",
      "loss: 14.753534  [ 8000/12672]\n",
      "loss: 6.271632  [ 9600/12672]\n",
      "loss: 16.111866  [11200/12672]\n",
      "AUC: 0.5567782502204701\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.398585  [    0/12672]\n",
      "loss: 18.065645  [ 1600/12672]\n",
      "loss: 5.917847  [ 3200/12672]\n",
      "loss: 24.259449  [ 4800/12672]\n",
      "loss: 17.809837  [ 6400/12672]\n",
      "loss: 15.637119  [ 8000/12672]\n",
      "loss: 20.454823  [ 9600/12672]\n",
      "loss: 11.143926  [11200/12672]\n",
      "AUC: 0.5819037113669486\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.024585  [    0/12672]\n",
      "loss: 14.726082  [ 1600/12672]\n",
      "loss: 8.537585  [ 3200/12672]\n",
      "loss: 14.124620  [ 4800/12672]\n",
      "loss: 8.396895  [ 6400/12672]\n",
      "loss: 20.322254  [ 8000/12672]\n",
      "loss: 15.560442  [ 9600/12672]\n",
      "loss: 16.985985  [11200/12672]\n",
      "AUC: 0.5552938721789558\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 16.660355  [    0/12672]\n",
      "loss: 11.611380  [ 1600/12672]\n",
      "loss: 8.388479  [ 3200/12672]\n",
      "loss: 16.205341  [ 4800/12672]\n",
      "loss: 9.616090  [ 6400/12672]\n",
      "loss: 4.674777  [ 8000/12672]\n",
      "loss: 21.660072  [ 9600/12672]\n",
      "loss: 22.177238  [11200/12672]\n",
      "AUC: 0.630951053517112\n",
      "Done!\n",
      "##############################\n",
      "SESSION 6\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.781263  [    0/12672]\n",
      "loss: 15.764732  [ 1600/12672]\n",
      "loss: 9.314082  [ 3200/12672]\n",
      "loss: 22.163721  [ 4800/12672]\n",
      "loss: 13.273639  [ 6400/12672]\n",
      "loss: 19.215988  [ 8000/12672]\n",
      "loss: 2.481458  [ 9600/12672]\n",
      "loss: 20.053154  [11200/12672]\n",
      "AUC: 0.5837440306142787\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.265342  [    0/12672]\n",
      "loss: 27.765854  [ 1600/12672]\n",
      "loss: 16.036860  [ 3200/12672]\n",
      "loss: 22.446161  [ 4800/12672]\n",
      "loss: 13.956430  [ 6400/12672]\n",
      "loss: 11.147503  [ 8000/12672]\n",
      "loss: 9.752005  [ 9600/12672]\n",
      "loss: 16.854477  [11200/12672]\n",
      "AUC: 0.527637053375676\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.713266  [    0/12672]\n",
      "loss: 19.375999  [ 1600/12672]\n",
      "loss: 22.384056  [ 3200/12672]\n",
      "loss: 10.389393  [ 4800/12672]\n",
      "loss: 20.602196  [ 6400/12672]\n",
      "loss: 11.809483  [ 8000/12672]\n",
      "loss: 4.170840  [ 9600/12672]\n",
      "loss: 16.749306  [11200/12672]\n",
      "AUC: 0.5992844576728364\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.260502  [    0/12672]\n",
      "loss: 20.618311  [ 1600/12672]\n",
      "loss: 19.300356  [ 3200/12672]\n",
      "loss: 24.812464  [ 4800/12672]\n",
      "loss: 25.220850  [ 6400/12672]\n",
      "loss: 15.267987  [ 8000/12672]\n",
      "loss: 12.718666  [ 9600/12672]\n",
      "loss: 14.918570  [11200/12672]\n",
      "AUC: 0.5301424015732339\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 24.724180  [    0/12672]\n",
      "loss: 6.009122  [ 1600/12672]\n",
      "loss: 11.466046  [ 3200/12672]\n",
      "loss: 13.044214  [ 4800/12672]\n",
      "loss: 15.155224  [ 6400/12672]\n",
      "loss: 19.216724  [ 8000/12672]\n",
      "loss: 11.937214  [ 9600/12672]\n",
      "loss: 14.131810  [11200/12672]\n",
      "AUC: 0.6317278996531976\n",
      "Done!\n",
      "##############################\n",
      "SESSION 7\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.704533  [    0/12672]\n",
      "loss: 9.348142  [ 1600/12672]\n",
      "loss: 14.870744  [ 3200/12672]\n",
      "loss: 25.428961  [ 4800/12672]\n",
      "loss: 22.799410  [ 6400/12672]\n",
      "loss: 12.531997  [ 8000/12672]\n",
      "loss: 24.459454  [ 9600/12672]\n",
      "loss: 9.012276  [11200/12672]\n",
      "AUC: 0.5246001209953319\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 33.072628  [    0/12672]\n",
      "loss: 6.756789  [ 1600/12672]\n",
      "loss: 11.719251  [ 3200/12672]\n",
      "loss: 9.914344  [ 4800/12672]\n",
      "loss: 11.423139  [ 6400/12672]\n",
      "loss: 15.690159  [ 8000/12672]\n",
      "loss: 29.722776  [ 9600/12672]\n",
      "loss: 22.387354  [11200/12672]\n",
      "AUC: 0.5719605074406846\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.261319  [    0/12672]\n",
      "loss: 16.354210  [ 1600/12672]\n",
      "loss: 11.121556  [ 3200/12672]\n",
      "loss: 6.249462  [ 4800/12672]\n",
      "loss: 9.125505  [ 6400/12672]\n",
      "loss: 5.956802  [ 8000/12672]\n",
      "loss: 9.447570  [ 9600/12672]\n",
      "loss: 7.810426  [11200/12672]\n",
      "AUC: 0.5529055523119883\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.406349  [    0/12672]\n",
      "loss: 10.497344  [ 1600/12672]\n",
      "loss: 17.090393  [ 3200/12672]\n",
      "loss: 17.525242  [ 4800/12672]\n",
      "loss: 19.258940  [ 6400/12672]\n",
      "loss: 13.893556  [ 8000/12672]\n",
      "loss: 21.115425  [ 9600/12672]\n",
      "loss: 12.056801  [11200/12672]\n",
      "AUC: 0.5958624615945494\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 19.433838  [    0/12672]\n",
      "loss: 5.747532  [ 1600/12672]\n",
      "loss: 14.793535  [ 3200/12672]\n",
      "loss: 14.870832  [ 4800/12672]\n",
      "loss: 10.711168  [ 6400/12672]\n",
      "loss: 8.646398  [ 8000/12672]\n",
      "loss: 14.634919  [ 9600/12672]\n",
      "loss: 17.021189  [11200/12672]\n",
      "AUC: 0.612259099086723\n",
      "Done!\n",
      "##############################\n",
      "SESSION 8\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.710401  [    0/12672]\n",
      "loss: 11.097256  [ 1600/12672]\n",
      "loss: 15.830924  [ 3200/12672]\n",
      "loss: 10.372213  [ 4800/12672]\n",
      "loss: 20.283909  [ 6400/12672]\n",
      "loss: 14.257603  [ 8000/12672]\n",
      "loss: 6.909352  [ 9600/12672]\n",
      "loss: 18.756651  [11200/12672]\n",
      "AUC: 0.5077017916307048\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.637143  [    0/12672]\n",
      "loss: 18.800756  [ 1600/12672]\n",
      "loss: 10.220440  [ 3200/12672]\n",
      "loss: 15.832799  [ 4800/12672]\n",
      "loss: 13.581001  [ 6400/12672]\n",
      "loss: 18.284719  [ 8000/12672]\n",
      "loss: 26.438013  [ 9600/12672]\n",
      "loss: 10.521675  [11200/12672]\n",
      "AUC: 0.525559361502419\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.965492  [    0/12672]\n",
      "loss: 7.549736  [ 1600/12672]\n",
      "loss: 19.394087  [ 3200/12672]\n",
      "loss: 13.582598  [ 4800/12672]\n",
      "loss: 13.640011  [ 6400/12672]\n",
      "loss: 6.760755  [ 8000/12672]\n",
      "loss: 20.070881  [ 9600/12672]\n",
      "loss: 4.707345  [11200/12672]\n",
      "AUC: 0.5827834576006198\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.187993  [    0/12672]\n",
      "loss: 23.777412  [ 1600/12672]\n",
      "loss: 7.766585  [ 3200/12672]\n",
      "loss: 12.882132  [ 4800/12672]\n",
      "loss: 14.980778  [ 6400/12672]\n",
      "loss: 13.138389  [ 8000/12672]\n",
      "loss: 12.673107  [ 9600/12672]\n",
      "loss: 17.688274  [11200/12672]\n",
      "AUC: 0.5314513973602155\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 25.341429  [    0/12672]\n",
      "loss: 19.249659  [ 1600/12672]\n",
      "loss: 6.359468  [ 3200/12672]\n",
      "loss: 19.350775  [ 4800/12672]\n",
      "loss: 6.883957  [ 6400/12672]\n",
      "loss: 7.830611  [ 8000/12672]\n",
      "loss: 14.366782  [ 9600/12672]\n",
      "loss: 18.429316  [11200/12672]\n",
      "AUC: 0.572872931210764\n",
      "Done!\n",
      "##############################\n",
      "SESSION 9\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.748303  [    0/12672]\n",
      "loss: 7.936151  [ 1600/12672]\n",
      "loss: 20.725136  [ 3200/12672]\n",
      "loss: 10.707813  [ 4800/12672]\n",
      "loss: 14.526614  [ 6400/12672]\n",
      "loss: 13.811191  [ 8000/12672]\n",
      "loss: 26.483376  [ 9600/12672]\n",
      "loss: 6.072716  [11200/12672]\n",
      "AUC: 0.5160318682683138\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.547757  [    0/12672]\n",
      "loss: 9.282087  [ 1600/12672]\n",
      "loss: 10.966198  [ 3200/12672]\n",
      "loss: 8.378568  [ 4800/12672]\n",
      "loss: 13.332497  [ 6400/12672]\n",
      "loss: 17.862913  [ 8000/12672]\n",
      "loss: 21.669296  [ 9600/12672]\n",
      "loss: 17.077230  [11200/12672]\n",
      "AUC: 0.5105425240471617\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.267071  [    0/12672]\n",
      "loss: 15.385608  [ 1600/12672]\n",
      "loss: 5.400661  [ 3200/12672]\n",
      "loss: 23.366566  [ 4800/12672]\n",
      "loss: 21.468515  [ 6400/12672]\n",
      "loss: 9.984352  [ 8000/12672]\n",
      "loss: 18.296101  [ 9600/12672]\n",
      "loss: 4.145028  [11200/12672]\n",
      "AUC: 0.5278937697971346\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 20.164444  [    0/12672]\n",
      "loss: 19.370020  [ 1600/12672]\n",
      "loss: 12.250197  [ 3200/12672]\n",
      "loss: 7.828905  [ 4800/12672]\n",
      "loss: 24.590340  [ 6400/12672]\n",
      "loss: 19.278118  [ 8000/12672]\n",
      "loss: 21.397993  [ 9600/12672]\n",
      "loss: 9.593742  [11200/12672]\n",
      "AUC: 0.5531907631423915\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 20.921209  [    0/12672]\n",
      "loss: 22.875908  [ 1600/12672]\n",
      "loss: 14.561709  [ 3200/12672]\n",
      "loss: 16.353199  [ 4800/12672]\n",
      "loss: 13.240146  [ 6400/12672]\n",
      "loss: 9.649020  [ 8000/12672]\n",
      "loss: 22.179497  [ 9600/12672]\n",
      "loss: 13.723084  [11200/12672]\n",
      "AUC: 0.5644905779508463\n",
      "Done!\n",
      "##############################\n",
      "SESSION 10\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.712035  [    0/12672]\n",
      "loss: 27.308434  [ 1600/12672]\n",
      "loss: 10.942783  [ 3200/12672]\n",
      "loss: 19.809189  [ 4800/12672]\n",
      "loss: 6.781195  [ 6400/12672]\n",
      "loss: 18.083742  [ 8000/12672]\n",
      "loss: 23.530596  [ 9600/12672]\n",
      "loss: 21.854525  [11200/12672]\n",
      "AUC: 0.5429301605321533\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.018834  [    0/12672]\n",
      "loss: 10.325314  [ 1600/12672]\n",
      "loss: 6.741055  [ 3200/12672]\n",
      "loss: 11.510046  [ 4800/12672]\n",
      "loss: 19.059340  [ 6400/12672]\n",
      "loss: 15.055028  [ 8000/12672]\n",
      "loss: 19.028658  [ 9600/12672]\n",
      "loss: 10.181606  [11200/12672]\n",
      "AUC: 0.5701682542593138\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.546700  [    0/12672]\n",
      "loss: 24.841446  [ 1600/12672]\n",
      "loss: 21.642864  [ 3200/12672]\n",
      "loss: 23.779448  [ 4800/12672]\n",
      "loss: 4.571509  [ 6400/12672]\n",
      "loss: 6.001882  [ 8000/12672]\n",
      "loss: 11.913072  [ 9600/12672]\n",
      "loss: 8.399316  [11200/12672]\n",
      "AUC: 0.5736476688856236\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 7.979867  [    0/12672]\n",
      "loss: 8.865288  [ 1600/12672]\n",
      "loss: 6.985500  [ 3200/12672]\n",
      "loss: 14.539007  [ 4800/12672]\n",
      "loss: 6.569482  [ 6400/12672]\n",
      "loss: 21.581167  [ 8000/12672]\n",
      "loss: 21.355864  [ 9600/12672]\n",
      "loss: 23.274717  [11200/12672]\n",
      "AUC: 0.5504775032324558\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.693204  [    0/12672]\n",
      "loss: 14.180065  [ 1600/12672]\n",
      "loss: 11.614965  [ 3200/12672]\n",
      "loss: 13.988416  [ 4800/12672]\n",
      "loss: 20.809843  [ 6400/12672]\n",
      "loss: 6.787203  [ 8000/12672]\n",
      "loss: 18.110603  [ 9600/12672]\n",
      "loss: 18.095034  [11200/12672]\n",
      "AUC: 0.6294347590750097\n",
      "Done!\n",
      "##############################\n",
      "SESSION 11\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.704315  [    0/12672]\n",
      "loss: 20.412889  [ 1600/12672]\n",
      "loss: 25.953648  [ 3200/12672]\n",
      "loss: 7.633456  [ 4800/12672]\n",
      "loss: 18.119261  [ 6400/12672]\n",
      "loss: 5.133259  [ 8000/12672]\n",
      "loss: 4.756819  [ 9600/12672]\n",
      "loss: 15.310956  [11200/12672]\n",
      "AUC: 0.5318369551830072\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 7.518592  [    0/12672]\n",
      "loss: 10.047850  [ 1600/12672]\n",
      "loss: 15.182771  [ 3200/12672]\n",
      "loss: 9.952289  [ 4800/12672]\n",
      "loss: 9.916888  [ 6400/12672]\n",
      "loss: 19.781710  [ 8000/12672]\n",
      "loss: 11.325500  [ 9600/12672]\n",
      "loss: 15.254220  [11200/12672]\n",
      "AUC: 0.49505727991837234\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.731310  [    0/12672]\n",
      "loss: 10.364454  [ 1600/12672]\n",
      "loss: 8.143648  [ 3200/12672]\n",
      "loss: 12.392374  [ 4800/12672]\n",
      "loss: 21.469032  [ 6400/12672]\n",
      "loss: 18.774431  [ 8000/12672]\n",
      "loss: 18.339651  [ 9600/12672]\n",
      "loss: 21.740437  [11200/12672]\n",
      "AUC: 0.5517649690144496\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 19.981926  [    0/12672]\n",
      "loss: 6.130752  [ 1600/12672]\n",
      "loss: 13.080803  [ 3200/12672]\n",
      "loss: 23.345867  [ 4800/12672]\n",
      "loss: 11.459679  [ 6400/12672]\n",
      "loss: 13.295467  [ 8000/12672]\n",
      "loss: 19.869835  [ 9600/12672]\n",
      "loss: 10.065614  [11200/12672]\n",
      "AUC: 0.5999923431881233\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.175488  [    0/12672]\n",
      "loss: 16.601288  [ 1600/12672]\n",
      "loss: 18.422144  [ 3200/12672]\n",
      "loss: 19.055571  [ 4800/12672]\n",
      "loss: 17.281008  [ 6400/12672]\n",
      "loss: 8.125804  [ 8000/12672]\n",
      "loss: 16.126484  [ 9600/12672]\n",
      "loss: 18.301113  [11200/12672]\n",
      "AUC: 0.547400084937315\n",
      "Done!\n",
      "##############################\n",
      "SESSION 12\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.754203  [    0/12672]\n",
      "loss: 33.075317  [ 1600/12672]\n",
      "loss: 21.082348  [ 3200/12672]\n",
      "loss: 11.588009  [ 4800/12672]\n",
      "loss: 17.617052  [ 6400/12672]\n",
      "loss: 15.604836  [ 8000/12672]\n",
      "loss: 4.866552  [ 9600/12672]\n",
      "loss: 9.687130  [11200/12672]\n",
      "AUC: 0.5817535163574535\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.651123  [    0/12672]\n",
      "loss: 12.774558  [ 1600/12672]\n",
      "loss: 23.122890  [ 3200/12672]\n",
      "loss: 15.966972  [ 4800/12672]\n",
      "loss: 14.732173  [ 6400/12672]\n",
      "loss: 11.801238  [ 8000/12672]\n",
      "loss: 25.601656  [ 9600/12672]\n",
      "loss: 15.616955  [11200/12672]\n",
      "AUC: 0.5598691443829269\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 16.246061  [    0/12672]\n",
      "loss: 16.858810  [ 1600/12672]\n",
      "loss: 15.042475  [ 3200/12672]\n",
      "loss: 10.554617  [ 4800/12672]\n",
      "loss: 9.313137  [ 6400/12672]\n",
      "loss: 12.184534  [ 8000/12672]\n",
      "loss: 12.101797  [ 9600/12672]\n",
      "loss: 11.648016  [11200/12672]\n",
      "AUC: 0.5452191767461243\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.681980  [    0/12672]\n",
      "loss: 11.496425  [ 1600/12672]\n",
      "loss: 27.888884  [ 3200/12672]\n",
      "loss: 13.503428  [ 4800/12672]\n",
      "loss: 16.040800  [ 6400/12672]\n",
      "loss: 12.816006  [ 8000/12672]\n",
      "loss: 11.740731  [ 9600/12672]\n",
      "loss: 24.700127  [11200/12672]\n",
      "AUC: 0.5547622561096355\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.345071  [    0/12672]\n",
      "loss: 23.314192  [ 1600/12672]\n",
      "loss: 7.417016  [ 3200/12672]\n",
      "loss: 23.475819  [ 4800/12672]\n",
      "loss: 15.069730  [ 6400/12672]\n",
      "loss: 21.390661  [ 8000/12672]\n",
      "loss: 21.388130  [ 9600/12672]\n",
      "loss: 18.376514  [11200/12672]\n",
      "AUC: 0.5781209295309848\n",
      "Done!\n",
      "##############################\n",
      "SESSION 13\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.753824  [    0/12672]\n",
      "loss: 23.458136  [ 1600/12672]\n",
      "loss: 18.084045  [ 3200/12672]\n",
      "loss: 3.970020  [ 4800/12672]\n",
      "loss: 10.175546  [ 6400/12672]\n",
      "loss: 8.119101  [ 8000/12672]\n",
      "loss: 8.741718  [ 9600/12672]\n",
      "loss: 32.293938  [11200/12672]\n",
      "AUC: 0.5653608746450061\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.366346  [    0/12672]\n",
      "loss: 9.778618  [ 1600/12672]\n",
      "loss: 14.374590  [ 3200/12672]\n",
      "loss: 9.101529  [ 4800/12672]\n",
      "loss: 14.357322  [ 6400/12672]\n",
      "loss: 7.112517  [ 8000/12672]\n",
      "loss: 9.458587  [ 9600/12672]\n",
      "loss: 18.986370  [11200/12672]\n",
      "AUC: 0.5815170542687433\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 16.884106  [    0/12672]\n",
      "loss: 6.658088  [ 1600/12672]\n",
      "loss: 5.533974  [ 3200/12672]\n",
      "loss: 9.741771  [ 4800/12672]\n",
      "loss: 5.955732  [ 6400/12672]\n",
      "loss: 7.209926  [ 8000/12672]\n",
      "loss: 19.923164  [ 9600/12672]\n",
      "loss: 14.470799  [11200/12672]\n",
      "AUC: 0.6050602970638416\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.761018  [    0/12672]\n",
      "loss: 14.644378  [ 1600/12672]\n",
      "loss: 16.837200  [ 3200/12672]\n",
      "loss: 8.263916  [ 4800/12672]\n",
      "loss: 28.433132  [ 6400/12672]\n",
      "loss: 18.343452  [ 8000/12672]\n",
      "loss: 20.542986  [ 9600/12672]\n",
      "loss: 8.342428  [11200/12672]\n",
      "AUC: 0.5207731040055549\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 24.385365  [    0/12672]\n",
      "loss: 15.986883  [ 1600/12672]\n",
      "loss: 14.447851  [ 3200/12672]\n",
      "loss: 20.384577  [ 4800/12672]\n",
      "loss: 20.306192  [ 6400/12672]\n",
      "loss: 10.636785  [ 8000/12672]\n",
      "loss: 10.255014  [ 9600/12672]\n",
      "loss: 19.376022  [11200/12672]\n",
      "AUC: 0.5128518785082452\n",
      "Done!\n",
      "##############################\n",
      "SESSION 14\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.773701  [    0/12672]\n",
      "loss: 10.495410  [ 1600/12672]\n",
      "loss: 8.124262  [ 3200/12672]\n",
      "loss: 7.415558  [ 4800/12672]\n",
      "loss: 15.911886  [ 6400/12672]\n",
      "loss: 21.941168  [ 8000/12672]\n",
      "loss: 23.387827  [ 9600/12672]\n",
      "loss: 11.610407  [11200/12672]\n",
      "AUC: 0.5856445972428707\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.794983  [    0/12672]\n",
      "loss: 12.438693  [ 1600/12672]\n",
      "loss: 16.165909  [ 3200/12672]\n",
      "loss: 17.133089  [ 4800/12672]\n",
      "loss: 4.951224  [ 6400/12672]\n",
      "loss: 17.612970  [ 8000/12672]\n",
      "loss: 25.592529  [ 9600/12672]\n",
      "loss: 8.129426  [11200/12672]\n",
      "AUC: 0.5701756175876808\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 8.429403  [    0/12672]\n",
      "loss: 19.291533  [ 1600/12672]\n",
      "loss: 21.040827  [ 3200/12672]\n",
      "loss: 16.647335  [ 4800/12672]\n",
      "loss: 7.174587  [ 6400/12672]\n",
      "loss: 19.582712  [ 8000/12672]\n",
      "loss: 6.656965  [ 9600/12672]\n",
      "loss: 17.990620  [11200/12672]\n",
      "AUC: 0.5705488732641235\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.312908  [    0/12672]\n",
      "loss: 22.999836  [ 1600/12672]\n",
      "loss: 4.944500  [ 3200/12672]\n",
      "loss: 8.752696  [ 4800/12672]\n",
      "loss: 20.774757  [ 6400/12672]\n",
      "loss: 11.171482  [ 8000/12672]\n",
      "loss: 7.682131  [ 9600/12672]\n",
      "loss: 23.517729  [11200/12672]\n",
      "AUC: 0.5319779937533788\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.084287  [    0/12672]\n",
      "loss: 14.226170  [ 1600/12672]\n",
      "loss: 6.723845  [ 3200/12672]\n",
      "loss: 10.553730  [ 4800/12672]\n",
      "loss: 17.693771  [ 6400/12672]\n",
      "loss: 11.649330  [ 8000/12672]\n",
      "loss: 6.295319  [ 9600/12672]\n",
      "loss: 18.160908  [11200/12672]\n",
      "AUC: 0.5799308553098391\n",
      "Done!\n",
      "##############################\n",
      "SESSION 15\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.740471  [    0/12672]\n",
      "loss: 23.938982  [ 1600/12672]\n",
      "loss: 6.077385  [ 3200/12672]\n",
      "loss: 22.678804  [ 4800/12672]\n",
      "loss: 10.801258  [ 6400/12672]\n",
      "loss: 30.210115  [ 8000/12672]\n",
      "loss: 9.545631  [ 9600/12672]\n",
      "loss: 15.455153  [11200/12672]\n",
      "AUC: 0.5234549427552453\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.451771  [    0/12672]\n",
      "loss: 20.187569  [ 1600/12672]\n",
      "loss: 23.412575  [ 3200/12672]\n",
      "loss: 6.309510  [ 4800/12672]\n",
      "loss: 13.929440  [ 6400/12672]\n",
      "loss: 16.067242  [ 8000/12672]\n",
      "loss: 7.072131  [ 9600/12672]\n",
      "loss: 15.664577  [11200/12672]\n",
      "AUC: 0.6060263202064564\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 7.176349  [    0/12672]\n",
      "loss: 18.611086  [ 1600/12672]\n",
      "loss: 27.991844  [ 3200/12672]\n",
      "loss: 6.996548  [ 4800/12672]\n",
      "loss: 13.202961  [ 6400/12672]\n",
      "loss: 10.844012  [ 8000/12672]\n",
      "loss: 12.464673  [ 9600/12672]\n",
      "loss: 21.969244  [11200/12672]\n",
      "AUC: 0.6145241729595485\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 8.072975  [    0/12672]\n",
      "loss: 7.565277  [ 1600/12672]\n",
      "loss: 16.931366  [ 3200/12672]\n",
      "loss: 19.129435  [ 4800/12672]\n",
      "loss: 19.928516  [ 6400/12672]\n",
      "loss: 20.439262  [ 8000/12672]\n",
      "loss: 10.116639  [ 9600/12672]\n",
      "loss: 6.957938  [11200/12672]\n",
      "AUC: 0.5950292595436163\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 6.289287  [    0/12672]\n",
      "loss: 13.278704  [ 1600/12672]\n",
      "loss: 16.290361  [ 3200/12672]\n",
      "loss: 30.175230  [ 4800/12672]\n",
      "loss: 15.047771  [ 6400/12672]\n",
      "loss: 9.884965  [ 8000/12672]\n",
      "loss: 14.998715  [ 9600/12672]\n",
      "loss: 15.276874  [11200/12672]\n",
      "AUC: 0.5825950841224241\n",
      "Done!\n",
      "##############################\n",
      "SESSION 16\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.738339  [    0/12672]\n",
      "loss: 25.380424  [ 1600/12672]\n",
      "loss: 13.626614  [ 3200/12672]\n",
      "loss: 14.982024  [ 4800/12672]\n",
      "loss: 24.969725  [ 6400/12672]\n",
      "loss: 11.770169  [ 8000/12672]\n",
      "loss: 18.362207  [ 9600/12672]\n",
      "loss: 11.122885  [11200/12672]\n",
      "AUC: 0.5088387659589251\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 23.044104  [    0/12672]\n",
      "loss: 17.077171  [ 1600/12672]\n",
      "loss: 16.594994  [ 3200/12672]\n",
      "loss: 19.383368  [ 4800/12672]\n",
      "loss: 22.473289  [ 6400/12672]\n",
      "loss: 20.698687  [ 8000/12672]\n",
      "loss: 26.153284  [ 9600/12672]\n",
      "loss: 22.463186  [11200/12672]\n",
      "AUC: 0.5650452704240754\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.912349  [    0/12672]\n",
      "loss: 20.830299  [ 1600/12672]\n",
      "loss: 8.240032  [ 3200/12672]\n",
      "loss: 16.284163  [ 4800/12672]\n",
      "loss: 18.895485  [ 6400/12672]\n",
      "loss: 7.534851  [ 8000/12672]\n",
      "loss: 6.451750  [ 9600/12672]\n",
      "loss: 8.099949  [11200/12672]\n",
      "AUC: 0.6112490808685793\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.232889  [    0/12672]\n",
      "loss: 12.886440  [ 1600/12672]\n",
      "loss: 24.553711  [ 3200/12672]\n",
      "loss: 10.337929  [ 4800/12672]\n",
      "loss: 13.697483  [ 6400/12672]\n",
      "loss: 17.987581  [ 8000/12672]\n",
      "loss: 17.761192  [ 9600/12672]\n",
      "loss: 21.866543  [11200/12672]\n",
      "AUC: 0.5452528704067033\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.850958  [    0/12672]\n",
      "loss: 19.092865  [ 1600/12672]\n",
      "loss: 22.105310  [ 3200/12672]\n",
      "loss: 10.730127  [ 4800/12672]\n",
      "loss: 16.460857  [ 6400/12672]\n",
      "loss: 11.429312  [ 8000/12672]\n",
      "loss: 7.266893  [ 9600/12672]\n",
      "loss: 9.151433  [11200/12672]\n",
      "AUC: 0.6128444894293599\n",
      "Done!\n",
      "##############################\n",
      "SESSION 17\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.737433  [    0/12672]\n",
      "loss: 23.102480  [ 1600/12672]\n",
      "loss: 7.869224  [ 3200/12672]\n",
      "loss: 8.436029  [ 4800/12672]\n",
      "loss: 9.718714  [ 6400/12672]\n",
      "loss: 16.455385  [ 8000/12672]\n",
      "loss: 6.451011  [ 9600/12672]\n",
      "loss: 18.272331  [11200/12672]\n",
      "AUC: 0.5217119740906335\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.573706  [    0/12672]\n",
      "loss: 18.082958  [ 1600/12672]\n",
      "loss: 18.753529  [ 3200/12672]\n",
      "loss: 24.106401  [ 4800/12672]\n",
      "loss: 22.166277  [ 6400/12672]\n",
      "loss: 14.322836  [ 8000/12672]\n",
      "loss: 9.221214  [ 9600/12672]\n",
      "loss: 12.831013  [11200/12672]\n",
      "AUC: 0.6082676518443082\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 7.850994  [    0/12672]\n",
      "loss: 20.700405  [ 1600/12672]\n",
      "loss: 9.054766  [ 3200/12672]\n",
      "loss: 12.554743  [ 4800/12672]\n",
      "loss: 4.389660  [ 6400/12672]\n",
      "loss: 7.331088  [ 8000/12672]\n",
      "loss: 7.960245  [ 9600/12672]\n",
      "loss: 8.076980  [11200/12672]\n",
      "AUC: 0.5034966460717616\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 23.415895  [    0/12672]\n",
      "loss: 15.348455  [ 1600/12672]\n",
      "loss: 16.970226  [ 3200/12672]\n",
      "loss: 4.566327  [ 4800/12672]\n",
      "loss: 11.812400  [ 6400/12672]\n",
      "loss: 13.843641  [ 8000/12672]\n",
      "loss: 19.278042  [ 9600/12672]\n",
      "loss: 17.063814  [11200/12672]\n",
      "AUC: 0.5646129653256002\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.510760  [    0/12672]\n",
      "loss: 7.133606  [ 1600/12672]\n",
      "loss: 10.619167  [ 3200/12672]\n",
      "loss: 21.986900  [ 4800/12672]\n",
      "loss: 16.307842  [ 6400/12672]\n",
      "loss: 18.976158  [ 8000/12672]\n",
      "loss: 24.408035  [ 9600/12672]\n",
      "loss: 9.240088  [11200/12672]\n",
      "AUC: 0.618692034764839\n",
      "Done!\n",
      "##############################\n",
      "SESSION 18\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.690764  [    0/12672]\n",
      "loss: 10.011233  [ 1600/12672]\n",
      "loss: 17.762932  [ 3200/12672]\n",
      "loss: 22.280846  [ 4800/12672]\n",
      "loss: 20.872896  [ 6400/12672]\n",
      "loss: 19.618891  [ 8000/12672]\n",
      "loss: 29.657532  [ 9600/12672]\n",
      "loss: 15.101323  [11200/12672]\n",
      "AUC: 0.5672566675954505\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.944849  [    0/12672]\n",
      "loss: 6.255785  [ 1600/12672]\n",
      "loss: 11.791917  [ 3200/12672]\n",
      "loss: 11.118100  [ 4800/12672]\n",
      "loss: 28.472031  [ 6400/12672]\n",
      "loss: 19.323191  [ 8000/12672]\n",
      "loss: 12.688002  [ 9600/12672]\n",
      "loss: 16.378178  [11200/12672]\n",
      "AUC: 0.5731622322497243\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 23.814259  [    0/12672]\n",
      "loss: 6.677822  [ 1600/12672]\n",
      "loss: 5.022784  [ 3200/12672]\n",
      "loss: 11.579763  [ 4800/12672]\n",
      "loss: 17.760677  [ 6400/12672]\n",
      "loss: 12.302190  [ 8000/12672]\n",
      "loss: 20.146433  [ 9600/12672]\n",
      "loss: 19.753138  [11200/12672]\n",
      "AUC: 0.5277617026748971\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.360149  [    0/12672]\n",
      "loss: 16.848801  [ 1600/12672]\n",
      "loss: 15.387479  [ 3200/12672]\n",
      "loss: 13.747839  [ 4800/12672]\n",
      "loss: 21.548386  [ 6400/12672]\n",
      "loss: 7.840837  [ 8000/12672]\n",
      "loss: 21.462774  [ 9600/12672]\n",
      "loss: 17.181143  [11200/12672]\n",
      "AUC: 0.5787050695621184\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.752174  [    0/12672]\n",
      "loss: 9.500867  [ 1600/12672]\n",
      "loss: 21.597906  [ 3200/12672]\n",
      "loss: 7.247507  [ 4800/12672]\n",
      "loss: 13.848420  [ 6400/12672]\n",
      "loss: 24.005394  [ 8000/12672]\n",
      "loss: 9.120103  [ 9600/12672]\n",
      "loss: 18.811199  [11200/12672]\n",
      "AUC: 0.5521685681517119\n",
      "Done!\n",
      "##############################\n",
      "SESSION 19\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.723551  [    0/12672]\n",
      "loss: 15.298618  [ 1600/12672]\n",
      "loss: 18.981596  [ 3200/12672]\n",
      "loss: 22.486578  [ 4800/12672]\n",
      "loss: 23.318510  [ 6400/12672]\n",
      "loss: 21.618670  [ 8000/12672]\n",
      "loss: 9.367621  [ 9600/12672]\n",
      "loss: 10.061339  [11200/12672]\n",
      "AUC: 0.5154909571054299\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.154558  [    0/12672]\n",
      "loss: 18.409157  [ 1600/12672]\n",
      "loss: 20.162466  [ 3200/12672]\n",
      "loss: 7.312500  [ 4800/12672]\n",
      "loss: 21.903170  [ 6400/12672]\n",
      "loss: 14.566277  [ 8000/12672]\n",
      "loss: 15.959089  [ 9600/12672]\n",
      "loss: 19.492090  [11200/12672]\n",
      "AUC: 0.562656426231694\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.704329  [    0/12672]\n",
      "loss: 6.677120  [ 1600/12672]\n",
      "loss: 19.749802  [ 3200/12672]\n",
      "loss: 24.083820  [ 4800/12672]\n",
      "loss: 17.670019  [ 6400/12672]\n",
      "loss: 18.503412  [ 8000/12672]\n",
      "loss: 11.260724  [ 9600/12672]\n",
      "loss: 8.187584  [11200/12672]\n",
      "AUC: 0.5470434057673506\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 25.451509  [    0/12672]\n",
      "loss: 6.477763  [ 1600/12672]\n",
      "loss: 23.765032  [ 3200/12672]\n",
      "loss: 26.161352  [ 4800/12672]\n",
      "loss: 19.729944  [ 6400/12672]\n",
      "loss: 12.784533  [ 8000/12672]\n",
      "loss: 11.198655  [ 9600/12672]\n",
      "loss: 14.248121  [11200/12672]\n",
      "AUC: 0.6407735475607477\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 6.800081  [    0/12672]\n",
      "loss: 11.521721  [ 1600/12672]\n",
      "loss: 15.068906  [ 3200/12672]\n",
      "loss: 13.363743  [ 4800/12672]\n",
      "loss: 11.782667  [ 6400/12672]\n",
      "loss: 14.989574  [ 8000/12672]\n",
      "loss: 9.711823  [ 9600/12672]\n",
      "loss: 13.627232  [11200/12672]\n",
      "AUC: 0.5808826205406327\n",
      "Done!\n",
      "##############################\n",
      "SESSION 20\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.742073  [    0/12672]\n",
      "loss: 10.795681  [ 1600/12672]\n",
      "loss: 29.025164  [ 3200/12672]\n",
      "loss: 26.210932  [ 4800/12672]\n",
      "loss: 20.090981  [ 6400/12672]\n",
      "loss: 5.686260  [ 8000/12672]\n",
      "loss: 10.445408  [ 9600/12672]\n",
      "loss: 19.235180  [11200/12672]\n",
      "AUC: 0.5511707120511635\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.263036  [    0/12672]\n",
      "loss: 15.763339  [ 1600/12672]\n",
      "loss: 18.038017  [ 3200/12672]\n",
      "loss: 11.033905  [ 4800/12672]\n",
      "loss: 18.832506  [ 6400/12672]\n",
      "loss: 9.413512  [ 8000/12672]\n",
      "loss: 15.712053  [ 9600/12672]\n",
      "loss: 15.037798  [11200/12672]\n",
      "AUC: 0.5414926527608345\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 32.584713  [    0/12672]\n",
      "loss: 5.107491  [ 1600/12672]\n",
      "loss: 17.233166  [ 3200/12672]\n",
      "loss: 8.382354  [ 4800/12672]\n",
      "loss: 32.215561  [ 6400/12672]\n",
      "loss: 13.023075  [ 8000/12672]\n",
      "loss: 21.509844  [ 9600/12672]\n",
      "loss: 7.523603  [11200/12672]\n",
      "AUC: 0.5997374428817738\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.018820  [    0/12672]\n",
      "loss: 10.549146  [ 1600/12672]\n",
      "loss: 8.349483  [ 3200/12672]\n",
      "loss: 23.267847  [ 4800/12672]\n",
      "loss: 13.845148  [ 6400/12672]\n",
      "loss: 17.596132  [ 8000/12672]\n",
      "loss: 12.305585  [ 9600/12672]\n",
      "loss: 9.763297  [11200/12672]\n",
      "AUC: 0.5628377613831064\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 23.650246  [    0/12672]\n",
      "loss: 12.153852  [ 1600/12672]\n",
      "loss: 9.465432  [ 3200/12672]\n",
      "loss: 19.502777  [ 4800/12672]\n",
      "loss: 21.790997  [ 6400/12672]\n",
      "loss: 5.837168  [ 8000/12672]\n",
      "loss: 28.245520  [ 9600/12672]\n",
      "loss: 10.439323  [11200/12672]\n",
      "AUC: 0.5384413549327793\n",
      "Done!\n",
      "##############################\n",
      "SESSION 21\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.703579  [    0/12672]\n",
      "loss: 22.976818  [ 1600/12672]\n",
      "loss: 5.451097  [ 3200/12672]\n",
      "loss: 16.276699  [ 4800/12672]\n",
      "loss: 14.262928  [ 6400/12672]\n",
      "loss: 21.433620  [ 8000/12672]\n",
      "loss: 10.388838  [ 9600/12672]\n",
      "loss: 13.879939  [11200/12672]\n",
      "AUC: 0.5528286892812918\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 22.853043  [    0/12672]\n",
      "loss: 13.238284  [ 1600/12672]\n",
      "loss: 14.926622  [ 3200/12672]\n",
      "loss: 23.929506  [ 4800/12672]\n",
      "loss: 15.794361  [ 6400/12672]\n",
      "loss: 17.297558  [ 8000/12672]\n",
      "loss: 17.949465  [ 9600/12672]\n",
      "loss: 15.136959  [11200/12672]\n",
      "AUC: 0.5660262844715034\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.567547  [    0/12672]\n",
      "loss: 7.418877  [ 1600/12672]\n",
      "loss: 8.845499  [ 3200/12672]\n",
      "loss: 8.947466  [ 4800/12672]\n",
      "loss: 10.178836  [ 6400/12672]\n",
      "loss: 10.535131  [ 8000/12672]\n",
      "loss: 11.708604  [ 9600/12672]\n",
      "loss: 15.671900  [11200/12672]\n",
      "AUC: 0.5695346460280842\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.591914  [    0/12672]\n",
      "loss: 10.509556  [ 1600/12672]\n",
      "loss: 12.913015  [ 3200/12672]\n",
      "loss: 12.845883  [ 4800/12672]\n",
      "loss: 22.408323  [ 6400/12672]\n",
      "loss: 7.251567  [ 8000/12672]\n",
      "loss: 15.759706  [ 9600/12672]\n",
      "loss: 11.776686  [11200/12672]\n",
      "AUC: 0.5677802344755744\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.756279  [    0/12672]\n",
      "loss: 9.326797  [ 1600/12672]\n",
      "loss: 17.060114  [ 3200/12672]\n",
      "loss: 7.099850  [ 4800/12672]\n",
      "loss: 14.486869  [ 6400/12672]\n",
      "loss: 16.963049  [ 8000/12672]\n",
      "loss: 11.804417  [ 9600/12672]\n",
      "loss: 14.137643  [11200/12672]\n",
      "AUC: 0.615191386862596\n",
      "Done!\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.704518  [    0/12672]\n",
      "loss: 14.180456  [ 1600/12672]\n",
      "loss: 5.579216  [ 3200/12672]\n",
      "loss: 13.938001  [ 4800/12672]\n",
      "loss: 23.388384  [ 6400/12672]\n",
      "loss: 11.858977  [ 8000/12672]\n",
      "loss: 5.564117  [ 9600/12672]\n",
      "loss: 16.060665  [11200/12672]\n",
      "AUC: 0.4884023785069235\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 25.287939  [    0/12672]\n",
      "loss: 5.674261  [ 1600/12672]\n",
      "loss: 19.797283  [ 3200/12672]\n",
      "loss: 19.422771  [ 4800/12672]\n",
      "loss: 17.421898  [ 6400/12672]\n",
      "loss: 21.076775  [ 8000/12672]\n",
      "loss: 27.898491  [ 9600/12672]\n",
      "loss: 9.631737  [11200/12672]\n",
      "AUC: 0.5265203020821959\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.503298  [    0/12672]\n",
      "loss: 15.635280  [ 1600/12672]\n",
      "loss: 12.097729  [ 3200/12672]\n",
      "loss: 14.796345  [ 4800/12672]\n",
      "loss: 10.826221  [ 6400/12672]\n",
      "loss: 21.155396  [ 8000/12672]\n",
      "loss: 26.664192  [ 9600/12672]\n",
      "loss: 23.859356  [11200/12672]\n",
      "AUC: 0.5655046994608162\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.552946  [    0/12672]\n",
      "loss: 14.106026  [ 1600/12672]\n",
      "loss: 11.053598  [ 3200/12672]\n",
      "loss: 11.584133  [ 4800/12672]\n",
      "loss: 18.247974  [ 6400/12672]\n",
      "loss: 22.289101  [ 8000/12672]\n",
      "loss: 7.206044  [ 9600/12672]\n",
      "loss: 11.851271  [11200/12672]\n",
      "AUC: 0.5883351585715242\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.859236  [    0/12672]\n",
      "loss: 17.981602  [ 1600/12672]\n",
      "loss: 17.057690  [ 3200/12672]\n",
      "loss: 10.990130  [ 4800/12672]\n",
      "loss: 21.159399  [ 6400/12672]\n",
      "loss: 4.491208  [ 8000/12672]\n",
      "loss: 18.213173  [ 9600/12672]\n",
      "loss: 27.528118  [11200/12672]\n",
      "AUC: 0.5309735271884\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "for i, (train_idx, test_idx) in enumerate(logo.split(X, y, groups=sessions)):\n",
    "    print(f\"{'#'*30}\\nSESSION {i}\\n{'#'*30}\")\n",
    "    ## create model ##\n",
    "    model = LogisticRegressionTorch(X.shape[-1])\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    weight_decay = 1e-4\n",
    "    lr = 5e-1\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "    ## data ##\n",
    "    train_set = SimpleDataset(X[train_idx], y[train_idx])\n",
    "    test_set = SimpleDataset(X[test_idx], y[test_idx])\n",
    "    ## class balancing ##\n",
    "    cls_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(train_set.y.detach().numpy()),\n",
    "        y=train_set.y.detach().numpy()\n",
    "    )\n",
    "    weights = cls_weights[train_set.y.detach().numpy().astype(int)]\n",
    "    sampler = WeightedRandomSampler(weights, len(train_set.y.detach().numpy()),\n",
    "                                    replacement=True)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_set, batch_size=200, sampler=sampler)\n",
    "    \n",
    "    ## training epochs ##\n",
    "    epochs = 5\n",
    "    for t in range(epochs):\n",
    "        print(f\"{'-'*30}\\nEpoch {t+1}\\n{'-'*30}\")\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, print_nth_batch=8)\n",
    "        out = test_auc_score(train_set, model)\n",
    "    \n",
    "    \n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d6e5d-6764-4dda-9216-e6fc426e9dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
