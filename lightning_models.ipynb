{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"torchvision.dataset\").setLevel(logging.ERROR)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "\n",
    "n_cpu = multiprocessing.cpu_count()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from typing import Any\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from numpy import dtype\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import BinaryAUROC\n",
    "\n",
    "### data location ###\n",
    "data_dir = \"/Users/jrudoler/data/small_scalp_features/\"\n",
    "log_dir = \"/Users/jrudoler/Library/CloudStorage/Box-Box/JR_CML/pytorch_logs/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precondition Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitPrecondition(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, learning_rate, weight_decay, batch_size):\n",
    "        super().__init__()\n",
    "        if output_dim is None:\n",
    "            output_dim = input_dim\n",
    "        self.condition = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=input_dim,\n",
    "                out_channels=2 * input_dim,\n",
    "                kernel_size=2,\n",
    "                padding=1,\n",
    "                groups=input_dim,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=4),\n",
    "            nn.Conv1d(\n",
    "                in_channels=2 * input_dim,\n",
    "                out_channels=2 * input_dim,\n",
    "                kernel_size=4,\n",
    "                padding=1,\n",
    "                groups=2 * input_dim,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            nn.Conv1d(\n",
    "                in_channels=2 * input_dim,\n",
    "                out_channels=input_dim,\n",
    "                kernel_size=4,\n",
    "                padding=1,\n",
    "                groups=input_dim,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=4),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.logistic = nn.Sequential(nn.Linear(output_dim, 1, bias=True), nn.Sigmoid())\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cond = self.condition(x)\n",
    "        probs = self.logistic(x_cond)\n",
    "        return probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.forward(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\n",
    "            \"Loss/train\",\n",
    "            loss,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "        auroc = BinaryAUROC()\n",
    "        train_auc = auroc(y_hat, y)\n",
    "        self.log(\"AUC/train\", train_auc, on_epoch=True, on_step=False, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.forward(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\"Loss/test\", loss, on_epoch=True, on_step=False, logger=True)\n",
    "        auroc = BinaryAUROC()\n",
    "        test_auc = auroc(y_hat, y)\n",
    "        self.log(\n",
    "            \"AUC/test\",\n",
    "            test_auc,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.forward(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        # self.log(\"Loss/test\", loss, on_epoch=True, on_step=False, logger=True)\n",
    "        auroc = BinaryAUROC()\n",
    "        test_auc = auroc(y_hat, y)\n",
    "        self.log(\n",
    "            \"AUC/test\",\n",
    "            test_auc,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams[\"learning_rate\"],\n",
    "            weight_decay=self.hparams[\"weight_decay\"],\n",
    "        )\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer=optimizer,\n",
    "                threshold=1e-4,\n",
    "                threshold_mode=\"rel\",\n",
    "                patience=10,\n",
    "                verbose=False,\n",
    "            ),\n",
    "            # The unit of the scheduler's step size, 'epoch' or 'step'.\n",
    "            \"interval\": \"epoch\",\n",
    "            # How many epochs/steps should pass between calls to\n",
    "            # `scheduler.step()`. 1 corresponds to updating the learning\n",
    "            # rate after every epoch/step.\n",
    "            \"frequency\": 1,\n",
    "            # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n",
    "            \"monitor\": \"Loss/train\",\n",
    "            # If set to `True`, will enforce that the value specified 'monitor'\n",
    "            # is available when the scheduler is updated\n",
    "            \"strict\": True,\n",
    "            # If using the `LearningRateMonitor` callback to monitor the\n",
    "            # learning rate progress, this keyword can be used to specify\n",
    "            # a custom logged name\n",
    "            \"name\": \"learning_rate\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 56\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LitPrecondition' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m## create model ##\u001b[39;00m\n\u001b[1;32m     61\u001b[0m n_features \u001b[39m=\u001b[39m train_dataset[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 62\u001b[0m model \u001b[39m=\u001b[39m LitPrecondition(\n\u001b[1;32m     63\u001b[0m     n_features, n_features, learning_rate, weight_decay, batch_size\n\u001b[1;32m     64\u001b[0m )\n\u001b[1;32m     65\u001b[0m es \u001b[39m=\u001b[39m EarlyStopping(\u001b[39m\"\u001b[39m\u001b[39mLoss/train\u001b[39m\u001b[39m\"\u001b[39m, min_delta\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m25\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m lr_mtr \u001b[39m=\u001b[39m LearningRateMonitor(\u001b[39m\"\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LitPrecondition' is not defined"
     ]
    }
   ],
   "source": [
    "### HYPERPARAMETERS ####\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 0.5\n",
    "batch_size = 256\n",
    "########################\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "_ = pl.seed_everything(56)\n",
    "subject = \"LTP093\"\n",
    "test_result = []\n",
    "for sess in range(24):\n",
    "    try:\n",
    "        test_file_crit = (\n",
    "            lambda s: s.endswith(\".pt\")\n",
    "            and s.count(f\"sub_{subject}\")\n",
    "            and s.count(f\"sess_{sess}\")\n",
    "        )\n",
    "        test_dataset = DatasetFolder(\n",
    "            data_dir,\n",
    "            loader=partial(torch.load),\n",
    "            is_valid_file=test_file_crit,\n",
    "        )\n",
    "        train_file_crit = (\n",
    "            lambda s: s.endswith(\".pt\")\n",
    "            and s.count(f\"sub_{subject}\")\n",
    "            and not s.count(f\"sess_{sess}\")\n",
    "        )\n",
    "        train_dataset = DatasetFolder(\n",
    "            data_dir,\n",
    "            loader=partial(torch.load),\n",
    "            is_valid_file=train_file_crit,\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"no session {sess}\")\n",
    "        test_result += [{\"subject\": subject, \"session\": sess}]\n",
    "        continue\n",
    "    ## class balancing ##\n",
    "    cls_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(train_dataset.targets),\n",
    "        y=train_dataset.targets,\n",
    "    )\n",
    "    weights = cls_weights[train_dataset.targets]\n",
    "    sampler = WeightedRandomSampler(weights, len(train_dataset), replacement=True)  # type: ignore\n",
    "    ## data loaders ##\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        pin_memory=True,\n",
    "        num_workers=n_cpu,\n",
    "        prefetch_factor=10,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=len(test_dataset),\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    ## create model ##\n",
    "    n_features = train_dataset[0][0].shape[0]\n",
    "    model = LitPrecondition(\n",
    "        n_features, n_features, learning_rate, weight_decay, batch_size\n",
    "    )\n",
    "    es = EarlyStopping(\"Loss/train\", min_delta=1e-3, patience=25, mode=\"min\")\n",
    "    lr_mtr = LearningRateMonitor(\"epoch\")\n",
    "    check = ModelCheckpoint(monitor=\"AUC/train\", mode=\"max\")\n",
    "    run_dir = f\"run_{subject}_{sess}_{timestr}\"\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=log_dir,\n",
    "        name=\"precondition\",\n",
    "        version=run_dir,\n",
    "        default_hp_metric=False,\n",
    "    )\n",
    "    logger.log_hyperparams(\n",
    "        {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"batch_size\": batch_size,\n",
    "        },\n",
    "        {\"AUC/train\": 0, \"Loss/train\": 0, \"AUC/test\": 0, \"Loss/test\": 0},\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        min_epochs=50,\n",
    "        max_epochs=500,\n",
    "        accelerator=\"mps\",\n",
    "        devices=1,\n",
    "        callbacks=[lr_mtr, es, check],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=5,\n",
    "    )\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=test_dataloader,\n",
    "    )\n",
    "    model = LitPrecondition.load_from_checkpoint(\n",
    "        trainer.checkpoint_callback.best_model_path  # type: ignore\n",
    "    )  # Load best checkpoint after training\n",
    "    test_result += trainer.test(model, dataloaders=test_dataloader, verbose=False)\n",
    "    test_result[-1].update({\"subject\": subject, \"session\": sess})\n",
    "    torch.mps.empty_cache()\n",
    "    result_df = pd.DataFrame(test_result)\n",
    "    result_df.to_csv(log_dir + f\"precond_results_LTP093_{timestr}.csv\")\n",
    "    result_df.to_csv(f\"precond_results_LTP093_{timestr}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitLogisticRegression(pl.LightningModule):\n",
    "    def __init__(self, input_dim, learning_rate, weight_decay, batch_size):\n",
    "        super().__init__()\n",
    "        self.logistic = nn.Sequential(nn.Linear(input_dim, 1, bias=True), nn.Sigmoid())\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, X):\n",
    "        probs = self.logistic(X)\n",
    "        return probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.forward(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\n",
    "            \"Loss/train\",\n",
    "            loss,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "        auroc = BinaryAUROC()\n",
    "        train_auc = auroc(y_hat, y)\n",
    "        self.log(\n",
    "            \"AUC/train\",\n",
    "            train_auc,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.forward(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\n",
    "            \"Loss/test\",\n",
    "            loss,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "        auroc = BinaryAUROC()\n",
    "        test_auc = auroc(y_hat, y)\n",
    "        self.log(\n",
    "            \"AUC/test\",\n",
    "            test_auc,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.forward(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\"Loss/test\", loss, on_epoch=True, on_step=False, logger=True)\n",
    "        auroc = BinaryAUROC()\n",
    "        test_auc = auroc(y_hat, y)\n",
    "        self.log(\n",
    "            \"AUC/test\",\n",
    "            test_auc,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams[\"learning_rate\"],\n",
    "            weight_decay=self.hparams[\"weight_decay\"],\n",
    "        )\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer=optimizer,\n",
    "                threshold=1e-4,\n",
    "                patience=10,\n",
    "                verbose=True,\n",
    "            ),\n",
    "            # The unit of the scheduler's step size, 'epoch' or 'step'.\n",
    "            \"interval\": \"epoch\",\n",
    "            # How many epochs/steps should pass between calls to\n",
    "            # `scheduler.step()`. 1 corresponds to updating the learning\n",
    "            # rate after every epoch/step.\n",
    "            \"frequency\": 1,\n",
    "            # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n",
    "            \"monitor\": \"Loss/train\",\n",
    "            # If set to `True`, will enforce that the value specified 'monitor'\n",
    "            # is available when the scheduler is updated\n",
    "            \"strict\": True,\n",
    "            # If using the `LearningRateMonitor` callback to monitor the\n",
    "            # learning rate progress, this keyword can be used to specify\n",
    "            # a custom logged name\n",
    "            \"name\": \"learning_rate\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HYPERPARAMETERS ####\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 1\n",
    "batch_size = 256\n",
    "########################\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "_ = pl.seed_everything(56)\n",
    "subject = \"LTP093\"\n",
    "test_result = []\n",
    "for sess in range(24):\n",
    "    try:\n",
    "        test_file_crit = (\n",
    "            lambda s: s.endswith(\".pt\")\n",
    "            and s.count(f\"sub_{subject}\")\n",
    "            and s.count(f\"sess_{sess}\")\n",
    "        )\n",
    "        test_dataset = DatasetFolder(\n",
    "            data_dir,\n",
    "            loader=partial(torch.load),\n",
    "            is_valid_file=test_file_crit,\n",
    "            transform=partial(torch.mean, dim=-1),\n",
    "        )\n",
    "        train_file_crit = (\n",
    "            lambda s: s.endswith(\".pt\")\n",
    "            and s.count(f\"sub_{subject}\")\n",
    "            and not s.count(f\"sess_{sess}\")\n",
    "        )\n",
    "        train_dataset = DatasetFolder(\n",
    "            data_dir,\n",
    "            loader=partial(torch.load),\n",
    "            is_valid_file=train_file_crit,\n",
    "            transform=partial(torch.mean, dim=-1),\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"no session {sess}\")\n",
    "        test_result += [{\"subject\": subject, \"session\": sess}]\n",
    "        continue\n",
    "    ## class balancing ##\n",
    "    cls_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(train_dataset.targets),\n",
    "        y=train_dataset.targets,\n",
    "    )\n",
    "    weights = cls_weights[train_dataset.targets]\n",
    "    sampler = WeightedRandomSampler(weights, len(train_dataset), replacement=True)  # type: ignore\n",
    "    ## data loaders ##\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        pin_memory=True,\n",
    "        num_workers=n_cpu,\n",
    "        prefetch_factor=10,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=len(test_dataset),\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    ## create model ##\n",
    "    n_features = train_dataset[0][0].shape[0]\n",
    "    model = LitLogisticRegression(n_features, learning_rate, weight_decay, batch_size)\n",
    "    es = EarlyStopping(\"Loss/train\", min_delta=0.0001, patience=25, mode=\"min\")\n",
    "    lr_mtr = LearningRateMonitor(\"epoch\")\n",
    "    check = ModelCheckpoint(monitor=\"AUC/train\", mode=\"max\")\n",
    "    run_dir = f\"run_{subject}_{sess}_{timestr}\"\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=log_dir,\n",
    "        name=\"logreg\",\n",
    "        version=run_dir,  # , default_hp_metric=False\n",
    "    )\n",
    "    logger.log_hyperparams(\n",
    "        {\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"batch_size\": batch_size,\n",
    "        },\n",
    "        {\"AUC/train\": 0, \"Loss/train\": 0, \"AUC/test\": 0, \"Loss/test\": 0},\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        min_epochs=50,\n",
    "        max_epochs=500,\n",
    "        accelerator=\"mps\",\n",
    "        devices=1,\n",
    "        callbacks=[lr_mtr, es, check],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=10,\n",
    "        # fast_dev_run=True\n",
    "    )\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=train_dataloader,\n",
    "        val_dataloaders=test_dataloader,\n",
    "    )\n",
    "    model = LitLogisticRegression.load_from_checkpoint(\n",
    "        trainer.checkpoint_callback.best_model_path  # type: ignore\n",
    "    )  # Load best checkpoint after training\n",
    "    test_result += trainer.test(model, dataloaders=test_dataloader, verbose=False)\n",
    "    test_result[-1].update({\"subject\": subject, \"session\": sess})\n",
    "    torch.mps.empty_cache()\n",
    "    result_df = pd.DataFrame(test_result)\n",
    "    result_df.to_csv(log_dir + f\"logreg_results_LTP093_{timestr}.csv\")\n",
    "    result_df.to_csv(f\"logreg_results_LTP093_{timestr}.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\n",
    "    \"/Users/jrudoler/Library/CloudStorage/Box-Box/JR_CML/pytorch_logs/models/foundation_model_20230427-152237.pt\"\n",
    ").cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"batch_size\":    256\n",
       "\"input_dim\":     992\n",
       "\"learning_rate\": 0.01\n",
       "\"output_dim\":    992\n",
       "\"weight_decay\":  0.5"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams_initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.hparams[\"batch_size\"] = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"batch_size\":    256\n",
       "\"input_dim\":     992\n",
       "\"learning_rate\": 0.01\n",
       "\"output_dim\":    992\n",
       "\"weight_decay\":  0.5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
