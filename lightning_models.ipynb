{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 56\n"
     ]
    }
   ],
   "source": [
    "# import warnings\n",
    "# warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"torchvision.dataset\").setLevel(logging.ERROR)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "\n",
    "n_cpu = multiprocessing.cpu_count()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from typing import Any\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from numpy import dtype\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import BinaryAUROC\n",
    "\n",
    "_ = pl.seed_everything(56)\n",
    "### data location ###\n",
    "data_dir = \"/Users/jrudoler/data/small_scalp_features/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallerPrecondFeatLogisticRegressionTorch(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=None):\n",
    "        super().__init__()\n",
    "        if output_dim is None:\n",
    "            output_dim = input_dim\n",
    "        self.condition = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=input_dim,\n",
    "                out_channels=2 * input_dim,\n",
    "                kernel_size=2,\n",
    "                padding=1,\n",
    "                groups=input_dim,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=4),\n",
    "            nn.Conv1d(\n",
    "                in_channels=2 * input_dim,\n",
    "                out_channels=2 * input_dim,\n",
    "                kernel_size=4,\n",
    "                padding=1,\n",
    "                groups=2 * input_dim,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            nn.Conv1d(\n",
    "                in_channels=2 * input_dim,\n",
    "                out_channels=input_dim,\n",
    "                kernel_size=4,\n",
    "                padding=1,\n",
    "                groups=input_dim,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=4),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.logistic = nn.Sequential(nn.Linear(output_dim, 1, bias=True), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cond = self.condition(x)\n",
    "        probs = self.logistic(x_cond)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HYPERPARAMETERS ####\n",
    "lr = 1e-2\n",
    "weight_decay = 1e-4  # 1e-4\n",
    "batch_size = 256\n",
    "########################\n",
    "\n",
    "\n",
    "class LitPrecondition(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *args: Any, **kwargs: Any):\n",
    "        return self.model.forward(*args, **kwargs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.model(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\n",
    "            \"Loss/train\", loss, on_epoch=True, on_step=False, prog_bar=True, logger=True\n",
    "        )\n",
    "        auroc = BinaryAUROC()\n",
    "        train_auc = auroc(y_hat, y)\n",
    "        self.log(\"AUC/train\", train_auc, on_epoch=True, on_step=False, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.model(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\"Loss/test\", loss, on_epoch=True, on_step=False, logger=True)\n",
    "        auroc = BinaryAUROC()\n",
    "        test_auc = auroc(y_hat, y)\n",
    "        self.log(\n",
    "            \"AUC/test\",\n",
    "            test_auc,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        self.logger.log_hyperparams(\n",
    "            {\n",
    "                \"learning_rate\": lr,\n",
    "                \"weight_decay\": weight_decay,\n",
    "                \"batch_size\": batch_size,\n",
    "            }\n",
    "        )\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer=optimizer, threshold=1e-3\n",
    "            ),\n",
    "            # The unit of the scheduler's step size, 'epoch' or 'step'.\n",
    "            \"interval\": \"epoch\",\n",
    "            # How many epochs/steps should pass between calls to\n",
    "            # `scheduler.step()`. 1 corresponds to updating the learning\n",
    "            # rate after every epoch/step.\n",
    "            \"frequency\": 1,\n",
    "            # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n",
    "            \"monitor\": \"Loss/train\",\n",
    "            # If set to `True`, will enforce that the value specified 'monitor'\n",
    "            # is available when the scheduler is updated\n",
    "            \"strict\": True,\n",
    "            # If using the `LearningRateMonitor` callback to monitor the\n",
    "            # learning rate progress, this keyword can be used to specify\n",
    "            # a custom logged name\n",
    "            \"name\": None,\n",
    "        }\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type                                      | Params\n",
      "--------------------------------------------------------------------\n",
      "0 | model | SmallerPrecondFeatLogisticRegressionTorch | 25.8 K\n",
      "--------------------------------------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f62e226ed04256b89bb891b81d3166",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=150` reached.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f0605307844efc956624a2aebce44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "subject = \"LTP093\"\n",
    "sess = 5\n",
    "for sess in [5]:\n",
    "    try:\n",
    "        test_file_crit = (\n",
    "            lambda s: s.endswith(\".pt\")\n",
    "            and s.count(f\"sub_{subject}\")\n",
    "            and s.count(f\"sess_{sess}\")\n",
    "        )\n",
    "        test_dataset = DatasetFolder(\n",
    "            data_dir,\n",
    "            loader=partial(torch.load),\n",
    "            is_valid_file=test_file_crit,\n",
    "        )\n",
    "        train_file_crit = (\n",
    "            lambda s: s.endswith(\".pt\")\n",
    "            and s.count(f\"sub_{subject}\")\n",
    "            and not s.count(f\"sess_{sess}\")\n",
    "        )\n",
    "        train_dataset = DatasetFolder(\n",
    "            data_dir,\n",
    "            loader=partial(torch.load),\n",
    "            is_valid_file=train_file_crit,\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"no session {sess}\")\n",
    "        continue\n",
    "    ## class balancing ##\n",
    "    cls_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(train_dataset.targets),\n",
    "        y=train_dataset.targets,\n",
    "    )\n",
    "    weights = cls_weights[train_dataset.targets]\n",
    "    sampler = WeightedRandomSampler(weights, len(train_dataset), replacement=True)\n",
    "    ## data loaders ##\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        pin_memory=True,\n",
    "        num_workers=n_cpu,\n",
    "        prefetch_factor=10,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=len(test_dataset), shuffle=False, pin_memory=True\n",
    "    )\n",
    "    ## create model ##\n",
    "    n_features = train_dataset[0][0].shape[0]\n",
    "    model = LitPrecondition(SmallerPrecondFeatLogisticRegressionTorch(n_features))\n",
    "    es = EarlyStopping(\"AUC/train\", min_delta=0.01, patience=15)\n",
    "    trainer = Trainer(min_epochs=10, max_epochs=150, accelerator=\"mps\", devices=1)\n",
    "    trainer.logger._default_hp_metric = None\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader)\n",
    "    test_result = trainer.test(model, dataloaders=test_dataloader, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss/test</th>\n",
       "      <th>AUC/test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.695903</td>\n",
       "      <td>0.431641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Loss/test  AUC/test\n",
       "0   0.695903  0.431641"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
