{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"torchvision.dataset\").setLevel(logging.ERROR)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "\n",
    "n_cpu = multiprocessing.cpu_count()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from typing import Any\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from numpy import dtype\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import BinaryAUROC\n",
    "\n",
    "### data location ###\n",
    "data_dir = \"/Users/jrudoler/data/small_scalp_features/\"\n",
    "log_dir = \"/Users/jrudoler/Library/CloudStorage/Box-Box/JR_CML/pytorch_logs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitPrecondition(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, learning_rate, weight_decay, batch_size):\n",
    "        super().__init__()\n",
    "        if output_dim is None:\n",
    "            output_dim = input_dim\n",
    "        self.condition = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=input_dim,\n",
    "                out_channels=2 * input_dim,\n",
    "                kernel_size=2,\n",
    "                padding=1,\n",
    "                groups=input_dim,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=4),\n",
    "            nn.Conv1d(\n",
    "                in_channels=2 * input_dim,\n",
    "                out_channels=2 * input_dim,\n",
    "                kernel_size=4,\n",
    "                padding=1,\n",
    "                groups=2 * input_dim,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            nn.Conv1d(\n",
    "                in_channels=2 * input_dim,\n",
    "                out_channels=input_dim,\n",
    "                kernel_size=4,\n",
    "                padding=1,\n",
    "                groups=input_dim,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=4),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.logistic = nn.Sequential(nn.Linear(output_dim, 1, bias=True), nn.Sigmoid())\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cond = self.condition(x)\n",
    "        probs = self.logistic(x_cond)\n",
    "        return probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.forward(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\n",
    "            \"Loss/train\", loss, on_epoch=True, on_step=False, prog_bar=True, logger=True\n",
    "        )\n",
    "        auroc = BinaryAUROC()\n",
    "        train_auc = auroc(y_hat, y)\n",
    "        self.log(\"AUC/train\", train_auc, on_epoch=True, on_step=False, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.forward(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\"Loss/test\", loss, on_epoch=True, on_step=False, logger=True)\n",
    "        auroc = BinaryAUROC()\n",
    "        test_auc = auroc(y_hat, y)\n",
    "        self.log(\n",
    "            \"AUC/test\",\n",
    "            test_auc,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        self.logger.log_hyperparams(\n",
    "            {\n",
    "                \"learning_rate\": self.hparams[\"learning_rate\"],\n",
    "                \"weight_decay\": self.hparams[\"weight_decay\"],\n",
    "                \"batch_size\": self.hparams[\"batch_size\"],\n",
    "            }\n",
    "        )\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams[\"learning_rate\"],\n",
    "            weight_decay=self.hparams[\"weight_decay\"],\n",
    "        )\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer=optimizer, threshold=1e-4\n",
    "            ),\n",
    "            # The unit of the scheduler's step size, 'epoch' or 'step'.\n",
    "            \"interval\": \"epoch\",\n",
    "            # How many epochs/steps should pass between calls to\n",
    "            # `scheduler.step()`. 1 corresponds to updating the learning\n",
    "            # rate after every epoch/step.\n",
    "            \"frequency\": 1,\n",
    "            # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n",
    "            \"monitor\": \"Loss/train\",\n",
    "            # If set to `True`, will enforce that the value specified 'monitor'\n",
    "            # is available when the scheduler is updated\n",
    "            \"strict\": True,\n",
    "            # If using the `LearningRateMonitor` callback to monitor the\n",
    "            # learning rate progress, this keyword can be used to specify\n",
    "            # a custom logged name\n",
    "            \"name\": None,\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78: 100%|██████████| 13/13 [00:01<00:00, 12.13it/s, v_num=0155, Loss/train=0.690]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jrudoler/miniconda3/envs/torch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 7/7 [00:00<00:00, 10.63it/s, v_num=0155, Loss/train=0.693]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:03<00:00,  3.05s/it]\n",
      "Epoch 63: 100%|██████████| 11/11 [00:01<00:00, 10.75it/s, v_num=0155, Loss/train=0.691]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  2.34it/s]\n",
      "Epoch 151: 100%|██████████| 13/13 [00:01<00:00, 11.75it/s, v_num=0155, Loss/train=0.686]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 16.90it/s]\n",
      "Epoch 88: 100%|██████████| 13/13 [00:01<00:00, 11.49it/s, v_num=0155, Loss/train=0.690]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 16.79it/s]\n",
      "Epoch 65: 100%|██████████| 13/13 [00:01<00:00, 11.77it/s, v_num=0155, Loss/train=0.692]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 17.19it/s]\n",
      "Epoch 110: 100%|██████████| 13/13 [00:01<00:00, 11.68it/s, v_num=0155, Loss/train=0.689]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 17.74it/s]\n",
      "Epoch 98: 100%|██████████| 13/13 [00:01<00:00, 11.80it/s, v_num=0155, Loss/train=0.691]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 16.67it/s]\n",
      "Epoch 104: 100%|██████████| 13/13 [00:01<00:00, 11.99it/s, v_num=0155, Loss/train=0.689]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 17.67it/s]\n",
      "Epoch 100: 100%|██████████| 13/13 [00:01<00:00, 11.76it/s, v_num=0155, Loss/train=0.691]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 18.83it/s]\n",
      "Epoch 71: 100%|██████████| 13/13 [00:01<00:00, 12.01it/s, v_num=0155, Loss/train=0.692]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 19.75it/s]\n",
      "Epoch 100: 100%|██████████| 13/13 [00:01<00:00, 11.97it/s, v_num=0155, Loss/train=0.692]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 18.44it/s]\n",
      "Epoch 115: 100%|██████████| 13/13 [00:01<00:00, 11.99it/s, v_num=0155, Loss/train=0.690]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  9.81it/s]\n",
      "Epoch 71: 100%|██████████| 13/13 [00:01<00:00, 11.68it/s, v_num=0155, Loss/train=0.691]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 19.26it/s]\n",
      "Epoch 88: 100%|██████████| 13/13 [00:01<00:00, 12.04it/s, v_num=0155, Loss/train=0.691]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 19.13it/s]\n",
      "Epoch 126: 100%|██████████| 13/13 [00:01<00:00, 11.70it/s, v_num=0155, Loss/train=0.689]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 18.58it/s]\n",
      "Epoch 93: 100%|██████████| 13/13 [00:01<00:00, 11.95it/s, v_num=0155, Loss/train=0.689]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 18.16it/s]\n",
      "Epoch 69: 100%|██████████| 13/13 [00:01<00:00, 12.15it/s, v_num=0155, Loss/train=0.691]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 18.67it/s]\n",
      "Epoch 49: 100%|██████████| 13/13 [00:01<00:00, 11.84it/s, v_num=0155, Loss/train=0.693]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 18.71it/s]\n",
      "Epoch 86: 100%|██████████| 13/13 [00:01<00:00, 12.00it/s, v_num=0155, Loss/train=0.692]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 17.95it/s]\n",
      "Epoch 130: 100%|██████████| 13/13 [00:01<00:00, 12.39it/s, v_num=0155, Loss/train=0.691]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 18.44it/s]\n",
      "no session 21\n",
      "Epoch 112: 100%|██████████| 13/13 [00:01<00:00, 11.45it/s, v_num=0155, Loss/train=0.690]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 17.97it/s]\n",
      "Epoch 49: 100%|██████████| 13/13 [00:01<00:00, 11.84it/s, v_num=0155, Loss/train=0.693]\n",
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 19.61it/s]\n"
     ]
    }
   ],
   "source": [
    "### HYPERPARAMETERS ####\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-4  # 1e-4\n",
    "batch_size = 256\n",
    "########################\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "_ = pl.seed_everything(56)\n",
    "subject = \"LTP093\"\n",
    "test_result = []\n",
    "for sess in range(24):\n",
    "    try:\n",
    "        test_file_crit = (\n",
    "            lambda s: s.endswith(\".pt\")\n",
    "            and s.count(f\"sub_{subject}\")\n",
    "            and s.count(f\"sess_{sess}\")\n",
    "        )\n",
    "        test_dataset = DatasetFolder(\n",
    "            data_dir,\n",
    "            loader=partial(torch.load),\n",
    "            is_valid_file=test_file_crit,\n",
    "        )\n",
    "        train_file_crit = (\n",
    "            lambda s: s.endswith(\".pt\")\n",
    "            and s.count(f\"sub_{subject}\")\n",
    "            and not s.count(f\"sess_{sess}\")\n",
    "        )\n",
    "        train_dataset = DatasetFolder(\n",
    "            data_dir,\n",
    "            loader=partial(torch.load),\n",
    "            is_valid_file=train_file_crit,\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"no session {sess}\")\n",
    "        continue\n",
    "    ## class balancing ##\n",
    "    cls_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(train_dataset.targets),\n",
    "        y=train_dataset.targets,\n",
    "    )\n",
    "    weights = cls_weights[train_dataset.targets]\n",
    "    sampler = WeightedRandomSampler(weights, len(train_dataset), replacement=True)  # type: ignore\n",
    "    ## data loaders ##\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        pin_memory=True,\n",
    "        num_workers=n_cpu,\n",
    "        prefetch_factor=10,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=len(test_dataset),\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=n_cpu,\n",
    "    )\n",
    "    ## create model ##\n",
    "    n_features = train_dataset[0][0].shape[0]\n",
    "    model = LitPrecondition(\n",
    "        n_features, n_features, learning_rate, weight_decay, batch_size\n",
    "    )\n",
    "    es = EarlyStopping(\"Loss/train\", min_delta=0.0001, patience=25, mode=\"min\")\n",
    "    check = ModelCheckpoint(monitor=\"AUC/train\", mode=\"max\")\n",
    "    run_dir = f\"run_{subject}_{sess}_{timestr}\"\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=log_dir, name=\"precondition\", version=run_dir, default_hp_metric=False\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        min_epochs=50,\n",
    "        max_epochs=200,\n",
    "        accelerator=\"mps\",\n",
    "        devices=1,\n",
    "        callbacks=[es, check],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=10,\n",
    "    )\n",
    "    # trainer.logger._default_hp_metric = None\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader)\n",
    "    model = LitPrecondition.load_from_checkpoint(\n",
    "        trainer.checkpoint_callback.best_model_path  # type: ignore\n",
    "    )  # Load best checkpoint after training\n",
    "    test_result += trainer.test(model, dataloaders=test_dataloader, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"precond_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
