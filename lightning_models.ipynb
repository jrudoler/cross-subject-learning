{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"torchvision.dataset\").setLevel(logging.ERROR)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "\n",
    "n_cpu = multiprocessing.cpu_count()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from typing import Any\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from numpy import dtype\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import BinaryAUROC\n",
    "\n",
    "### data location ###\n",
    "data_dir = \"/Users/jrudoler/data/small_scalp_features/\"\n",
    "log_dir = \"/Users/jrudoler/Library/CloudStorage/Box-Box/JR_CML/pytorch_logs/\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precondition Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitPrecondition(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, learning_rate, weight_decay, batch_size):\n",
    "        super().__init__()\n",
    "        if output_dim is None:\n",
    "            output_dim = input_dim\n",
    "        self.condition = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=input_dim,\n",
    "                out_channels=2 * input_dim,\n",
    "                kernel_size=2,\n",
    "                padding=1,\n",
    "                groups=input_dim,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=4),\n",
    "            nn.Conv1d(\n",
    "                in_channels=2 * input_dim,\n",
    "                out_channels=2 * input_dim,\n",
    "                kernel_size=4,\n",
    "                padding=1,\n",
    "                groups=2 * input_dim,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            nn.Conv1d(\n",
    "                in_channels=2 * input_dim,\n",
    "                out_channels=input_dim,\n",
    "                kernel_size=4,\n",
    "                padding=1,\n",
    "                groups=input_dim,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=4),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.logistic = nn.Sequential(nn.Linear(output_dim, 1, bias=True), nn.Sigmoid())\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cond = self.condition(x)\n",
    "        probs = self.logistic(x_cond)\n",
    "        return probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.forward(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\n",
    "            \"Loss/train\", loss, on_epoch=True, on_step=False, prog_bar=True, logger=True\n",
    "        )\n",
    "        auroc = BinaryAUROC()\n",
    "        train_auc = auroc(y_hat, y)\n",
    "        self.log(\"AUC/train\", train_auc, on_epoch=True, on_step=False, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.forward(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\"Loss/test\", loss, on_epoch=True, on_step=False, logger=True)\n",
    "        auroc = BinaryAUROC()\n",
    "        test_auc = auroc(y_hat, y)\n",
    "        self.log(\n",
    "            \"AUC/test\",\n",
    "            test_auc,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.forward(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\"Loss/test\", loss, on_epoch=True, on_step=False, logger=True)\n",
    "        auroc = BinaryAUROC()\n",
    "        test_auc = auroc(y_hat, y)\n",
    "        self.log(\n",
    "            \"AUC/test\",\n",
    "            test_auc,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        self.logger.log_hyperparams(\n",
    "            {\n",
    "                \"learning_rate\": self.hparams[\"learning_rate\"],\n",
    "                \"weight_decay\": self.hparams[\"weight_decay\"],\n",
    "                \"batch_size\": self.hparams[\"batch_size\"],\n",
    "            }\n",
    "        )\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams[\"learning_rate\"],\n",
    "            weight_decay=self.hparams[\"weight_decay\"],\n",
    "        )\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer=optimizer,\n",
    "                threshold=1e-4,\n",
    "                threshold_mode=\"rel\",\n",
    "                patience=10,\n",
    "                verbose=False,\n",
    "            ),\n",
    "            # The unit of the scheduler's step size, 'epoch' or 'step'.\n",
    "            \"interval\": \"epoch\",\n",
    "            # How many epochs/steps should pass between calls to\n",
    "            # `scheduler.step()`. 1 corresponds to updating the learning\n",
    "            # rate after every epoch/step.\n",
    "            \"frequency\": 1,\n",
    "            # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n",
    "            \"monitor\": \"Loss/train\",\n",
    "            # If set to `True`, will enforce that the value specified 'monitor'\n",
    "            # is available when the scheduler is updated\n",
    "            \"strict\": True,\n",
    "            # If using the `LearningRateMonitor` callback to monitor the\n",
    "            # learning rate progress, this keyword can be used to specify\n",
    "            # a custom logged name\n",
    "            \"name\": \"learning_rate\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 56\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | condition | Sequential | 24.8 K\n",
      "1 | logistic  | Sequential | 993   \n",
      "-----------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jrudoler/miniconda3/envs/torch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|██████████| 13/13 [00:02<00:00,  5.10it/s, v_num=1004, Loss/train=0.654]Epoch 00059: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 69: 100%|██████████| 13/13 [00:01<00:00, 11.18it/s, v_num=1004, Loss/train=0.648]Epoch 00070: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 73:   0%|          | 0/13 [00:00<?, ?it/s, v_num=1004, Loss/train=0.647]         "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer was signaled to stop but the required `min_epochs=75` or `min_steps=None` has not been met. Training will continue...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74: 100%|██████████| 13/13 [00:01<00:00, 11.17it/s, v_num=1004, Loss/train=0.650]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jrudoler/miniconda3/envs/torch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 11.92it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | condition | Sequential | 24.8 K\n",
      "1 | logistic  | Sequential | 993   \n",
      "-----------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jrudoler/miniconda3/envs/torch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|██████████| 7/7 [00:06<00:00,  1.05it/s, v_num=1004, Loss/train=0.633]Epoch 00046: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 62: 100%|██████████| 7/7 [00:01<00:00,  4.90it/s, v_num=1004, Loss/train=0.634]Epoch 00063: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 94: 100%|██████████| 7/7 [00:01<00:00,  5.05it/s, v_num=1004, Loss/train=0.628]Epoch 00095: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 105: 100%|██████████| 7/7 [00:01<00:00,  5.00it/s, v_num=1004, Loss/train=0.624]Epoch 00106: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 108: 100%|██████████| 7/7 [00:01<00:00,  4.72it/s, v_num=1004, Loss/train=0.630]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jrudoler/miniconda3/envs/torch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | condition | Sequential | 24.8 K\n",
      "1 | logistic  | Sequential | 993   \n",
      "-----------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jrudoler/miniconda3/envs/torch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|██████████| 11/11 [00:01<00:00,  8.63it/s, v_num=1004, Loss/train=0.644]Epoch 00061: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 87: 100%|██████████| 11/11 [00:01<00:00,  8.65it/s, v_num=1004, Loss/train=0.642]Epoch 00088: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 98: 100%|██████████| 11/11 [00:01<00:00,  7.71it/s, v_num=1004, Loss/train=0.643]Epoch 00099: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 101: 100%|██████████| 11/11 [00:01<00:00,  9.00it/s, v_num=1004, Loss/train=0.632]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jrudoler/miniconda3/envs/torch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  8.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | condition | Sequential | 24.8 K\n",
      "1 | logistic  | Sequential | 993   \n",
      "-----------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jrudoler/miniconda3/envs/torch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70: 100%|██████████| 13/13 [00:01<00:00, 11.48it/s, v_num=1004, Loss/train=0.658]Epoch 00071: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 74: 100%|██████████| 13/13 [00:01<00:00, 10.78it/s, v_num=1004, Loss/train=0.656]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jrudoler/miniconda3/envs/torch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 26.47it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type       | Params\n",
      "-----------------------------------------\n",
      "0 | condition | Sequential | 24.8 K\n",
      "1 | logistic  | Sequential | 993   \n",
      "-----------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jrudoler/miniconda3/envs/torch/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42:  15%|█▌        | 2/13 [00:00<00:02,  5.28it/s, v_num=1004, Loss/train=0.661] "
     ]
    }
   ],
   "source": [
    "### HYPERPARAMETERS ####\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 0.5  # 1e-4\n",
    "batch_size = 256\n",
    "########################\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "_ = pl.seed_everything(56)\n",
    "subject = \"LTP093\"\n",
    "test_result = []\n",
    "for sess in range(24):\n",
    "    try:\n",
    "        test_file_crit = (\n",
    "            lambda s: s.endswith(\".pt\")\n",
    "            and s.count(f\"sub_{subject}\")\n",
    "            and s.count(f\"sess_{sess}\")\n",
    "        )\n",
    "        test_dataset = DatasetFolder(\n",
    "            data_dir,\n",
    "            loader=partial(torch.load),\n",
    "            is_valid_file=test_file_crit,\n",
    "        )\n",
    "        train_file_crit = (\n",
    "            lambda s: s.endswith(\".pt\")\n",
    "            and s.count(f\"sub_{subject}\")\n",
    "            and not s.count(f\"sess_{sess}\")\n",
    "        )\n",
    "        train_dataset = DatasetFolder(\n",
    "            data_dir,\n",
    "            loader=partial(torch.load),\n",
    "            is_valid_file=train_file_crit,\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"no session {sess}\")\n",
    "        test_result += [{\"subject\": subject, \"session\": sess}]\n",
    "        continue\n",
    "    ## class balancing ##\n",
    "    cls_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(train_dataset.targets),\n",
    "        y=train_dataset.targets,\n",
    "    )\n",
    "    weights = cls_weights[train_dataset.targets]\n",
    "    sampler = WeightedRandomSampler(weights, len(train_dataset), replacement=True)  # type: ignore\n",
    "    ## data loaders ##\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        pin_memory=True,\n",
    "        num_workers=n_cpu,\n",
    "        prefetch_factor=10,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=len(test_dataset),\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    ## create model ##\n",
    "    n_features = train_dataset[0][0].shape[0]\n",
    "    model = LitPrecondition(\n",
    "        n_features, n_features, learning_rate, weight_decay, batch_size\n",
    "    )\n",
    "    es = EarlyStopping(\"Loss/train\", min_delta=1e-3, patience=25, mode=\"min\")\n",
    "    lr_mtr = LearningRateMonitor(\"epoch\")\n",
    "    check = ModelCheckpoint(monitor=\"AUC/train\", mode=\"max\")\n",
    "    run_dir = f\"run_{subject}_{sess}_{timestr}\"\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=log_dir, name=\"precondition\", version=run_dir, default_hp_metric=False\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        min_epochs=75,\n",
    "        max_epochs=500,\n",
    "        accelerator=\"mps\",\n",
    "        devices=1,\n",
    "        callbacks=[lr_mtr, es, check],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=5,\n",
    "    )\n",
    "    trainer.fit(\n",
    "        model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader\n",
    "    )\n",
    "    model = LitPrecondition.load_from_checkpoint(\n",
    "        trainer.checkpoint_callback.best_model_path  # type: ignore\n",
    "    )  # Load best checkpoint after training\n",
    "    test_result += trainer.test(model, dataloaders=test_dataloader, verbose=False)\n",
    "    test_result[-1].update({\"subject\": subject, \"session\": sess})\n",
    "result_df = pd.DataFrame(test_result)\n",
    "result_df.to_csv(log_dir + \"precond_results_LTP093.csv\")\n",
    "result_df.to_csv(\"precond_results_LTP093.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitLogisticRegression(pl.LightningModule):\n",
    "    def __init__(self, input_dim, learning_rate, weight_decay, batch_size):\n",
    "        super().__init__()\n",
    "        self.logistic = nn.Sequential(nn.Linear(input_dim, 1, bias=True), nn.Sigmoid())\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, X):\n",
    "        probs = self.logistic(X)\n",
    "        return probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.forward(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\n",
    "            \"Loss/train\", loss, on_epoch=True, on_step=False, prog_bar=True, logger=True\n",
    "        )\n",
    "        auroc = BinaryAUROC()\n",
    "        train_auc = auroc(y_hat, y)\n",
    "        self.log(\"AUC/train\", train_auc, on_epoch=True, on_step=False, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.forward(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\"Loss/test\", loss, on_epoch=True, on_step=False, logger=True)\n",
    "        auroc = BinaryAUROC()\n",
    "        test_auc = auroc(y_hat, y)\n",
    "        self.log(\n",
    "            \"AUC/test\",\n",
    "            test_auc,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        self.logger.log_hyperparams(\n",
    "            {\n",
    "                \"learning_rate\": self.hparams[\"learning_rate\"],\n",
    "                \"weight_decay\": self.hparams[\"weight_decay\"],\n",
    "                \"batch_size\": self.hparams[\"batch_size\"],\n",
    "            }\n",
    "        )\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams[\"learning_rate\"],\n",
    "            weight_decay=self.hparams[\"weight_decay\"],\n",
    "        )\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer=optimizer,\n",
    "                threshold=1e-4,\n",
    "                patience=25,\n",
    "                verbose=True,\n",
    "            ),\n",
    "            # The unit of the scheduler's step size, 'epoch' or 'step'.\n",
    "            \"interval\": \"epoch\",\n",
    "            # How many epochs/steps should pass between calls to\n",
    "            # `scheduler.step()`. 1 corresponds to updating the learning\n",
    "            # rate after every epoch/step.\n",
    "            \"frequency\": 1,\n",
    "            # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n",
    "            \"monitor\": \"Loss/train\",\n",
    "            # If set to `True`, will enforce that the value specified 'monitor'\n",
    "            # is available when the scheduler is updated\n",
    "            \"strict\": True,\n",
    "            # If using the `LearningRateMonitor` callback to monitor the\n",
    "            # learning rate progress, this keyword can be used to specify\n",
    "            # a custom logged name\n",
    "            \"name\": \"learning_rate\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "_ = pl.seed_everything(56)\n",
    "subject = \"LTP093\"\n",
    "test_result = []\n",
    "for sess in range(24):\n",
    "    try:\n",
    "        test_file_crit = (\n",
    "            lambda s: s.endswith(\".pt\")\n",
    "            and s.count(f\"sub_{subject}\")\n",
    "            and s.count(f\"sess_{sess}\")\n",
    "        )\n",
    "        test_dataset = DatasetFolder(\n",
    "            data_dir,\n",
    "            loader=partial(torch.load),\n",
    "            is_valid_file=test_file_crit,\n",
    "        )\n",
    "        train_file_crit = (\n",
    "            lambda s: s.endswith(\".pt\")\n",
    "            and s.count(f\"sub_{subject}\")\n",
    "            and not s.count(f\"sess_{sess}\")\n",
    "        )\n",
    "        train_dataset = DatasetFolder(\n",
    "            data_dir,\n",
    "            loader=partial(torch.load),\n",
    "            is_valid_file=train_file_crit,\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"no session {sess}\")\n",
    "        test_result += [{\"subject\": subject, \"session\": sess}]\n",
    "        continue\n",
    "    ## class balancing ##\n",
    "    cls_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(train_dataset.targets),\n",
    "        y=train_dataset.targets,\n",
    "    )\n",
    "    weights = cls_weights[train_dataset.targets]\n",
    "    sampler = WeightedRandomSampler(weights, len(train_dataset), replacement=True)  # type: ignore\n",
    "    ## data loaders ##\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        pin_memory=True,\n",
    "        num_workers=n_cpu,\n",
    "        prefetch_factor=10,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=len(test_dataset),\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    ### HYPERPARAMETERS ####\n",
    "    learning_rate = 1e-2\n",
    "    weight_decay = 1e-4  # 1e-4\n",
    "    batch_size = 256\n",
    "    ########################\n",
    "    ## create model ##\n",
    "    n_features = train_dataset[0][0].shape[0]\n",
    "    model = LitLogisticRegression(n_features, learning_rate, weight_decay, batch_size)\n",
    "    es = EarlyStopping(\"Loss/train\", min_delta=0.0001, patience=50, mode=\"min\")\n",
    "    lr_mtr = LearningRateMonitor(\"epoch\")\n",
    "    check = ModelCheckpoint(monitor=\"AUC/train\", mode=\"max\")\n",
    "    run_dir = f\"run_{subject}_{sess}_{timestr}\"\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=log_dir, name=\"precondition\", version=run_dir, default_hp_metric=False\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        min_epochs=75,\n",
    "        max_epochs=500,\n",
    "        accelerator=\"mps\",\n",
    "        devices=1,\n",
    "        callbacks=[lr_mtr, es, check],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=10,\n",
    "    )\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader)\n",
    "    model = LitPrecondition.load_from_checkpoint(\n",
    "        trainer.checkpoint_callback.best_model_path  # type: ignore\n",
    "    )  # Load best checkpoint after training\n",
    "    test_result += trainer.test(model, dataloaders=test_dataloader, verbose=False)\n",
    "    test_result[-1].update({\"subject\": subject, \"session\": sess})\n",
    "result_df = pd.DataFrame(test_result)\n",
    "result_df.to_csv(log_dir + \"logreg_results_LTP093.csv\")\n",
    "result_df.to_csv(\"logreg_results_LTP093.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
