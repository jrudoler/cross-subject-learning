{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"lightning.pytorch\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"torchvision.dataset\").setLevel(logging.ERROR)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "\n",
    "n_cpu = multiprocessing.cpu_count()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from typing import Any\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    ")\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from numpy import dtype\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics.classification import BinaryAUROC\n",
    "\n",
    "### data location ###\n",
    "data_dir = \"/Users/jrudoler/data/small_scalp_features/\"\n",
    "log_dir = \"/Users/jrudoler/Library/CloudStorage/Box-Box/JR_CML/pytorch_logs/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precondition Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitPrecondition(pl.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, learning_rate, weight_decay, batch_size):\n",
    "        super().__init__()\n",
    "        if output_dim is None:\n",
    "            output_dim = input_dim\n",
    "        self.condition = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=input_dim,\n",
    "                out_channels=2 * input_dim,\n",
    "                kernel_size=2,\n",
    "                padding=1,\n",
    "                groups=input_dim,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=4),\n",
    "            nn.Conv1d(\n",
    "                in_channels=2 * input_dim,\n",
    "                out_channels=2 * input_dim,\n",
    "                kernel_size=4,\n",
    "                padding=1,\n",
    "                groups=2 * input_dim,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=4),\n",
    "            nn.Conv1d(\n",
    "                in_channels=2 * input_dim,\n",
    "                out_channels=input_dim,\n",
    "                kernel_size=4,\n",
    "                padding=1,\n",
    "                groups=input_dim,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool1d(kernel_size=4),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        self.logistic = nn.Sequential(nn.Linear(output_dim, 1, bias=True), nn.Sigmoid())\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cond = self.condition(x)\n",
    "        probs = self.logistic(x_cond)\n",
    "        return probs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.forward(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\n",
    "            \"Loss/train\", loss, on_epoch=True, on_step=False, prog_bar=True, logger=True\n",
    "        )\n",
    "        auroc = BinaryAUROC()\n",
    "        train_auc = auroc(y_hat, y)\n",
    "        self.log(\"AUC/train\", train_auc, on_epoch=True, on_step=False, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        X, y = X.float(), y.float()\n",
    "        y_hat = torch.squeeze(self.forward(X))\n",
    "        loss = F.binary_cross_entropy(y_hat, y)\n",
    "        self.log(\"Loss/test\", loss, on_epoch=True, on_step=False, logger=True)\n",
    "        auroc = BinaryAUROC()\n",
    "        test_auc = auroc(y_hat, y)\n",
    "        self.log(\n",
    "            \"AUC/test\",\n",
    "            test_auc,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self) -> Any:\n",
    "        self.logger.log_hyperparams(\n",
    "            {\n",
    "                \"learning_rate\": self.hparams[\"learning_rate\"],\n",
    "                \"weight_decay\": self.hparams[\"weight_decay\"],\n",
    "                \"batch_size\": self.hparams[\"batch_size\"],\n",
    "            }\n",
    "        )\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams[\"learning_rate\"],\n",
    "            weight_decay=self.hparams[\"weight_decay\"],\n",
    "        )\n",
    "        lr_scheduler_config = {\n",
    "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer=optimizer, threshold=1e-4, verbose=True\n",
    "            ),\n",
    "            # The unit of the scheduler's step size, 'epoch' or 'step'.\n",
    "            \"interval\": \"epoch\",\n",
    "            # How many epochs/steps should pass between calls to\n",
    "            # `scheduler.step()`. 1 corresponds to updating the learning\n",
    "            # rate after every epoch/step.\n",
    "            \"frequency\": 1,\n",
    "            # Metric to to monitor for schedulers like `ReduceLROnPlateau`\n",
    "            \"monitor\": \"Loss/train\",\n",
    "            # If set to `True`, will enforce that the value specified 'monitor'\n",
    "            # is available when the scheduler is updated\n",
    "            \"strict\": True,\n",
    "            # If using the `LearningRateMonitor` callback to monitor the\n",
    "            # learning rate progress, this keyword can be used to specify\n",
    "            # a custom logged name\n",
    "            \"name\": \"learning_rate\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": lr_scheduler_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 56\n",
      "/Users/jrudoler/miniconda3/envs/torch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|██████████| 7/7 [00:01<00:00,  5.82it/s, v_num=3738, Loss/train=0.692]Epoch 00065: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 75: 100%|██████████| 7/7 [00:01<00:00,  5.89it/s, v_num=3738, Loss/train=0.692]Epoch 00076: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 78: 100%|██████████| 7/7 [00:01<00:00,  5.87it/s, v_num=3738, Loss/train=0.692]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jrudoler/miniconda3/envs/torch/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 16.65it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jrudoler/miniconda3/envs/torch/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  25%|██▌       | 1/4 [00:00<00:00,  3.00it/s, v_num=3738, Loss/train=0.694]"
     ]
    }
   ],
   "source": [
    "### HYPERPARAMETERS ####\n",
    "learning_rate = 1e-2\n",
    "weight_decay = 1e-4  # 1e-4\n",
    "batch_size = 512\n",
    "########################\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "_ = pl.seed_everything(56)\n",
    "subject = \"LTP093\"\n",
    "test_result = []\n",
    "for sess in range(24):\n",
    "    try:\n",
    "        test_file_crit = (\n",
    "            lambda s: s.endswith(\".pt\")\n",
    "            and s.count(f\"sub_{subject}\")\n",
    "            and s.count(f\"sess_{sess}\")\n",
    "        )\n",
    "        test_dataset = DatasetFolder(\n",
    "            data_dir,\n",
    "            loader=partial(torch.load),\n",
    "            is_valid_file=test_file_crit,\n",
    "        )\n",
    "        train_file_crit = (\n",
    "            lambda s: s.endswith(\".pt\")\n",
    "            and s.count(f\"sub_{subject}\")\n",
    "            and not s.count(f\"sess_{sess}\")\n",
    "        )\n",
    "        train_dataset = DatasetFolder(\n",
    "            data_dir,\n",
    "            loader=partial(torch.load),\n",
    "            is_valid_file=train_file_crit,\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        print(f\"no session {sess}\")\n",
    "        continue\n",
    "    ## class balancing ##\n",
    "    cls_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(train_dataset.targets),\n",
    "        y=train_dataset.targets,\n",
    "    )\n",
    "    weights = cls_weights[train_dataset.targets]\n",
    "    sampler = WeightedRandomSampler(weights, len(train_dataset), replacement=True)  # type: ignore\n",
    "    ## data loaders ##\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        pin_memory=True,\n",
    "        num_workers=n_cpu,\n",
    "        prefetch_factor=10,\n",
    "        persistent_workers=True,\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=len(test_dataset),\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=n_cpu,\n",
    "    )\n",
    "    ## create model ##\n",
    "    n_features = train_dataset[0][0].shape[0]\n",
    "    model = LitPrecondition(\n",
    "        n_features, n_features, learning_rate, weight_decay, batch_size\n",
    "    )\n",
    "    es = EarlyStopping(\"Loss/train\", min_delta=0.0001, patience=25, mode=\"min\")\n",
    "    lr_mtr = LearningRateMonitor(\"epoch\")\n",
    "    check = ModelCheckpoint(monitor=\"AUC/train\", mode=\"max\")\n",
    "    run_dir = f\"run_{subject}_{sess}_{timestr}\"\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=log_dir, name=\"precondition\", version=run_dir, default_hp_metric=False\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        min_epochs=75,\n",
    "        max_epochs=200,\n",
    "        accelerator=\"mps\",\n",
    "        devices=1,\n",
    "        callbacks=[lr_mtr, es, check],\n",
    "        logger=logger,\n",
    "        log_every_n_steps=10,\n",
    "    )\n",
    "    # trainer.logger._default_hp_metric = None\n",
    "    trainer.fit(model, train_dataloaders=train_dataloader)\n",
    "    model = LitPrecondition.load_from_checkpoint(\n",
    "        trainer.checkpoint_callback.best_model_path  # type: ignore\n",
    "    )  # Load best checkpoint after training\n",
    "    test_result += trainer.test(model, dataloaders=test_dataloader, verbose=False)\n",
    "    test_result[-1].update({\"subject\": subject, \"session\": sess})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss/test</th>\n",
       "      <th>AUC/test</th>\n",
       "      <th>subject</th>\n",
       "      <th>session</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.691342</td>\n",
       "      <td>0.463612</td>\n",
       "      <td>LTP093</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.693647</td>\n",
       "      <td>0.492343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.691620</td>\n",
       "      <td>0.536043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.692682</td>\n",
       "      <td>0.498934</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.690038</td>\n",
       "      <td>0.594484</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.693460</td>\n",
       "      <td>0.510938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.693099</td>\n",
       "      <td>0.471304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.695018</td>\n",
       "      <td>0.452026</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.694144</td>\n",
       "      <td>0.504188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.688533</td>\n",
       "      <td>0.574561</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.692521</td>\n",
       "      <td>0.551489</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.691922</td>\n",
       "      <td>0.594158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.697345</td>\n",
       "      <td>0.552457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.688622</td>\n",
       "      <td>0.567979</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.692098</td>\n",
       "      <td>0.548944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.693012</td>\n",
       "      <td>0.506291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.699772</td>\n",
       "      <td>0.621490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.686473</td>\n",
       "      <td>0.513514</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.693405</td>\n",
       "      <td>0.543316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.689435</td>\n",
       "      <td>0.605144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.678450</td>\n",
       "      <td>0.629216</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.684536</td>\n",
       "      <td>0.641032</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.692404</td>\n",
       "      <td>0.517506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Loss/test  AUC/test subject  session\n",
       "0    0.691342  0.463612  LTP093      0.0\n",
       "1    0.693647  0.492343     NaN      NaN\n",
       "2    0.691620  0.536043     NaN      NaN\n",
       "3    0.692682  0.498934     NaN      NaN\n",
       "4    0.690038  0.594484     NaN      NaN\n",
       "5    0.693460  0.510938     NaN      NaN\n",
       "6    0.693099  0.471304     NaN      NaN\n",
       "7    0.695018  0.452026     NaN      NaN\n",
       "8    0.694144  0.504188     NaN      NaN\n",
       "9    0.688533  0.574561     NaN      NaN\n",
       "10   0.692521  0.551489     NaN      NaN\n",
       "11   0.691922  0.594158     NaN      NaN\n",
       "12   0.697345  0.552457     NaN      NaN\n",
       "13   0.688622  0.567979     NaN      NaN\n",
       "14   0.692098  0.548944     NaN      NaN\n",
       "15   0.693012  0.506291     NaN      NaN\n",
       "16   0.699772  0.621490     NaN      NaN\n",
       "17   0.686473  0.513514     NaN      NaN\n",
       "18   0.693405  0.543316     NaN      NaN\n",
       "19   0.689435  0.605144     NaN      NaN\n",
       "20   0.678450  0.629216     NaN      NaN\n",
       "21   0.684536  0.641032     NaN      NaN\n",
       "22   0.692404  0.517506     NaN      NaN"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"precond_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
