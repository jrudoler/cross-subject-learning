{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using mps device \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import multiprocessing\n",
        "import time\n",
        "import sys\n",
        "from functools import partial\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import DatasetFolder\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.model_selection import LeaveOneGroupOut, cross_validate\n",
        "\n",
        "#### PYTORCH CONFIGURATION SETTINGS ######\n",
        "if not torch.backends.mps.is_available():\n",
        "    if not torch.backends.mps.is_built():\n",
        "        print(\"MPS not available because the current PyTorch install was not \"\n",
        "              \"built with MPS enabled.\")\n",
        "    else:\n",
        "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
        "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
        "device = torch.device(\"mps\") if torch.has_mps else torch.device(\"cpu\")\n",
        "print(f\"Using {device.type} device \")\n",
        "\n",
        "### DATA LOCATION ###\n",
        "data_dir = \"/Users/jrudoler/data/scalp_features/\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test MPS speed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "def test_speed(device):\n",
        "    print(f\"Running on {device}...\")\n",
        "\n",
        "    model = torchvision.models.resnet18(pretrained=True).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    input_data = torch.randn(64, 3, 224, 224).to(device)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(100):\n",
        "            _ = model(input_data)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "\n",
        "    print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     cpu_device = torch.device(\"cpu\")\n",
        "#     gpu_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#     test_speed(cpu_device)\n",
        "#     test_speed(gpu_device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on mps...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jrudoler/miniforge3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/Users/jrudoler/miniforge3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "test_speed(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on cpu...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jrudoler/miniforge3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/Users/jrudoler/miniforge3/envs/torch/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elapsed time: 167.17 seconds\n"
          ]
        }
      ],
      "source": [
        "test_speed(torch.device(\"cpu\"))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed=None, seed_torch=True):\n",
        "  \"\"\"\n",
        "  Function that controls randomness. NumPy and random modules must be imported.\n",
        "\n",
        "  Args:\n",
        "    seed : Integer\n",
        "      A non-negative integer that defines the random state. Default is `None`.\n",
        "    seed_torch : Boolean\n",
        "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
        "      must be imported. Default is `True`.\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "  if seed is None:\n",
        "    seed = np.random.choice(2 ** 32)\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  if seed_torch:\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  print(f'Random seed {seed} has been set.')\n",
        "\n",
        "class SimpleDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "        \n",
        "class LogisticRegressionTorch(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=1):\n",
        "        super().__init__()\n",
        "        self.logistic = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_dim, output_dim, bias=True),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        # logits = torch.sigmoid(self.linear(x))\n",
        "        probs = self.logistic(x)\n",
        "        return probs\n",
        "    \n",
        "def train_loop(dataloader, model, loss_fn, optimizer,profiler=None, writer=None, log_num=4, l2=None):\n",
        "    if writer:\n",
        "      global global_step\n",
        "    size = len(dataloader.dataset)\n",
        "    running_loss = 0.0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X = X.float().to(device); y = y.float().to(device) \n",
        "        # Compute prediction and loss\n",
        "        pred = torch.squeeze(model(X))\n",
        "        loss = loss_fn(pred, y)\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Profile code according to schedule\n",
        "        if profiler:\n",
        "            if profiler.schedule(batch).value>0:\n",
        "                profiler.step()\n",
        "            else:\n",
        "                profiler.stop()\n",
        "        running_loss += loss.item()\n",
        "        if batch % log_num == (log_num-1):\n",
        "            avg_loss = running_loss / log_num\n",
        "            running_loss = 0.0\n",
        "            pred_eval, y_eval = pred.detach().cpu().numpy(), y.detach().cpu().numpy()\n",
        "            train_auc = roc_auc_score(y_true=y_eval, y_score=pred_eval)\n",
        "            if writer:\n",
        "              # Log the training loss and performance\n",
        "              writer.add_scalar('training loss', avg_loss, global_step)\n",
        "              writer.add_scalar('training AUC', train_auc, global_step)\n",
        "              # Update the global step\n",
        "              global_step += 1\n",
        "              writer.flush()\n",
        "            else:\n",
        "              print(f\"loss: {avg_loss:>7f}  [{batch * len(X):>5d}/{size:>5d}] \" +\n",
        "                  f\"\\tAUC:{train_auc}\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "    \n",
        "    # we don't want to track gradients here because we're just doing\n",
        "    # a forward pass to evaluate predictions\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = torch.squeeze(model(X))\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            # round predicted probs to get label prediction, compute n correct\n",
        "            correct += (pred.round() == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    \n",
        "def test_auc_score(dataloader, model, writer=None):\n",
        "    if writer:\n",
        "      global global_step\n",
        "    with torch.no_grad():\n",
        "        scores = []\n",
        "        for X, y in dataloader:\n",
        "            X = X.float().to(device); y = y.float().to(device) \n",
        "            pred = model(X)\n",
        "            pred, y = pred.detach().cpu().numpy(), y.detach().cpu().numpy()\n",
        "            scores.append(roc_auc_score(y_true=y, y_score=pred))\n",
        "        mean_auc = np.mean(scores)\n",
        "        print(\"Average Test AUC:\", mean_auc)\n",
        "        if writer:\n",
        "          writer.add_scalar('test AUC', mean_auc, global_step)\n",
        "          global_step += 1\n",
        "    return mean_auc"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preconditioning Model (Raw signal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PrecondLogisticRegressionTorch(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=3968, n_elec = 124):\n",
        "        super().__init__()\n",
        "        self.condition = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=n_elec, out_channels=2*n_elec, kernel_size=1, padding=0, groups=n_elec),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool1d(kernel_size=2),\n",
        "            nn.Conv1d(in_channels=2*n_elec, out_channels=4*n_elec, kernel_size=2, padding=0, groups=2*n_elec),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "            nn.Conv1d(in_channels=4*n_elec, out_channels=8*n_elec, kernel_size=4, padding=0, groups=4*n_elec),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool1d(kernel_size=2),\n",
        "            nn.Conv1d(in_channels=8*n_elec, out_channels=16*n_elec, kernel_size=8, padding=0, groups=8*n_elec),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "            nn.Conv1d(in_channels=16*n_elec, out_channels=8*n_elec, kernel_size=8, padding=0, groups=8*n_elec),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=8),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.logistic = nn.Sequential(\n",
        "            nn.Linear(output_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x_cond = self.condition(x)\n",
        "        probs = self.logistic(x_cond)\n",
        "        return probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlZLSW-yVQTx",
        "tags": []
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ts = TimeSeries.from_hdf(\"/scratch/jrudoler/scalp_features/LTP093_eeg.h5\")\n",
        "# ts = ts.stack(row = (\"event\", \"channel\"), create_index=False).T\n",
        "# sessions = ts.session.values\n",
        "# trial = ts.trial.values\n",
        "# serialpos = ts.serialpos.values\n",
        "# y = torch.tensor(ts.recalled.values).float()\n",
        "# X = torch.tensor(ts.data).float()#[:, None, :]\n",
        "# del ts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkSvHH_eVQTy",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# # conv = nn.Conv1d(in_channels=1, out_channels=2, kernel_size=2, stride=1, groups=1)\n",
        "# # avgpool = nn.AvgPool1d(kernel_size=2)\n",
        "# # out = conv(X[0:1])\n",
        "# # print(out.shape)\n",
        "# # pooled = avgpool(out)\n",
        "# # print(pooled.shape)\n",
        "\n",
        "# n_elec = 124\n",
        "# condition = nn.Sequential(\n",
        "#     nn.Conv1d(in_channels=n_elec, out_channels=2*n_elec, kernel_size=1, padding=0, groups=n_elec),\n",
        "#     nn.ReLU(),\n",
        "#     nn.AvgPool1d(kernel_size=2),\n",
        "#     nn.Conv1d(in_channels=2*n_elec, out_channels=4*n_elec, kernel_size=2, padding=0, groups=2*n_elec),\n",
        "#     nn.ReLU(),\n",
        "#     nn.MaxPool1d(kernel_size=2),\n",
        "#     nn.Conv1d(in_channels=4*n_elec, out_channels=8*n_elec, kernel_size=4, padding=0, groups=4*n_elec),\n",
        "#     nn.ReLU(),\n",
        "#     nn.AvgPool1d(kernel_size=2),\n",
        "#     nn.Conv1d(in_channels=8*n_elec, out_channels=16*n_elec, kernel_size=8, padding=0, groups=8*n_elec),\n",
        "#     nn.ReLU(),\n",
        "#     nn.MaxPool1d(kernel_size=2),\n",
        "# #     nn.Conv1d(in_channels=16*n_elec, out_channels=16*n_elec, kernel_size=8, padding=0, groups=16*n_elec),\n",
        "# #     nn.ReLU(),\n",
        "#     nn.AvgPool1d(kernel_size=2),\n",
        "#     nn.Conv1d(in_channels=16*n_elec, out_channels=8*n_elec, kernel_size=8, padding=0, groups=8*n_elec),\n",
        "#     nn.ReLU(),\n",
        "#     nn.MaxPool1d(kernel_size=8),\n",
        "#     nn.Flatten()\n",
        "# )\n",
        "\n",
        "# condition = nn.Sequential(\n",
        "#     nn.Conv1d(in_channels=1, out_channels=2, kernel_size=1, padding=0),\n",
        "#     nn.AvgPool1d(kernel_size=2),\n",
        "#     nn.Conv1d(in_channels=2, out_channels=4, kernel_size=2, padding=0),\n",
        "#     nn.MaxPool1d(kernel_size=2),\n",
        "#     nn.Conv1d(in_channels=4, out_channels=8, kernel_size=4, padding=0),\n",
        "#     nn.AvgPool1d(kernel_size=2),\n",
        "#     nn.Conv1d(in_channels=8, out_channels=16, kernel_size=8, padding=0),\n",
        "#     nn.MaxPool1d(kernel_size=2),\n",
        "#     nn.Conv1d(in_channels=16, out_channels=16, kernel_size=8, padding=0),\n",
        "#     nn.AvgPool1d(kernel_size=2),\n",
        "#     nn.Flatten()\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PCH7rkENVQTy",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "4969858d-9e75-4184-ddb9-53e2a9c02253",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5, 992])\n"
          ]
        }
      ],
      "source": [
        "# out = condition(X[0:5])\n",
        "# print(out.shape)\n",
        "# pooled = avgpool(out)\n",
        "# print(pooled.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf6FDzwJVQTz"
      },
      "outputs": [],
      "source": [
        "# sched = torch.profiler.schedule(wait=0, warmup=2, active=3, repeat=2)\n",
        "# prof = torch.profiler.profile(\n",
        "#         schedule=sched,\n",
        "#         on_trace_ready=torch.profiler.tensorboard_trace_handler('/home1/jrudoler/logs/precond_raw'),\n",
        "#         record_shapes=True,\n",
        "#         with_stack=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJOlTTshVQTz",
        "outputId": "65ad3fc8-e790-4cb8-eaa7-b3656df1d902"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed 56 has been set.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'X' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m sk_auc_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m logo \u001b[38;5;241m=\u001b[39m LeaveOneGroupOut()\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (i, (train_idx, test_idx)) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(logo\u001b[38;5;241m.\u001b[39msplit(\u001b[43mX\u001b[49m, y, groups\u001b[38;5;241m=\u001b[39msessions)))):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSESSION \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m## create model ##\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
          ]
        }
      ],
      "source": [
        "# prof.start()\n",
        "# set_seed(56)\n",
        "# torch.use_deterministic_algorithms(True)\n",
        "# # torch.backends.cudnn.deterministic = True\n",
        "# torch_auc_list = []\n",
        "# sk_auc_list = []\n",
        "# logo = LeaveOneGroupOut()\n",
        "# for (i, (train_idx, test_idx)) in tqdm(list(enumerate(logo.split(X, y, groups=sessions)))):\n",
        "#     print(f\"{'#'*30}\\nSESSION {i}\\n{'#'*30}\")\n",
        "#     ## create model ##\n",
        "#     model = PrecondLogisticRegressionTorch(X.shape[-1])\n",
        "#     loss_fn = torch.nn.BCELoss()\n",
        "#     lr = 1e-4\n",
        "#     weight_decay = 1\n",
        "#     optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "#     ## data ##\n",
        "#     train_set = SimpleDataset(X[train_idx], y[train_idx])\n",
        "#     test_set = SimpleDataset(X[test_idx], y[test_idx])\n",
        "#     ## class balancing ##\n",
        "#     cls_weights = compute_class_weight(\n",
        "#         class_weight=\"balanced\",\n",
        "#         classes=np.unique(train_set.y.detach().numpy()),\n",
        "#         y=train_set.y.detach().numpy(),\n",
        "#     )\n",
        "#     weights = cls_weights[train_set.y.detach().numpy().astype(int)]\n",
        "#     sampler = WeightedRandomSampler(\n",
        "#         weights, len(train_set.y.detach().numpy()), replacement=True\n",
        "#     )\n",
        "\n",
        "#     train_dataloader = DataLoader(train_set, batch_size=200, sampler=sampler)\n",
        "#     test_dataloader = DataLoader(test_set, batch_size=200, shuffle=True)\n",
        "\n",
        "#     ## training epochs ##\n",
        "#     EPOCHS = 10\n",
        "# #     scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[2, 4, 6, 8])\n",
        "#     for t in range(EPOCHS):\n",
        "#         print(f\"{'-'*30}\\nEpoch {t+1}\\n{'-'*30}\")\n",
        "#         train_loop(train_dataloader, model, loss_fn, optimizer, prof, log_num=2)\n",
        "#         test_loop(test_dataloader, model, loss_fn)\n",
        "#         out = test_auc_score(test_set, model)\n",
        "# #         if t in scheduler.milestones:\n",
        "# #             scheduler.step()\n",
        "#     torch_auc_list.append(out)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NzyZ9367VQTz"
      },
      "source": [
        "## Preconditioning Model (Features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eeSFpcDiVQTz"
      },
      "outputs": [],
      "source": [
        "class PrecondFeatLogisticRegressionTorch(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim=None):\n",
        "        super().__init__()\n",
        "        if output_dim is None:\n",
        "            output_dim = input_dim\n",
        "        self.condition = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=input_dim, out_channels=2*input_dim, kernel_size=1, padding=1, groups=input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool1d(kernel_size=2),\n",
        "            nn.Conv1d(in_channels=2*input_dim, out_channels=4*input_dim, kernel_size=2, padding=1, groups=2*input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=2),\n",
        "            nn.Conv1d(in_channels=4*input_dim, out_channels=2*input_dim, kernel_size=4, padding=1, groups=2*input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool1d(kernel_size=4),\n",
        "            nn.Conv1d(in_channels=2*input_dim, out_channels=input_dim, kernel_size=2, padding=0, groups=input_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool1d(kernel_size=4),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.logistic = nn.Sequential(\n",
        "            nn.Linear(output_dim, 1, bias=True),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x_cond = self.condition(x)\n",
        "        probs = self.logistic(x_cond)\n",
        "        return probs\n",
        "\n",
        "model = PrecondFeatLogisticRegressionTorch(2480)\n",
        "dummy_input = torch.zeros(1, 2480, 140)  # Adjust the shape of this tensor to match the input shape of your model\n",
        "writer = SummaryWriter('./logs/model_graph_precond_feats')\n",
        "writer.add_graph(model, dummy_input)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MHZDdagVQT0",
        "tags": []
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23\n",
            "22\n",
            "21\n",
            "20\n",
            "19\n",
            "18\n",
            "17\n",
            "16\n",
            "15\n",
            "14\n",
            "13\n",
            "12\n",
            "11\n",
            "10\n",
            "9\n",
            "8\n",
            "7\n",
            "6\n",
            "5\n",
            "4\n",
            "3\n",
            "2\n",
            "1\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "for i in range(23, -1, -1):\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "t91cTJ03VQT1",
        "outputId": "f79cfc51-6302-4813-d7ec-6a66f40753a7",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed 56 has been set.\n",
            "##############################\n",
            "SESSION 23\n",
            "##############################\n",
            "0.7279958724975586 s to initialize session training\n",
            "------------------------------\n",
            "Epoch 1\n",
            "------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "STAGE:2023-04-20 10:09:55 15158:2642175 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
            "STAGE:2023-04-20 10:10:06 15158:2642175 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
            "STAGE:2023-04-20 10:10:06 15158:2642175 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n",
            "STAGE:2023-04-20 10:10:14 15158:2642175 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
            "STAGE:2023-04-20 10:10:32 15158:2642175 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
            "STAGE:2023-04-20 10:10:32 15158:2642175 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 62\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[1;32m     61\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m30\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m30\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m     train_loop(train_dataloader, model, loss_fn, optimizer, profiler, writer, log_num\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m     63\u001b[0m     out \u001b[39m=\u001b[39m test_auc_score(test_dataloader, model, writer)\n\u001b[1;32m     64\u001b[0m torch_auc_list\u001b[39m.\u001b[39mappend(out)\n",
            "Cell \u001b[0;32mIn[2], line 71\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, profiler, writer, log_num, l2)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m         profiler\u001b[39m.\u001b[39mstop()\n\u001b[0;32m---> 71\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m batch \u001b[39m%\u001b[39m log_num \u001b[39m==\u001b[39m (log_num\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m     73\u001b[0m     avg_loss \u001b[39m=\u001b[39m running_loss \u001b[39m/\u001b[39m log_num\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "subject = \"LTP093\"\n",
        "set_seed(56)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch_auc_list = []\n",
        "timestr = time.strftime(\"%m%d-%H%M%S\")\n",
        "### HYPERPARAMETERS ####\n",
        "lr = 1e-2 #1e-2\n",
        "weight_decay = 1e-8 #1e-4\n",
        "batch_size = 256\n",
        "########################\n",
        "profiler = torch.profiler.profile(\n",
        "    schedule=torch.profiler.schedule(wait=0, warmup=2, active=3, repeat=2),\n",
        "    on_trace_ready=torch.profiler.tensorboard_trace_handler(f'./logs/precond_LR_{lr:.0e}_WD_{weight_decay:.0e}_BS_{batch_size}_{subject}_{timestr}'),\n",
        "    record_shapes=True, with_stack=True, profile_memory=True\n",
        ")\n",
        "profiler.start()\n",
        "writer = SummaryWriter(f'./logs/precond_LR_{lr:.0e}_WD_{weight_decay:.0e}_BS_{batch_size}_{subject}_{timestr}')\n",
        "\n",
        "for sess in range(23, -1, -1):\n",
        "    start = time.time()\n",
        "    print(f\"{'#'*30}\\nSESSION {sess}\\n{'#'*30}\")\n",
        "    ## data ##\n",
        "    try:\n",
        "        test_file_crit = lambda s: s.endswith(\".pt\") and s.count(f\"sub_{subject}\") and s.count(f\"sess_{sess}\")\n",
        "        test_dataset = DatasetFolder(data_dir,\n",
        "                                     loader=partial(torch.load), #map_location=device\n",
        "                                    #  target_transform=partial(torch.tensor, device=device),\n",
        "                                     is_valid_file=test_file_crit)\n",
        "        train_file_crit = lambda s: s.endswith(\".pt\") and s.count(f\"sub_{subject}\") and not s.count(f\"sess_{sess}\")\n",
        "        train_dataset = DatasetFolder(data_dir,\n",
        "                                      loader=partial(torch.load),\n",
        "                                    #   target_transform=partial(torch.tensor, device=device),\n",
        "                                      is_valid_file=train_file_crit)  \n",
        "    except FileNotFoundError:\n",
        "        print(f\"no session {sess}\")\n",
        "        continue\n",
        "    ## class balancing ##\n",
        "    cls_weights = compute_class_weight(\n",
        "        class_weight=\"balanced\",\n",
        "        classes=np.unique(train_dataset.targets),\n",
        "        y=train_dataset.targets\n",
        "    )\n",
        "    weights = cls_weights[train_dataset.targets]\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights, len(train_dataset), replacement=True\n",
        "    )\n",
        "    ## data loaders ##\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)#, num_workers=2, prefetch_factor=2, persistent_workers=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "    ## create model ##\n",
        "    n_features = train_dataset[0][0].shape[0]\n",
        "    model = PrecondFeatLogisticRegressionTorch(n_features).to(device)\n",
        "    loss_fn = torch.nn.BCELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    ## training epochs ##\n",
        "    print(f\"{time.time()-start} s to initialize session training\")\n",
        "    EPOCHS = 50\n",
        "    global_step=0\n",
        "    for t in range(EPOCHS):\n",
        "        print(f\"{'-'*30}\\nEpoch {t+1}\\n{'-'*30}\")\n",
        "        train_loop(train_dataloader, model, loss_fn, optimizer, profiler, writer, log_num=10)\n",
        "        out = test_auc_score(test_dataloader, model, writer)\n",
        "    torch_auc_list.append(out)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "subject = \"LTP093\"\n",
        "set_seed(56)\n",
        "torch.use_deterministic_algorithms(True)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch_auc_list = []\n",
        "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "### HYPERPARAMETERS ####\n",
        "lr = 1e-4\n",
        "weight_decay = 10\n",
        "########################\n",
        "profiler = torch.profiler.profile(\n",
        "    schedule=torch.profiler.schedule(wait=0, warmup=2, active=3, repeat=2),\n",
        "    on_trace_ready=torch.profiler.tensorboard_trace_handler(f'./logs/logreg_LR_{lr}_WD_{weight_decay}_{timestr}'),\n",
        "    record_shapes=True, with_stack=True, profile_memory=True\n",
        ")\n",
        "profiler.start()\n",
        "writer = SummaryWriter(f'./logs/logreg_LR_{lr}_WD_{weight_decay}_{timestr}'))\n",
        "\n",
        "for sess in range(24):\n",
        "    start = time.time()\n",
        "    print(f\"{'#'*30}\\nSESSION {sess}\\n{'#'*30}\")\n",
        "    ## data ##\n",
        "    try:\n",
        "        test_file_crit = lambda s: s.endswith(\".pt\") and s.count(f\"sub_{subject}\") and s.count(f\"sess_{sess}\")\n",
        "        test_dataset = DatasetFolder(data_dir,\n",
        "                                     loader=partial(torch.load), \n",
        "                                     transform=partial(torch.mean, dim=-1),\n",
        "                                     is_valid_file=test_file_crit)\n",
        "        train_file_crit = lambda s: s.endswith(\".pt\") and s.count(f\"sub_{subject}\") and not s.count(f\"sess_{sess}\")\n",
        "        train_dataset = DatasetFolder(data_dir,\n",
        "                                      loader=partial(torch.load),\n",
        "                                      transform=partial(torch.mean, dim=-1),\n",
        "                                      is_valid_file=train_file_crit)  \n",
        "    except FileNotFoundError:\n",
        "        print(f\"no session {sess}\")\n",
        "        continue\n",
        "    ## class balancing ##\n",
        "    cls_weights = compute_class_weight(\n",
        "        class_weight=\"balanced\",\n",
        "        classes=np.unique(train_dataset.targets),\n",
        "        y=train_dataset.targets\n",
        "    )\n",
        "    weights = cls_weights[train_dataset.targets]\n",
        "    sampler = WeightedRandomSampler(\n",
        "        weights, len(train_dataset), replacement=True\n",
        "    )\n",
        "    ## data loaders ##\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=576, sampler=sampler)#, num_workers=2, prefetch_factor=2, persistent_workers=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=576, shuffle=True)\n",
        "    ## create model ##\n",
        "    n_features = train_dataset[0][0].shape[0]\n",
        "    model = LogisticRegressionTorch(n_features).to(device)\n",
        "    loss_fn = torch.nn.BCELoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    ## training epochs ##\n",
        "    print(f\"{time.time()-start} s to initialize session training\")\n",
        "    EPOCHS = 100\n",
        "#     scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[2, 4, 6, 8])\n",
        "    global_step=0\n",
        "    for t in range(EPOCHS):\n",
        "        print(f\"{'-'*30}\\nEpoch {t+1}\\n{'-'*30}\")\n",
        "        train_loop(train_dataloader, model, loss_fn, optimizer, profiler, writer, log_num=10)\n",
        "        out = test_auc_score(test_dataloader, model, writer)\n",
        "#         if t in scheduler.milestones:\n",
        "#             scheduler.step()\n",
        "    torch_auc_list.append(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UedE6U4_VQT1"
      },
      "source": [
        "## Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10749"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_dataloader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean: 0.0946647554340929\n",
            "median: 0.12308907508850098\n",
            "std: 0.22619469457632874\n",
            "max: 2.237199068069458\n",
            "min: 0.0012900829315185547\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=100, sampler=sampler, num_workers=2, prefetch_factor=2, persistent_workers=True)\n",
        "times = []\n",
        "last = time.time()\n",
        "i = 0\n",
        "for X, y in train_dataloader:\n",
        "    if i>100:\n",
        "        break\n",
        "    diff = time.time()-last\n",
        "    times.append(diff)\n",
        "    i+=1\n",
        "    last = time.time()\n",
        "print(\"mean:\", np.mean(times))\n",
        "print(\"median:\", np.median(times))\n",
        "print(\"std:\", np.std(times, ddof=1))\n",
        "print(\"max:\", np.max(times))\n",
        "print(\"min:\", np.min(times))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean: 0.1072290557445866\n",
            "median: 0.10455799102783203\n",
            "std: 0.009177068847984231\n",
            "max: 0.13523507118225098\n",
            "min: 0.09452080726623535\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=100, sampler=sampler, num_workers=0)\n",
        "times = []\n",
        "last = time.time()\n",
        "i = 0\n",
        "for X, y in train_dataloader:\n",
        "    if i>100:\n",
        "        break\n",
        "    diff = time.time()-last\n",
        "    times.append(diff)\n",
        "    i+=1\n",
        "    last = time.time()\n",
        "print(\"mean:\", np.mean(times))\n",
        "print(\"median:\", np.median(times))\n",
        "print(\"std:\", np.std(times, ddof=1))\n",
        "print(\"max:\", np.max(times))\n",
        "print(\"min:\", np.min(times))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HlZLSW-yVQTx"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
