{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55035abd-0f92-4b00-90a5-90048fe5149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import skorch\n",
    "device = \"cpu\"#\"mps\" if torch.has_mps else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "779b936d-440c-4e00-b43b-c02a844211ac",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mskorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNeuralNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'torch.optim.sgd.SGD'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0miterator_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'torch.utils.data.dataloader.DataLoader'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0miterator_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'torch.utils.data.dataloader.DataLoader'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'skorch.dataset.Dataset'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtrain_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mskorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mValidSplit\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x13dce84c0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpredict_nonlinearity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mwarm_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "NeuralNet base class.\n",
       "\n",
       "The base class covers more generic cases. Depending on your use\n",
       "case, you might want to use :class:`.NeuralNetClassifier` or\n",
       ":class:`.NeuralNetRegressor`.\n",
       "\n",
       "In addition to the parameters listed below, there are parameters\n",
       "with specific prefixes that are handled separately. To illustrate\n",
       "this, here is an example:\n",
       "\n",
       ">>> net = NeuralNet(\n",
       "...    ...,\n",
       "...    optimizer=torch.optimizer.SGD,\n",
       "...    optimizer__momentum=0.95,\n",
       "...)\n",
       "\n",
       "This way, when ``optimizer`` is initialized, :class:`.NeuralNet`\n",
       "will take care of setting the ``momentum`` parameter to 0.95.\n",
       "\n",
       "(Note that the double underscore notation in\n",
       "``optimizer__momentum`` means that the parameter ``momentum``\n",
       "should be set on the object ``optimizer``. This is the same\n",
       "semantic as used by sklearn.)\n",
       "\n",
       "Furthermore, this allows to change those parameters later:\n",
       "\n",
       "``net.set_params(optimizer__momentum=0.99)``\n",
       "\n",
       "This can be useful when you want to change certain parameters\n",
       "using a callback, when using the net in an sklearn grid search,\n",
       "etc.\n",
       "\n",
       "By default an :class:`.EpochTimer`, :class:`.BatchScoring` (for\n",
       "both training and validation datasets), and :class:`.PrintLog`\n",
       "callbacks are installed for the user's convenience.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "module : torch module (class or instance)\n",
       "  A PyTorch :class:`~torch.nn.Module`. In general, the\n",
       "  uninstantiated class should be passed, although instantiated\n",
       "  modules will also work.\n",
       "\n",
       "criterion : torch criterion (class)\n",
       "  The uninitialized criterion (loss) used to optimize the\n",
       "  module.\n",
       "\n",
       "optimizer : torch optim (class, default=torch.optim.SGD)\n",
       "  The uninitialized optimizer (update rule) used to optimize the\n",
       "  module\n",
       "\n",
       "lr : float (default=0.01)\n",
       "  Learning rate passed to the optimizer. You may use ``lr`` instead\n",
       "  of using ``optimizer__lr``, which would result in the same outcome.\n",
       "\n",
       "max_epochs : int (default=10)\n",
       "  The number of epochs to train for each ``fit`` call. Note that you\n",
       "  may keyboard-interrupt training at any time.\n",
       "\n",
       "batch_size : int (default=128)\n",
       "  Mini-batch size. Use this instead of setting\n",
       "  ``iterator_train__batch_size`` and ``iterator_test__batch_size``,\n",
       "  which would result in the same outcome. If ``batch_size`` is -1,\n",
       "  a single batch with all the data will be used during training\n",
       "  and validation.\n",
       "\n",
       "iterator_train : torch DataLoader\n",
       "  The default PyTorch :class:`~torch.utils.data.DataLoader` used for\n",
       "  training data.\n",
       "\n",
       "iterator_valid : torch DataLoader\n",
       "  The default PyTorch :class:`~torch.utils.data.DataLoader` used for\n",
       "  validation and test data, i.e. during inference.\n",
       "\n",
       "dataset : torch Dataset (default=skorch.dataset.Dataset)\n",
       "  The dataset is necessary for the incoming data to work with\n",
       "  pytorch's ``DataLoader``. It has to implement the ``__len__`` and\n",
       "  ``__getitem__`` methods. The provided dataset should be capable of\n",
       "  dealing with a lot of data types out of the box, so only change\n",
       "  this if your data is not supported. You should generally pass the\n",
       "  uninitialized ``Dataset`` class and define additional arguments to\n",
       "  X and y by prefixing them with ``dataset__``. It is also possible\n",
       "  to pass an initialzed ``Dataset``, in which case no additional\n",
       "  arguments may be passed.\n",
       "\n",
       "train_split : None or callable (default=skorch.dataset.ValidSplit(5))\n",
       "  If None, there is no train/validation split. Else, train_split\n",
       "  should be a function or callable that is called with X and y\n",
       "  data and should return the tuple ``dataset_train, dataset_valid``.\n",
       "  The validation data may be None.\n",
       "\n",
       "callbacks : None, \"disable\", or list of Callback instances (default=None)\n",
       "  Which callbacks to enable. There are three possible values:\n",
       "\n",
       "  If ``callbacks=None``, only use default callbacks,\n",
       "  those returned by ``get_default_callbacks``.\n",
       "\n",
       "  If ``callbacks=\"disable\"``, disable all callbacks, i.e. do not run\n",
       "  any of the callbacks.\n",
       "\n",
       "  If ``callbacks`` is a list of callbacks, use those callbacks in\n",
       "  addition to the default callbacks. Each callback should be an\n",
       "  instance of :class:`.Callback`.\n",
       "\n",
       "  Callback names are inferred from the class\n",
       "  name. Name conflicts are resolved by appending a count suffix\n",
       "  starting with 1, e.g. ``EpochScoring_1``. Alternatively,\n",
       "  a tuple ``(name, callback)`` can be passed, where ``name``\n",
       "  should be unique. Callbacks may or may not be instantiated.\n",
       "  The callback name can be used to set parameters on specific\n",
       "  callbacks (e.g., for the callback with name ``'print_log'``, use\n",
       "  ``net.set_params(callbacks__print_log__keys_ignored=['epoch',\n",
       "  'train_loss'])``).\n",
       "\n",
       "predict_nonlinearity : callable, None, or 'auto' (default='auto')\n",
       "  The nonlinearity to be applied to the prediction. When set to\n",
       "  'auto', infers the correct nonlinearity based on the criterion\n",
       "  (softmax for :class:`~torch.nn.CrossEntropyLoss` and sigmoid for\n",
       "  :class:`~torch.nn.BCEWithLogitsLoss`). If it cannot be inferred\n",
       "  or if the parameter is None, just use the identity\n",
       "  function. Don't pass a lambda function if you want the net to be\n",
       "  pickleable.\n",
       "\n",
       "  In case a callable is passed, it should accept the output of the\n",
       "  module (the first output if there is more than one), which is a\n",
       "  PyTorch tensor, and return the transformed PyTorch tensor.\n",
       "\n",
       "  This can be useful, e.g., when\n",
       "  :func:`~skorch.NeuralNetClassifier.predict_proba`\n",
       "  should return probabilities but a criterion is used that does\n",
       "  not expect probabilities. In that case, the module can return\n",
       "  whatever is required by the criterion and the\n",
       "  ``predict_nonlinearity`` transforms this output into\n",
       "  probabilities.\n",
       "\n",
       "  The nonlinearity is applied only when calling\n",
       "  :func:`~skorch.classifier.NeuralNetClassifier.predict` or\n",
       "  :func:`~skorch.classifier.NeuralNetClassifier.predict_proba` but\n",
       "  not anywhere else -- notably, the loss is unaffected by this\n",
       "  nonlinearity.\n",
       "\n",
       "warm_start : bool (default=False)\n",
       "  Whether each fit call should lead to a re-initialization of the\n",
       "  module (cold start) or whether the module should be trained\n",
       "  further (warm start).\n",
       "\n",
       "verbose : int (default=1)\n",
       "  This parameter controls how much print output is generated by\n",
       "  the net and its callbacks. By setting this value to 0, e.g. the\n",
       "  summary scores at the end of each epoch are no longer printed.\n",
       "  This can be useful when running a hyperparameter search. The\n",
       "  summary scores are always logged in the history attribute,\n",
       "  regardless of the verbose setting.\n",
       "\n",
       "device : str, torch.device (default='cpu')\n",
       "  The compute device to be used. If set to 'cuda', data in torch\n",
       "  tensors will be pushed to cuda tensors before being sent to the\n",
       "  module. If set to None, then all compute devices will be left\n",
       "  unmodified.\n",
       "\n",
       "Attributes\n",
       "----------\n",
       "prefixes_ : list of str\n",
       "  Contains the prefixes to special parameters. E.g., since there\n",
       "  is the ``'module'`` prefix, it is possible to set parameters like\n",
       "  so: ``NeuralNet(..., optimizer__momentum=0.95)``.\n",
       "\n",
       "cuda_dependent_attributes_ : list of str\n",
       "  Contains a list of all attribute prefixes whose values depend on a\n",
       "  CUDA device. If a ``NeuralNet`` trained with a CUDA-enabled device\n",
       "  is unpickled on a machine without CUDA or with CUDA disabled, the\n",
       "  listed attributes are mapped to CPU.  Expand this list if you\n",
       "  want to add other cuda-dependent attributes.\n",
       "\n",
       "initialized_ : bool\n",
       "  Whether the :class:`.NeuralNet` was initialized.\n",
       "\n",
       "module_ : torch module (instance)\n",
       "  The instantiated module.\n",
       "\n",
       "criterion_ : torch criterion (instance)\n",
       "  The instantiated criterion.\n",
       "\n",
       "callbacks_ : list of tuples\n",
       "  The complete (i.e. default and other), initialized callbacks, in\n",
       "  a tuple with unique names.\n",
       "\n",
       "_modules : list of str\n",
       "  List of names of all modules that are torch modules. This list is\n",
       "  collected dynamically when the net is initialized. Typically, there is no\n",
       "  reason for a user to modify this list.\n",
       "\n",
       "_criteria : list of str\n",
       "  List of names of all criteria that are torch modules. This list is\n",
       "  collected dynamically when the net is initialized. Typically, there is no\n",
       "  reason for a user to modify this list.\n",
       "\n",
       "_optimizers : list of str\n",
       "  List of names of all optimizers. This list is collected dynamically when\n",
       "  the net is initialized. Typically, there is no reason for a user to modify\n",
       "  this list.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniforge3/envs/torch/lib/python3.10/site-packages/skorch/net.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     NeuralNetClassifier, NeuralNetBinaryClassifier, NeuralNetRegressor\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "skorch.NeuralNet?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b18eb46-9cb5-4f79-bd67-59faf8336e78",
   "metadata": {},
   "source": [
    "### Logistic Regression PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d5cd46b-8998-4508-91e7-d08c09b4d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "def train_loop(dataloader, model, loss_fn, optimizer, print_nth_batch=4):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(\"yay training\")\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # print(y)\n",
    "        # Compute prediction and loss\n",
    "        pred = torch.squeeze(model(X))\n",
    "        # print(pred)\n",
    "        # regularization, computing largest singular value\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % print_nth_batch == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    # we don't want to track gradients here because we're just doing\n",
    "    # a forward pass to evaluate predictions\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = torch.squeeze(model(X))\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # round predicted probs to get label prediction, compute n correct\n",
    "            correct += (pred.round() == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "def test_auc_score(dataset, model):\n",
    "    X, y = dataset[:]\n",
    "    pred = model(X)\n",
    "    pred, y = pred.detach().numpy(), y.detach().numpy()\n",
    "    score = roc_auc_score(y_true=y, y_score=pred)\n",
    "    print(score)\n",
    "    return score\n",
    "    \n",
    "    \n",
    "    \n",
    "class LogisticRegressionTorch(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, output_dim, bias=True),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # logits = torch.sigmoid(self.linear(x))\n",
    "        probs = self.linear(x)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88a7ed46-6f7e-494b-9f74-1571075c8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "X_1 = torch.rand(1000, 128) + .2\n",
    "y_1 = torch.zeros(1000,)\n",
    "X_2 = torch.rand(1000, 128) - .1\n",
    "y_2 = torch.ones(1000,)\n",
    "\n",
    "X = torch.cat((X_1, X_2))\n",
    "y = torch.cat((y_1, y_2))\n",
    "\n",
    "dataset = SimpleDataset(X, y)\n",
    "\n",
    "train_prop = .8\n",
    "train_num = int(train_prop * len(dataset))\n",
    "test_num = len(X) - train_num\n",
    "\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset,\n",
    "                                                    [train_num, test_num])\n",
    "\n",
    "\n",
    "target = train_set.dataset.y[train_set.indices]\n",
    "cls_weights = torch.from_numpy(\n",
    "    compute_class_weight(class_weight='balanced',\n",
    "                         classes=np.unique(target.numpy()),\n",
    "                         y=target.numpy())\n",
    ")\n",
    "weights = cls_weights[target.numpy()]\n",
    "sampler = WeightedRandomSampler(weights, len(target.numpy()), replacement=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=100, sampler=sampler)\n",
    "test_dataloader = DataLoader(test_set, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "29b0b7b5-d1b1-44fd-b644-842d55fdd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionTorch(128)\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "weight_decay = 0\n",
    "lr = 5e-1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr,weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d62a1cb0-0e75-4aed-8389-5f90c13bb18c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.448199  [    0/12672]\n",
      "loss: 19.501102  [  400/12672]\n",
      "loss: 11.889750  [  800/12672]\n",
      "loss: 10.050867  [ 1200/12672]\n",
      "loss: 4.415318  [ 1600/12672]\n",
      "loss: 20.606892  [ 2000/12672]\n",
      "loss: 23.693605  [ 2400/12672]\n",
      "loss: 6.622545  [ 2800/12672]\n",
      "loss: 22.063299  [ 3200/12672]\n",
      "loss: 9.026217  [ 3600/12672]\n",
      "loss: 12.450096  [ 4000/12672]\n",
      "loss: 13.314853  [ 4400/12672]\n",
      "loss: 10.686407  [ 4800/12672]\n",
      "loss: 17.673269  [ 5200/12672]\n",
      "loss: 13.940762  [ 5600/12672]\n",
      "loss: 16.124735  [ 6000/12672]\n",
      "loss: 14.201858  [ 6400/12672]\n",
      "loss: 16.571611  [ 6800/12672]\n",
      "loss: 10.381608  [ 7200/12672]\n",
      "loss: 7.627161  [ 7600/12672]\n",
      "loss: 9.125181  [ 8000/12672]\n",
      "loss: 8.847769  [ 8400/12672]\n",
      "loss: 13.405208  [ 8800/12672]\n",
      "loss: 19.638165  [ 9200/12672]\n",
      "loss: 14.617030  [ 9600/12672]\n",
      "loss: 7.883918  [10000/12672]\n",
      "loss: 15.535515  [10400/12672]\n",
      "loss: 13.479076  [10800/12672]\n",
      "loss: 19.351795  [11200/12672]\n",
      "loss: 16.983923  [11600/12672]\n",
      "loss: 5.988626  [12000/12672]\n",
      "loss: 7.569260  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 18.016468 \n",
      "\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.649567  [    0/12672]\n",
      "loss: 13.509387  [  400/12672]\n",
      "loss: 9.397038  [  800/12672]\n",
      "loss: 16.511402  [ 1200/12672]\n",
      "loss: 13.522353  [ 1600/12672]\n",
      "loss: 10.554843  [ 2000/12672]\n",
      "loss: 20.753071  [ 2400/12672]\n",
      "loss: 8.883151  [ 2800/12672]\n",
      "loss: 20.038437  [ 3200/12672]\n",
      "loss: 11.116176  [ 3600/12672]\n",
      "loss: 13.765134  [ 4000/12672]\n",
      "loss: 25.880190  [ 4400/12672]\n",
      "loss: 11.455889  [ 4800/12672]\n",
      "loss: 31.708776  [ 5200/12672]\n",
      "loss: 13.507155  [ 5600/12672]\n",
      "loss: 12.163624  [ 6000/12672]\n",
      "loss: 11.731881  [ 6400/12672]\n",
      "loss: 17.309862  [ 6800/12672]\n",
      "loss: 17.024408  [ 7200/12672]\n",
      "loss: 12.313538  [ 7600/12672]\n",
      "loss: 22.971514  [ 8000/12672]\n",
      "loss: 24.267168  [ 8400/12672]\n",
      "loss: 17.090376  [ 8800/12672]\n",
      "loss: 11.761583  [ 9200/12672]\n",
      "loss: 12.240818  [ 9600/12672]\n",
      "loss: 14.493343  [10000/12672]\n",
      "loss: 7.649945  [10400/12672]\n",
      "loss: 12.533072  [10800/12672]\n",
      "loss: 14.956245  [11200/12672]\n",
      "loss: 10.299414  [11600/12672]\n",
      "loss: 19.823963  [12000/12672]\n",
      "loss: 7.478351  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 16.509207 \n",
      "\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.266824  [    0/12672]\n",
      "loss: 10.595530  [  400/12672]\n",
      "loss: 12.792263  [  800/12672]\n",
      "loss: 12.264021  [ 1200/12672]\n",
      "loss: 16.692595  [ 1600/12672]\n",
      "loss: 14.937693  [ 2000/12672]\n",
      "loss: 14.701788  [ 2400/12672]\n",
      "loss: 9.137777  [ 2800/12672]\n",
      "loss: 11.993700  [ 3200/12672]\n",
      "loss: 11.910131  [ 3600/12672]\n",
      "loss: 18.288343  [ 4000/12672]\n",
      "loss: 12.494102  [ 4400/12672]\n",
      "loss: 20.259302  [ 4800/12672]\n",
      "loss: 14.861511  [ 5200/12672]\n",
      "loss: 17.148876  [ 5600/12672]\n",
      "loss: 11.321703  [ 6000/12672]\n",
      "loss: 15.139669  [ 6400/12672]\n",
      "loss: 6.948258  [ 6800/12672]\n",
      "loss: 10.369998  [ 7200/12672]\n",
      "loss: 14.284306  [ 7600/12672]\n",
      "loss: 18.567276  [ 8000/12672]\n",
      "loss: 12.247922  [ 8400/12672]\n",
      "loss: 12.163288  [ 8800/12672]\n",
      "loss: 8.944297  [ 9200/12672]\n",
      "loss: 17.060509  [ 9600/12672]\n",
      "loss: 11.167281  [10000/12672]\n",
      "loss: 8.378854  [10400/12672]\n",
      "loss: 15.077158  [10800/12672]\n",
      "loss: 12.965886  [11200/12672]\n",
      "loss: 10.021579  [11600/12672]\n",
      "loss: 18.610182  [12000/12672]\n",
      "loss: 16.985973  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 12.171034 \n",
      "\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.183802  [    0/12672]\n",
      "loss: 15.160548  [  400/12672]\n",
      "loss: 10.933621  [  800/12672]\n",
      "loss: 12.116112  [ 1200/12672]\n",
      "loss: 14.420932  [ 1600/12672]\n",
      "loss: 14.012866  [ 2000/12672]\n",
      "loss: 18.241829  [ 2400/12672]\n",
      "loss: 10.419930  [ 2800/12672]\n",
      "loss: 12.127881  [ 3200/12672]\n",
      "loss: 14.237103  [ 3600/12672]\n",
      "loss: 12.597299  [ 4000/12672]\n",
      "loss: 15.388819  [ 4400/12672]\n",
      "loss: 13.214099  [ 4800/12672]\n",
      "loss: 11.429836  [ 5200/12672]\n",
      "loss: 11.230478  [ 5600/12672]\n",
      "loss: 12.669957  [ 6000/12672]\n",
      "loss: 13.378399  [ 6400/12672]\n",
      "loss: 6.739186  [ 6800/12672]\n",
      "loss: 9.320815  [ 7200/12672]\n",
      "loss: 12.317379  [ 7600/12672]\n",
      "loss: 11.085850  [ 8000/12672]\n",
      "loss: 8.723577  [ 8400/12672]\n",
      "loss: 15.511212  [ 8800/12672]\n",
      "loss: 7.381125  [ 9200/12672]\n",
      "loss: 16.983154  [ 9600/12672]\n",
      "loss: 16.036646  [10000/12672]\n",
      "loss: 14.431424  [10400/12672]\n",
      "loss: 15.025547  [10800/12672]\n",
      "loss: 24.832644  [11200/12672]\n",
      "loss: 18.524570  [11600/12672]\n",
      "loss: 15.385938  [12000/12672]\n",
      "loss: 9.860323  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 12.374711 \n",
      "\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.690251  [    0/12672]\n",
      "loss: 11.427903  [  400/12672]\n",
      "loss: 15.970511  [  800/12672]\n",
      "loss: 17.453661  [ 1200/12672]\n",
      "loss: 10.413849  [ 1600/12672]\n",
      "loss: 20.263115  [ 2000/12672]\n",
      "loss: 17.261705  [ 2400/12672]\n",
      "loss: 19.123299  [ 2800/12672]\n",
      "loss: 8.567987  [ 3200/12672]\n",
      "loss: 18.131485  [ 3600/12672]\n",
      "loss: 16.139654  [ 4000/12672]\n",
      "loss: 11.765730  [ 4400/12672]\n",
      "loss: 8.238675  [ 4800/12672]\n",
      "loss: 14.039829  [ 5200/12672]\n",
      "loss: 14.159582  [ 5600/12672]\n",
      "loss: 7.805453  [ 6000/12672]\n",
      "loss: 7.981040  [ 6400/12672]\n",
      "loss: 14.287803  [ 6800/12672]\n",
      "loss: 13.123188  [ 7200/12672]\n",
      "loss: 12.064352  [ 7600/12672]\n",
      "loss: 16.017061  [ 8000/12672]\n",
      "loss: 22.324369  [ 8400/12672]\n",
      "loss: 9.934460  [ 8800/12672]\n",
      "loss: 4.155950  [ 9200/12672]\n",
      "loss: 19.120373  [ 9600/12672]\n",
      "loss: 12.057743  [10000/12672]\n",
      "loss: 10.811903  [10400/12672]\n",
      "loss: 7.482214  [10800/12672]\n",
      "loss: 19.884293  [11200/12672]\n",
      "loss: 15.217796  [11600/12672]\n",
      "loss: 16.182228  [12000/12672]\n",
      "loss: 9.579833  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 23.999401 \n",
      "\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.469971  [    0/12672]\n",
      "loss: 8.185697  [  400/12672]\n",
      "loss: 10.853790  [  800/12672]\n",
      "loss: 12.540036  [ 1200/12672]\n",
      "loss: 15.900827  [ 1600/12672]\n",
      "loss: 17.563251  [ 2000/12672]\n",
      "loss: 12.235682  [ 2400/12672]\n",
      "loss: 16.347824  [ 2800/12672]\n",
      "loss: 7.042830  [ 3200/12672]\n",
      "loss: 8.950843  [ 3600/12672]\n",
      "loss: 13.864532  [ 4000/12672]\n",
      "loss: 14.991700  [ 4400/12672]\n",
      "loss: 10.055911  [ 4800/12672]\n",
      "loss: 17.945389  [ 5200/12672]\n",
      "loss: 13.379511  [ 5600/12672]\n",
      "loss: 13.228340  [ 6000/12672]\n",
      "loss: 14.096674  [ 6400/12672]\n",
      "loss: 15.256887  [ 6800/12672]\n",
      "loss: 12.325086  [ 7200/12672]\n",
      "loss: 16.655842  [ 7600/12672]\n",
      "loss: 6.907852  [ 8000/12672]\n",
      "loss: 10.289501  [ 8400/12672]\n",
      "loss: 15.458017  [ 8800/12672]\n",
      "loss: 13.156487  [ 9200/12672]\n",
      "loss: 9.132521  [ 9600/12672]\n",
      "loss: 18.210533  [10000/12672]\n",
      "loss: 10.921343  [10400/12672]\n",
      "loss: 16.609474  [10800/12672]\n",
      "loss: 25.303988  [11200/12672]\n",
      "loss: 9.111837  [11600/12672]\n",
      "loss: 12.410911  [12000/12672]\n",
      "loss: 17.120338  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 11.639173 \n",
      "\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.995352  [    0/12672]\n",
      "loss: 20.579924  [  400/12672]\n",
      "loss: 12.873269  [  800/12672]\n",
      "loss: 12.389066  [ 1200/12672]\n",
      "loss: 17.971207  [ 1600/12672]\n",
      "loss: 12.104944  [ 2000/12672]\n",
      "loss: 12.234894  [ 2400/12672]\n",
      "loss: 13.213955  [ 2800/12672]\n",
      "loss: 31.964170  [ 3200/12672]\n",
      "loss: 18.850887  [ 3600/12672]\n",
      "loss: 9.032213  [ 4000/12672]\n",
      "loss: 12.687379  [ 4400/12672]\n",
      "loss: 11.031095  [ 4800/12672]\n",
      "loss: 13.497263  [ 5200/12672]\n",
      "loss: 13.893620  [ 5600/12672]\n",
      "loss: 13.919647  [ 6000/12672]\n",
      "loss: 19.508148  [ 6400/12672]\n",
      "loss: 24.821772  [ 6800/12672]\n",
      "loss: 22.795626  [ 7200/12672]\n",
      "loss: 10.880245  [ 7600/12672]\n",
      "loss: 9.302649  [ 8000/12672]\n",
      "loss: 19.752058  [ 8400/12672]\n",
      "loss: 13.427460  [ 8800/12672]\n",
      "loss: 12.387313  [ 9200/12672]\n",
      "loss: 13.814828  [ 9600/12672]\n",
      "loss: 14.291482  [10000/12672]\n",
      "loss: 17.464123  [10400/12672]\n",
      "loss: 12.016816  [10800/12672]\n",
      "loss: 21.615026  [11200/12672]\n",
      "loss: 10.352825  [11600/12672]\n",
      "loss: 8.616590  [12000/12672]\n",
      "loss: 16.116655  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 12.515601 \n",
      "\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.303319  [    0/12672]\n",
      "loss: 8.861413  [  400/12672]\n",
      "loss: 12.084609  [  800/12672]\n",
      "loss: 14.408398  [ 1200/12672]\n",
      "loss: 26.308914  [ 1600/12672]\n",
      "loss: 9.421227  [ 2000/12672]\n",
      "loss: 14.875638  [ 2400/12672]\n",
      "loss: 15.288671  [ 2800/12672]\n",
      "loss: 16.106947  [ 3200/12672]\n",
      "loss: 12.155720  [ 3600/12672]\n",
      "loss: 10.166431  [ 4000/12672]\n",
      "loss: 6.269804  [ 4400/12672]\n",
      "loss: 9.990419  [ 4800/12672]\n",
      "loss: 11.910367  [ 5200/12672]\n",
      "loss: 23.577917  [ 5600/12672]\n",
      "loss: 20.741671  [ 6000/12672]\n",
      "loss: 15.915788  [ 6400/12672]\n",
      "loss: 17.503172  [ 6800/12672]\n",
      "loss: 12.012377  [ 7200/12672]\n",
      "loss: 14.923263  [ 7600/12672]\n",
      "loss: 18.974543  [ 8000/12672]\n",
      "loss: 15.195709  [ 8400/12672]\n",
      "loss: 10.174289  [ 8800/12672]\n",
      "loss: 6.401777  [ 9200/12672]\n",
      "loss: 21.268303  [ 9600/12672]\n",
      "loss: 16.403627  [10000/12672]\n",
      "loss: 16.186075  [10400/12672]\n",
      "loss: 11.771916  [10800/12672]\n",
      "loss: 8.495223  [11200/12672]\n",
      "loss: 13.724661  [11600/12672]\n",
      "loss: 16.110214  [12000/12672]\n",
      "loss: 15.796165  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 17.372392 \n",
      "\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.784058  [    0/12672]\n",
      "loss: 15.786152  [  400/12672]\n",
      "loss: 16.158751  [  800/12672]\n",
      "loss: 19.234642  [ 1200/12672]\n",
      "loss: 17.091730  [ 1600/12672]\n",
      "loss: 13.708818  [ 2000/12672]\n",
      "loss: 6.746881  [ 2400/12672]\n",
      "loss: 8.083761  [ 2800/12672]\n",
      "loss: 12.905032  [ 3200/12672]\n",
      "loss: 16.447067  [ 3600/12672]\n",
      "loss: 17.348621  [ 4000/12672]\n",
      "loss: 20.821470  [ 4400/12672]\n",
      "loss: 8.115767  [ 4800/12672]\n",
      "loss: 10.315418  [ 5200/12672]\n",
      "loss: 15.641731  [ 5600/12672]\n",
      "loss: 10.003778  [ 6000/12672]\n",
      "loss: 17.689465  [ 6400/12672]\n",
      "loss: 11.771424  [ 6800/12672]\n",
      "loss: 9.236159  [ 7200/12672]\n",
      "loss: 13.616966  [ 7600/12672]\n",
      "loss: 13.914340  [ 8000/12672]\n",
      "loss: 24.049797  [ 8400/12672]\n",
      "loss: 12.333526  [ 8800/12672]\n",
      "loss: 9.009104  [ 9200/12672]\n",
      "loss: 9.193885  [ 9600/12672]\n",
      "loss: 12.357063  [10000/12672]\n",
      "loss: 8.423096  [10400/12672]\n",
      "loss: 22.022852  [10800/12672]\n",
      "loss: 13.279924  [11200/12672]\n",
      "loss: 18.116661  [11600/12672]\n",
      "loss: 12.920464  [12000/12672]\n",
      "loss: 10.147761  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 13.401397 \n",
      "\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.949485  [    0/12672]\n",
      "loss: 8.294992  [  400/12672]\n",
      "loss: 17.950354  [  800/12672]\n",
      "loss: 7.952792  [ 1200/12672]\n",
      "loss: 21.010412  [ 1600/12672]\n",
      "loss: 12.966318  [ 2000/12672]\n",
      "loss: 14.722090  [ 2400/12672]\n",
      "loss: 13.706103  [ 2800/12672]\n",
      "loss: 13.002720  [ 3200/12672]\n",
      "loss: 20.948454  [ 3600/12672]\n",
      "loss: 12.959817  [ 4000/12672]\n",
      "loss: 12.776285  [ 4400/12672]\n",
      "loss: 9.594580  [ 4800/12672]\n",
      "loss: 6.740043  [ 5200/12672]\n",
      "loss: 10.858948  [ 5600/12672]\n",
      "loss: 12.081985  [ 6000/12672]\n",
      "loss: 12.982862  [ 6400/12672]\n",
      "loss: 11.654093  [ 6800/12672]\n",
      "loss: 16.540211  [ 7200/12672]\n",
      "loss: 8.533106  [ 7600/12672]\n",
      "loss: 18.562454  [ 8000/12672]\n",
      "loss: 19.952366  [ 8400/12672]\n",
      "loss: 13.183201  [ 8800/12672]\n",
      "loss: 13.489519  [ 9200/12672]\n",
      "loss: 15.693146  [ 9600/12672]\n",
      "loss: 14.538270  [10000/12672]\n",
      "loss: 18.448578  [10400/12672]\n",
      "loss: 9.776441  [10800/12672]\n",
      "loss: 14.023926  [11200/12672]\n",
      "loss: 6.790817  [11600/12672]\n",
      "loss: 5.435196  [12000/12672]\n",
      "loss: 11.927088  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 9.422605 \n",
      "\n",
      "------------------------------\n",
      "Epoch 11\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.481699  [    0/12672]\n",
      "loss: 17.628441  [  400/12672]\n",
      "loss: 15.440799  [  800/12672]\n",
      "loss: 13.526672  [ 1200/12672]\n",
      "loss: 8.630747  [ 1600/12672]\n",
      "loss: 17.374836  [ 2000/12672]\n",
      "loss: 20.244081  [ 2400/12672]\n",
      "loss: 8.898341  [ 2800/12672]\n",
      "loss: 9.706555  [ 3200/12672]\n",
      "loss: 12.080648  [ 3600/12672]\n",
      "loss: 6.360147  [ 4000/12672]\n",
      "loss: 16.686150  [ 4400/12672]\n",
      "loss: 16.738291  [ 4800/12672]\n",
      "loss: 9.030946  [ 5200/12672]\n",
      "loss: 14.754167  [ 5600/12672]\n",
      "loss: 14.072790  [ 6000/12672]\n",
      "loss: 15.118283  [ 6400/12672]\n",
      "loss: 21.216120  [ 6800/12672]\n",
      "loss: 17.123138  [ 7200/12672]\n",
      "loss: 9.326149  [ 7600/12672]\n",
      "loss: 23.808596  [ 8000/12672]\n",
      "loss: 12.617850  [ 8400/12672]\n",
      "loss: 15.786592  [ 8800/12672]\n",
      "loss: 13.495055  [ 9200/12672]\n",
      "loss: 11.627166  [ 9600/12672]\n",
      "loss: 13.130214  [10000/12672]\n",
      "loss: 13.557033  [10400/12672]\n",
      "loss: 28.438143  [10800/12672]\n",
      "loss: 12.197895  [11200/12672]\n",
      "loss: 8.645572  [11600/12672]\n",
      "loss: 17.772137  [12000/12672]\n",
      "loss: 9.375681  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 14.795683 \n",
      "\n",
      "------------------------------\n",
      "Epoch 12\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.015186  [    0/12672]\n",
      "loss: 18.378218  [  400/12672]\n",
      "loss: 10.905426  [  800/12672]\n",
      "loss: 14.407141  [ 1200/12672]\n",
      "loss: 17.095621  [ 1600/12672]\n",
      "loss: 24.367382  [ 2000/12672]\n",
      "loss: 13.952176  [ 2400/12672]\n",
      "loss: 14.862652  [ 2800/12672]\n",
      "loss: 14.088938  [ 3200/12672]\n",
      "loss: 8.695137  [ 3600/12672]\n",
      "loss: 17.837742  [ 4000/12672]\n",
      "loss: 10.056675  [ 4400/12672]\n",
      "loss: 15.008612  [ 4800/12672]\n",
      "loss: 13.683960  [ 5200/12672]\n",
      "loss: 8.771029  [ 5600/12672]\n",
      "loss: 24.330189  [ 6000/12672]\n",
      "loss: 10.008524  [ 6400/12672]\n",
      "loss: 13.807898  [ 6800/12672]\n",
      "loss: 9.575724  [ 7200/12672]\n",
      "loss: 6.328605  [ 7600/12672]\n",
      "loss: 8.506556  [ 8000/12672]\n",
      "loss: 13.222223  [ 8400/12672]\n",
      "loss: 11.756744  [ 8800/12672]\n",
      "loss: 19.167419  [ 9200/12672]\n",
      "loss: 14.429133  [ 9600/12672]\n",
      "loss: 13.435739  [10000/12672]\n",
      "loss: 11.294412  [10400/12672]\n",
      "loss: 13.987287  [10800/12672]\n",
      "loss: 7.798900  [11200/12672]\n",
      "loss: 16.432383  [11600/12672]\n",
      "loss: 14.301639  [12000/12672]\n",
      "loss: 17.070663  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 24.183126 \n",
      "\n",
      "------------------------------\n",
      "Epoch 13\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 24.290478  [    0/12672]\n",
      "loss: 12.562439  [  400/12672]\n",
      "loss: 20.123627  [  800/12672]\n",
      "loss: 16.138966  [ 1200/12672]\n",
      "loss: 12.933722  [ 1600/12672]\n",
      "loss: 14.098883  [ 2000/12672]\n",
      "loss: 14.329038  [ 2400/12672]\n",
      "loss: 19.382090  [ 2800/12672]\n",
      "loss: 14.456590  [ 3200/12672]\n",
      "loss: 9.724588  [ 3600/12672]\n",
      "loss: 21.644138  [ 4000/12672]\n",
      "loss: 5.628462  [ 4400/12672]\n",
      "loss: 13.378937  [ 4800/12672]\n",
      "loss: 11.573664  [ 5200/12672]\n",
      "loss: 20.805998  [ 5600/12672]\n",
      "loss: 12.693164  [ 6000/12672]\n",
      "loss: 10.590127  [ 6400/12672]\n",
      "loss: 11.371552  [ 6800/12672]\n",
      "loss: 9.033211  [ 7200/12672]\n",
      "loss: 10.337649  [ 7600/12672]\n",
      "loss: 16.199575  [ 8000/12672]\n",
      "loss: 9.331702  [ 8400/12672]\n",
      "loss: 6.877408  [ 8800/12672]\n",
      "loss: 11.971825  [ 9200/12672]\n",
      "loss: 6.992371  [ 9600/12672]\n",
      "loss: 10.481929  [10000/12672]\n",
      "loss: 16.138960  [10400/12672]\n",
      "loss: 12.549189  [10800/12672]\n",
      "loss: 11.699098  [11200/12672]\n",
      "loss: 6.820444  [11600/12672]\n",
      "loss: 10.163265  [12000/12672]\n",
      "loss: 7.893771  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 14.835085 \n",
      "\n",
      "------------------------------\n",
      "Epoch 14\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.221777  [    0/12672]\n",
      "loss: 18.183083  [  400/12672]\n",
      "loss: 12.206839  [  800/12672]\n",
      "loss: 8.189983  [ 1200/12672]\n",
      "loss: 7.791025  [ 1600/12672]\n",
      "loss: 15.121408  [ 2000/12672]\n",
      "loss: 10.265409  [ 2400/12672]\n",
      "loss: 11.727020  [ 2800/12672]\n",
      "loss: 10.553771  [ 3200/12672]\n",
      "loss: 11.364791  [ 3600/12672]\n",
      "loss: 18.377714  [ 4000/12672]\n",
      "loss: 12.504489  [ 4400/12672]\n",
      "loss: 11.490378  [ 4800/12672]\n",
      "loss: 13.047885  [ 5200/12672]\n",
      "loss: 12.386039  [ 5600/12672]\n",
      "loss: 12.195374  [ 6000/12672]\n",
      "loss: 15.164351  [ 6400/12672]\n",
      "loss: 20.665071  [ 6800/12672]\n",
      "loss: 17.923084  [ 7200/12672]\n",
      "loss: 14.874378  [ 7600/12672]\n",
      "loss: 18.247149  [ 8000/12672]\n",
      "loss: 9.630839  [ 8400/12672]\n",
      "loss: 15.790625  [ 8800/12672]\n",
      "loss: 10.902757  [ 9200/12672]\n",
      "loss: 14.080828  [ 9600/12672]\n",
      "loss: 8.631372  [10000/12672]\n",
      "loss: 9.469222  [10400/12672]\n",
      "loss: 11.847634  [10800/12672]\n",
      "loss: 11.521795  [11200/12672]\n",
      "loss: 9.161792  [11600/12672]\n",
      "loss: 12.765641  [12000/12672]\n",
      "loss: 19.950039  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 11.334845 \n",
      "\n",
      "------------------------------\n",
      "Epoch 15\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.034005  [    0/12672]\n",
      "loss: 24.054441  [  400/12672]\n",
      "loss: 18.953207  [  800/12672]\n",
      "loss: 17.820242  [ 1200/12672]\n",
      "loss: 15.984304  [ 1600/12672]\n",
      "loss: 9.668208  [ 2000/12672]\n",
      "loss: 18.507774  [ 2400/12672]\n",
      "loss: 14.505383  [ 2800/12672]\n",
      "loss: 13.899640  [ 3200/12672]\n",
      "loss: 15.674949  [ 3600/12672]\n",
      "loss: 14.519579  [ 4000/12672]\n",
      "loss: 9.527836  [ 4400/12672]\n",
      "loss: 9.200832  [ 4800/12672]\n",
      "loss: 11.880459  [ 5200/12672]\n",
      "loss: 13.229123  [ 5600/12672]\n",
      "loss: 15.025188  [ 6000/12672]\n",
      "loss: 14.049607  [ 6400/12672]\n",
      "loss: 12.471589  [ 6800/12672]\n",
      "loss: 11.801819  [ 7200/12672]\n",
      "loss: 7.602612  [ 7600/12672]\n",
      "loss: 7.913931  [ 8000/12672]\n",
      "loss: 7.411782  [ 8400/12672]\n",
      "loss: 20.745857  [ 8800/12672]\n",
      "loss: 16.161396  [ 9200/12672]\n",
      "loss: 10.264732  [ 9600/12672]\n",
      "loss: 10.733447  [10000/12672]\n",
      "loss: 10.308656  [10400/12672]\n",
      "loss: 9.667546  [10800/12672]\n",
      "loss: 16.173214  [11200/12672]\n",
      "loss: 6.567777  [11600/12672]\n",
      "loss: 17.029745  [12000/12672]\n",
      "loss: 16.983877  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 12.249934 \n",
      "\n",
      "Done!\n",
      "CPU times: user 1.88 s, sys: 182 ms, total: 2.06 s\n",
      "Wall time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 15\n",
    "for t in range(epochs):\n",
    "    print(f\"{'-'*30}\\nEpoch {t+1}\\n{'-'*30}\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(train_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "fa642a60-58bb-4586-846f-ab3748391f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2556009 , -0.11014569, -0.34770104, -0.05363564, -0.32045805,\n",
       "        -0.20047373,  0.04354731, -0.07021297, -0.2549383 , -0.08638977,\n",
       "        -0.12272437, -0.26145813, -0.132995  , -0.5590581 , -0.13800876,\n",
       "        -0.39361808, -0.18007791, -0.14691398, -0.11113048,  0.02109414,\n",
       "        -0.02971033, -0.06245779, -0.23700692, -0.0312883 , -0.05845731,\n",
       "         0.14545608, -0.22800045,  0.13870333, -0.2055197 , -0.5122873 ,\n",
       "        -0.02531125,  0.07469609,  0.00397547, -0.01759478, -0.16063288,\n",
       "        -0.04735122, -0.12375901, -0.19227822, -0.05999327, -0.04843619,\n",
       "        -0.2967028 ,  0.00551004,  0.03163636, -0.06197273,  0.01000183,\n",
       "        -0.21084203, -0.10228419, -0.07310296,  0.11168385, -0.23657438,\n",
       "        -0.2252271 , -0.30507943, -0.02275092, -0.31863666, -0.14079633,\n",
       "        -0.06563782,  0.15927301, -0.18031234,  0.01695587, -0.09139688,\n",
       "        -0.18292908, -0.11702715, -0.5671819 ,  0.15823328, -0.11645681,\n",
       "         0.01410398, -0.08147889, -0.09058848, -0.22371317, -0.10227858,\n",
       "        -0.07714693, -0.1457704 , -0.22057438, -0.20044486,  0.01164881,\n",
       "        -0.33475474, -0.00665838, -0.0153681 , -0.18155982, -0.24766387,\n",
       "        -0.22887512, -0.16248038, -0.16140792, -0.24882337, -0.21806884,\n",
       "        -0.01768024, -0.05802217,  0.03899268, -0.1101112 , -0.02605398,\n",
       "         0.005957  , -0.12131958, -0.03348484, -0.08030755, -0.18201545,\n",
       "        -0.22871569,  0.05486714, -0.02430068, -0.13328029, -0.23076108,\n",
       "        -0.13126285, -0.0348118 , -0.25455052,  0.01738402, -0.08646844,\n",
       "        -0.13921174, -0.01897634, -0.18064915,  0.03457617, -0.2869255 ,\n",
       "         0.08858623, -0.23258016, -0.15405336, -0.05003629,  0.04085288,\n",
       "        -0.26723537,  0.03861602, -0.3729332 ,  0.02717902, -0.2995317 ,\n",
       "        -0.17331788, -0.04101282, -0.07718829, -0.08079364, -0.07807177,\n",
       "        -0.27141556, -0.29208753,  0.01224815]], dtype=float32)"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p for p in model.parameters()][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ce08d-e451-45d9-b3fc-a28c2bedf854",
   "metadata": {},
   "source": [
    "## Compare to sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "23c67416-86b6-4434-a7e7-1e8c1a238572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "523260ef-dec1-468c-ab02-fe635822b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.2 ms, sys: 42.1 ms, total: 100 ms\n",
      "Wall time: 33.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sk_model = LogisticRegression(penalty='l2', C=1., class_weight='balanced', fit_intercept=True)\n",
    "\n",
    "train_idx = train_set.indices\n",
    "test_idx = test_set.indices\n",
    "sk_train_X = train_set.dataset.X[train_idx].detach().numpy()\n",
    "sk_train_y = train_set.dataset.y[train_idx].detach().numpy()\n",
    "sk_test_X = test_set.dataset.X[test_idx].detach().numpy()\n",
    "sk_test_y = test_set.dataset.y[test_idx].detach().numpy()\n",
    "fit = sk_model.fit(sk_train_X, sk_train_y)\n",
    "\n",
    "sk_pred = fit.predict(sk_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "5f6c6a99-81a3-4ea5-9786-93a96a6a24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "58151025-efbe-4cce-afff-9a5c7e397716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(sk_test_y, sk_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "7a1afe66-561c-4162-bf56-5b951363d036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.37388629, -0.32047521, -0.4095046 , -0.32130418, -0.39439804,\n",
       "        -0.35823253, -0.33484078, -0.33933441, -0.37543385, -0.36355242,\n",
       "        -0.34292282, -0.37185879, -0.3706311 , -0.44209721, -0.36840507,\n",
       "        -0.34789623, -0.41266399, -0.36281521, -0.36685014, -0.3846179 ,\n",
       "        -0.31708437, -0.31090994, -0.37258387, -0.37406865, -0.36461087,\n",
       "        -0.28830832, -0.41198973, -0.3104652 , -0.35061659, -0.40379479,\n",
       "        -0.33055622, -0.33311162, -0.35796505, -0.35008685, -0.35916163,\n",
       "        -0.35937264, -0.39998844, -0.3649274 , -0.37894635, -0.35347019,\n",
       "        -0.37401855, -0.3663856 , -0.30936565, -0.37154138, -0.32723201,\n",
       "        -0.36783979, -0.39499198, -0.3303809 , -0.33748294, -0.36837342,\n",
       "        -0.39150569, -0.36852701, -0.3572353 , -0.37949994, -0.33189788,\n",
       "        -0.40627196, -0.35212517, -0.37479089, -0.33094589, -0.37000063,\n",
       "        -0.3751616 , -0.33788822, -0.37261937, -0.31014997, -0.34313105,\n",
       "        -0.35801088, -0.36816738, -0.34082888, -0.3879882 , -0.3457667 ,\n",
       "        -0.36235728, -0.32733371, -0.39650571, -0.39248756, -0.32821235,\n",
       "        -0.3990739 , -0.30894712, -0.34339028, -0.34547438, -0.35086806,\n",
       "        -0.40395907, -0.37280974, -0.31477501, -0.42060536, -0.38097354,\n",
       "        -0.3758629 , -0.35815151, -0.34744084, -0.32995486, -0.35151104,\n",
       "        -0.32959605, -0.37197039, -0.3986891 , -0.3753245 , -0.39787029,\n",
       "        -0.39702534, -0.35969944, -0.32618243, -0.37155474, -0.40618902,\n",
       "        -0.38696675, -0.34085002, -0.37607757, -0.38614888, -0.39346724,\n",
       "        -0.38347606, -0.33018366, -0.37635166, -0.3128696 , -0.39317084,\n",
       "        -0.31900011, -0.37878595, -0.37034107, -0.33262278, -0.28697747,\n",
       "        -0.34756017, -0.35172575, -0.38714172, -0.33827419, -0.41180401,\n",
       "        -0.31481693, -0.33421305, -0.31779403, -0.30730132, -0.3548753 ,\n",
       "        -0.42106957, -0.38216169, -0.32547378]])"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a23101-0bd3-4961-b112-e36be91814f4",
   "metadata": {},
   "source": [
    "## Using lab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55e7286-4c5d-48d6-85ac-6bc3d166cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptsa.data.timeseries import TimeSeries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be80b2f1-34c7-458e-88a0-a1c8ae83f825",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: b'/Users/jrudoler/rhino_mount/scratch/jrudoler/scalp_features/LTP093_feats.h5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/xarray/backends/file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/xarray/backends/lru_cache.py:53\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m---> 53\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/Users/jrudoler/rhino_mount/scratch/jrudoler/scalp_features/LTP093_feats.h5',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False))]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ts \u001b[38;5;241m=\u001b[39m \u001b[43mTimeSeries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_hdf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/jrudoler/rhino_mount/scratch/jrudoler/scalp_features/LTP093_feats.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/ptsa/data/timeseries.py:212\u001b[0m, in \u001b[0;36mTimeSeries.from_hdf\u001b[0;34m(cls, filename, engine, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must install h5py to load from HDF5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 212\u001b[0m xarr \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# legacy base64 reading using h5py\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xarr\u001b[38;5;241m.\u001b[39mattrs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman_readable\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/xarray/backends/api.py:495\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, backend_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    484\u001b[0m     decode_cf,\n\u001b[1;32m    485\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    492\u001b[0m )\n\u001b[1;32m    494\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 495\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    502\u001b[0m     backend_ds,\n\u001b[1;32m    503\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    511\u001b[0m )\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:550\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    531\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    547\u001b[0m ):\n\u001b[1;32m    549\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[0;32m--> 550\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:379\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[1;32m    373\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    374\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[1;32m    375\u001b[0m )\n\u001b[1;32m    376\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[1;32m    377\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    378\u001b[0m )\n\u001b[0;32m--> 379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:327\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[0;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:388\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/xarray/backends/netCDF4_.py:382\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[1;32m    383\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/xarray/backends/file_manager.py:187\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/xarray/backends/file_manager.py:205\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[0;34m(self, needs_lock)\u001b[0m\n\u001b[1;32m    203\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    204\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[0;32m--> 205\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_opener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overriden when opened again\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2353\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:1963\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: b'/Users/jrudoler/rhino_mount/scratch/jrudoler/scalp_features/LTP093_feats.h5'"
     ]
    }
   ],
   "source": [
    "ts = TimeSeries.from_hdf(\n",
    "    \"/Users/jrudoler/rhino_mount/scratch/jrudoler/scalp_features/LTP093_feats.h5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3edface1-79d9-4147-87e1-79869a0c0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(ts.data).float()\n",
    "y = torch.tensor(ts.recalled.data).float()\n",
    "dataset = SimpleDataset(X, y)\n",
    "sessions = ts.session.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e42c29f-5d38-4187-88cb-36414f76b390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "SESSION 0\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.721016  [    0/12672]\n",
      "loss: 22.850132  [ 1600/12672]\n",
      "loss: 10.057935  [ 3200/12672]\n",
      "loss: 16.579311  [ 4800/12672]\n",
      "loss: 26.484034  [ 6400/12672]\n",
      "loss: 9.983425  [ 8000/12672]\n",
      "loss: 8.536345  [ 9600/12672]\n",
      "loss: 19.049088  [11200/12672]\n",
      "0.5316748549533714\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 29.721613  [    0/12672]\n",
      "loss: 11.930724  [ 1600/12672]\n",
      "loss: 27.278183  [ 3200/12672]\n",
      "loss: 13.954221  [ 4800/12672]\n",
      "loss: 11.727523  [ 6400/12672]\n",
      "loss: 11.423249  [ 8000/12672]\n",
      "loss: 20.344629  [ 9600/12672]\n",
      "loss: 26.809420  [11200/12672]\n",
      "0.5693106610065168\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 14.110417  [    0/12672]\n",
      "loss: 20.571135  [ 1600/12672]\n",
      "loss: 18.280279  [ 3200/12672]\n",
      "loss: 17.496914  [ 4800/12672]\n",
      "loss: 19.399500  [ 6400/12672]\n",
      "loss: 11.755685  [ 8000/12672]\n",
      "loss: 22.565601  [ 9600/12672]\n",
      "loss: 11.250213  [11200/12672]\n",
      "0.546339264751607\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 15.273142  [    0/12672]\n",
      "loss: 10.298801  [ 1600/12672]\n",
      "loss: 9.876720  [ 3200/12672]\n",
      "loss: 29.668839  [ 4800/12672]\n",
      "loss: 13.404018  [ 6400/12672]\n",
      "loss: 12.005584  [ 8000/12672]\n",
      "loss: 10.770758  [ 9600/12672]\n",
      "loss: 11.004983  [11200/12672]\n",
      "0.5730040224925388\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 14.318136  [    0/12672]\n",
      "loss: 19.023825  [ 1600/12672]\n",
      "loss: 7.020838  [ 3200/12672]\n",
      "loss: 12.605739  [ 4800/12672]\n",
      "loss: 9.241146  [ 6400/12672]\n",
      "loss: 12.678673  [ 8000/12672]\n",
      "loss: 28.835930  [ 9600/12672]\n",
      "loss: 7.917662  [11200/12672]\n",
      "0.5644940606499019\n",
      "Done!\n",
      "##############################\n",
      "SESSION 1\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.687228  [    0/12672]\n",
      "loss: 19.875744  [ 1600/12672]\n",
      "loss: 8.747512  [ 3200/12672]\n",
      "loss: 17.998753  [ 4800/12672]\n",
      "loss: 16.343098  [ 6400/12672]\n",
      "loss: 10.184842  [ 8000/12672]\n",
      "loss: 7.878410  [ 9600/12672]\n",
      "loss: 27.105797  [11200/12672]\n",
      "0.5103446418741531\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 25.953585  [    0/12672]\n",
      "loss: 21.458694  [ 1600/12672]\n",
      "loss: 14.188264  [ 3200/12672]\n",
      "loss: 14.256552  [ 4800/12672]\n",
      "loss: 26.690630  [ 6400/12672]\n",
      "loss: 15.404265  [ 8000/12672]\n",
      "loss: 17.390175  [ 9600/12672]\n",
      "loss: 16.170721  [11200/12672]\n",
      "0.5617102811653116\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 20.626335  [    0/12672]\n",
      "loss: 19.022659  [ 1600/12672]\n",
      "loss: 22.458321  [ 3200/12672]\n",
      "loss: 17.170843  [ 4800/12672]\n",
      "loss: 9.309275  [ 6400/12672]\n",
      "loss: 12.726754  [ 8000/12672]\n",
      "loss: 17.252274  [ 9600/12672]\n",
      "loss: 14.164667  [11200/12672]\n",
      "0.5951961196858062\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 12.723274  [    0/12672]\n",
      "loss: 14.635220  [ 1600/12672]\n",
      "loss: 6.648695  [ 3200/12672]\n",
      "loss: 11.361340  [ 4800/12672]\n",
      "loss: 11.269244  [ 6400/12672]\n",
      "loss: 7.378933  [ 8000/12672]\n",
      "loss: 15.934251  [ 9600/12672]\n",
      "loss: 7.436906  [11200/12672]\n",
      "0.6457842934874661\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 5.192892  [    0/12672]\n",
      "loss: 9.115146  [ 1600/12672]\n",
      "loss: 18.305914  [ 3200/12672]\n",
      "loss: 12.454182  [ 4800/12672]\n",
      "loss: 13.796861  [ 6400/12672]\n",
      "loss: 15.515947  [ 8000/12672]\n",
      "loss: 12.512169  [ 9600/12672]\n",
      "loss: 9.270020  [11200/12672]\n",
      "0.5487573837652439\n",
      "Done!\n",
      "##############################\n",
      "SESSION 2\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.701596  [    0/12672]\n",
      "loss: 17.350756  [ 1600/12672]\n",
      "loss: 22.781651  [ 3200/12672]\n",
      "loss: 11.527157  [ 4800/12672]\n",
      "loss: 23.478666  [ 6400/12672]\n",
      "loss: 17.075212  [ 8000/12672]\n",
      "loss: 23.543798  [ 9600/12672]\n",
      "loss: 6.913842  [11200/12672]\n",
      "0.5395081045322037\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 20.484621  [    0/12672]\n",
      "loss: 21.762039  [ 1600/12672]\n",
      "loss: 5.263704  [ 3200/12672]\n",
      "loss: 25.891550  [ 4800/12672]\n",
      "loss: 16.063869  [ 6400/12672]\n",
      "loss: 23.266768  [ 8000/12672]\n",
      "loss: 11.245349  [ 9600/12672]\n",
      "loss: 13.049072  [11200/12672]\n",
      "0.5249239747796562\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 15.689116  [    0/12672]\n",
      "loss: 9.716855  [ 1600/12672]\n",
      "loss: 12.734550  [ 3200/12672]\n",
      "loss: 18.799772  [ 4800/12672]\n",
      "loss: 10.811963  [ 6400/12672]\n",
      "loss: 24.465553  [ 8000/12672]\n",
      "loss: 17.497713  [ 9600/12672]\n",
      "loss: 21.942192  [11200/12672]\n",
      "0.538080922187466\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 19.178940  [    0/12672]\n",
      "loss: 15.781513  [ 1600/12672]\n",
      "loss: 6.753122  [ 3200/12672]\n",
      "loss: 10.158736  [ 4800/12672]\n",
      "loss: 14.657626  [ 6400/12672]\n",
      "loss: 20.945374  [ 8000/12672]\n",
      "loss: 7.551339  [ 9600/12672]\n",
      "loss: 11.938916  [11200/12672]\n",
      "0.6300580424647177\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 9.896620  [    0/12672]\n",
      "loss: 16.737621  [ 1600/12672]\n",
      "loss: 17.757776  [ 3200/12672]\n",
      "loss: 6.749499  [ 4800/12672]\n",
      "loss: 16.134863  [ 6400/12672]\n",
      "loss: 18.939734  [ 8000/12672]\n",
      "loss: 11.366107  [ 9600/12672]\n",
      "loss: 6.941980  [11200/12672]\n",
      "0.5070671340955686\n",
      "Done!\n",
      "##############################\n",
      "SESSION 3\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.738877  [    0/12672]\n",
      "loss: 19.288973  [ 1600/12672]\n",
      "loss: 26.313347  [ 3200/12672]\n",
      "loss: 14.670187  [ 4800/12672]\n",
      "loss: 24.549599  [ 6400/12672]\n",
      "loss: 7.225990  [ 8000/12672]\n",
      "loss: 18.539602  [ 9600/12672]\n",
      "loss: 13.569042  [11200/12672]\n",
      "0.5502265807396012\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 20.863386  [    0/12672]\n",
      "loss: 9.345115  [ 1600/12672]\n",
      "loss: 13.745526  [ 3200/12672]\n",
      "loss: 10.879867  [ 4800/12672]\n",
      "loss: 14.572876  [ 6400/12672]\n",
      "loss: 10.720279  [ 8000/12672]\n",
      "loss: 18.917435  [ 9600/12672]\n",
      "loss: 23.328117  [11200/12672]\n",
      "0.5829785687161992\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 11.553006  [    0/12672]\n",
      "loss: 7.373527  [ 1600/12672]\n",
      "loss: 15.019318  [ 3200/12672]\n",
      "loss: 18.913475  [ 4800/12672]\n",
      "loss: 14.347844  [ 6400/12672]\n",
      "loss: 18.000086  [ 8000/12672]\n",
      "loss: 24.244480  [ 9600/12672]\n",
      "loss: 6.984986  [11200/12672]\n",
      "0.5223241391458031\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 29.709600  [    0/12672]\n",
      "loss: 12.289995  [ 1600/12672]\n",
      "loss: 15.178457  [ 3200/12672]\n",
      "loss: 8.934763  [ 4800/12672]\n",
      "loss: 7.972679  [ 6400/12672]\n",
      "loss: 10.899614  [ 8000/12672]\n",
      "loss: 25.521286  [ 9600/12672]\n",
      "loss: 16.421232  [11200/12672]\n",
      "0.595661732763336\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 9.712137  [    0/12672]\n",
      "loss: 7.205117  [ 1600/12672]\n",
      "loss: 13.623596  [ 3200/12672]\n",
      "loss: 19.294458  [ 4800/12672]\n",
      "loss: 12.353164  [ 6400/12672]\n",
      "loss: 14.436852  [ 8000/12672]\n",
      "loss: 18.274639  [ 9600/12672]\n",
      "loss: 6.215447  [11200/12672]\n",
      "0.541687946891342\n",
      "Done!\n",
      "##############################\n",
      "SESSION 4\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.782205  [    0/12672]\n",
      "loss: 36.947739  [ 1600/12672]\n",
      "loss: 10.997159  [ 3200/12672]\n",
      "loss: 5.265156  [ 4800/12672]\n",
      "loss: 19.963821  [ 6400/12672]\n",
      "loss: 12.998231  [ 8000/12672]\n",
      "loss: 6.376485  [ 9600/12672]\n",
      "loss: 9.335332  [11200/12672]\n",
      "0.5808130962682878\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 13.344580  [    0/12672]\n",
      "loss: 7.131467  [ 1600/12672]\n",
      "loss: 7.461768  [ 3200/12672]\n",
      "loss: 5.147999  [ 4800/12672]\n",
      "loss: 10.682181  [ 6400/12672]\n",
      "loss: 13.143250  [ 8000/12672]\n",
      "loss: 14.232147  [ 9600/12672]\n",
      "loss: 21.194756  [11200/12672]\n",
      "0.588024812007639\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 18.720442  [    0/12672]\n",
      "loss: 17.178434  [ 1600/12672]\n",
      "loss: 20.165932  [ 3200/12672]\n",
      "loss: 6.920156  [ 4800/12672]\n",
      "loss: 15.724835  [ 6400/12672]\n",
      "loss: 24.712744  [ 8000/12672]\n",
      "loss: 8.865678  [ 9600/12672]\n",
      "loss: 9.575326  [11200/12672]\n",
      "0.5395814638892678\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 20.076414  [    0/12672]\n",
      "loss: 7.847543  [ 1600/12672]\n",
      "loss: 9.656802  [ 3200/12672]\n",
      "loss: 11.276900  [ 4800/12672]\n",
      "loss: 20.846252  [ 6400/12672]\n",
      "loss: 14.400678  [ 8000/12672]\n",
      "loss: 19.097452  [ 9600/12672]\n",
      "loss: 25.415869  [11200/12672]\n",
      "0.6302970630989326\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 8.299976  [    0/12672]\n",
      "loss: 20.548164  [ 1600/12672]\n",
      "loss: 8.777724  [ 3200/12672]\n",
      "loss: 11.296898  [ 4800/12672]\n",
      "loss: 6.390085  [ 6400/12672]\n",
      "loss: 20.040886  [ 8000/12672]\n",
      "loss: 6.853412  [ 9600/12672]\n",
      "loss: 21.826765  [11200/12672]\n",
      "0.574282133478839\n",
      "Done!\n",
      "##############################\n",
      "SESSION 5\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.763523  [    0/12672]\n",
      "loss: 15.909379  [ 1600/12672]\n",
      "loss: 12.326782  [ 3200/12672]\n",
      "loss: 4.305189  [ 4800/12672]\n",
      "loss: 10.568667  [ 6400/12672]\n",
      "loss: 22.532907  [ 8000/12672]\n",
      "loss: 20.676390  [ 9600/12672]\n",
      "loss: 21.355627  [11200/12672]\n",
      "0.593275920473338\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 4.379077  [    0/12672]\n",
      "loss: 15.405535  [ 1600/12672]\n",
      "loss: 23.251083  [ 3200/12672]\n",
      "loss: 28.229702  [ 4800/12672]\n",
      "loss: 23.145889  [ 6400/12672]\n",
      "loss: 23.858437  [ 8000/12672]\n",
      "loss: 29.760733  [ 9600/12672]\n",
      "loss: 20.367277  [11200/12672]\n",
      "0.5449104461790539\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 12.539925  [    0/12672]\n",
      "loss: 15.164246  [ 1600/12672]\n",
      "loss: 7.706423  [ 3200/12672]\n",
      "loss: 22.864779  [ 4800/12672]\n",
      "loss: 25.256987  [ 6400/12672]\n",
      "loss: 17.711308  [ 8000/12672]\n",
      "loss: 22.086277  [ 9600/12672]\n",
      "loss: 12.795118  [11200/12672]\n",
      "0.6100972935494082\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 13.175342  [    0/12672]\n",
      "loss: 24.354107  [ 1600/12672]\n",
      "loss: 17.352879  [ 3200/12672]\n",
      "loss: 8.940426  [ 4800/12672]\n",
      "loss: 24.603264  [ 6400/12672]\n",
      "loss: 11.944958  [ 8000/12672]\n",
      "loss: 8.810482  [ 9600/12672]\n",
      "loss: 17.718889  [11200/12672]\n",
      "0.5364071389156425\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 29.255562  [    0/12672]\n",
      "loss: 13.238672  [ 1600/12672]\n",
      "loss: 5.968396  [ 3200/12672]\n",
      "loss: 10.400799  [ 4800/12672]\n",
      "loss: 23.623098  [ 6400/12672]\n",
      "loss: 9.299472  [ 8000/12672]\n",
      "loss: 11.185738  [ 9600/12672]\n",
      "loss: 22.595692  [11200/12672]\n",
      "0.5232228880993528\n",
      "Done!\n",
      "##############################\n",
      "SESSION 6\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.688340  [    0/12672]\n",
      "loss: 19.430872  [ 1600/12672]\n",
      "loss: 20.404617  [ 3200/12672]\n",
      "loss: 29.317896  [ 4800/12672]\n",
      "loss: 10.780964  [ 6400/12672]\n",
      "loss: 11.514051  [ 8000/12672]\n",
      "loss: 7.875288  [ 9600/12672]\n",
      "loss: 20.011694  [11200/12672]\n",
      "0.4938914016928208\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 20.586092  [    0/12672]\n",
      "loss: 24.942238  [ 1600/12672]\n",
      "loss: 14.877955  [ 3200/12672]\n",
      "loss: 5.029113  [ 4800/12672]\n",
      "loss: 19.983681  [ 6400/12672]\n",
      "loss: 15.260437  [ 8000/12672]\n",
      "loss: 10.690122  [ 9600/12672]\n",
      "loss: 15.343332  [11200/12672]\n",
      "0.608309677247904\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 6.676093  [    0/12672]\n",
      "loss: 6.036421  [ 1600/12672]\n",
      "loss: 13.459931  [ 3200/12672]\n",
      "loss: 19.098757  [ 4800/12672]\n",
      "loss: 21.758202  [ 6400/12672]\n",
      "loss: 19.460579  [ 8000/12672]\n",
      "loss: 10.432228  [ 9600/12672]\n",
      "loss: 16.425282  [11200/12672]\n",
      "0.5942427749505043\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 13.493686  [    0/12672]\n",
      "loss: 15.238866  [ 1600/12672]\n",
      "loss: 18.754326  [ 3200/12672]\n",
      "loss: 9.851706  [ 4800/12672]\n",
      "loss: 18.152594  [ 6400/12672]\n",
      "loss: 17.008207  [ 8000/12672]\n",
      "loss: 12.450886  [ 9600/12672]\n",
      "loss: 13.560385  [11200/12672]\n",
      "0.5974275634807797\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 16.221687  [    0/12672]\n",
      "loss: 17.754581  [ 1600/12672]\n",
      "loss: 27.058088  [ 3200/12672]\n",
      "loss: 11.464391  [ 4800/12672]\n",
      "loss: 13.844875  [ 6400/12672]\n",
      "loss: 19.256638  [ 8000/12672]\n",
      "loss: 16.658474  [ 9600/12672]\n",
      "loss: 10.393506  [11200/12672]\n",
      "0.5896239785274852\n",
      "Done!\n",
      "##############################\n",
      "SESSION 7\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.718238  [    0/12672]\n",
      "loss: 16.105543  [ 1600/12672]\n",
      "loss: 23.040144  [ 3200/12672]\n",
      "loss: 9.308678  [ 4800/12672]\n",
      "loss: 14.097317  [ 6400/12672]\n",
      "loss: 10.519181  [ 8000/12672]\n",
      "loss: 6.012815  [ 9600/12672]\n",
      "loss: 17.254026  [11200/12672]\n",
      "0.5476661902585549\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 10.577831  [    0/12672]\n",
      "loss: 17.241005  [ 1600/12672]\n",
      "loss: 9.094709  [ 3200/12672]\n",
      "loss: 5.396731  [ 4800/12672]\n",
      "loss: 11.880184  [ 6400/12672]\n",
      "loss: 17.530706  [ 8000/12672]\n",
      "loss: 9.438611  [ 9600/12672]\n",
      "loss: 10.789948  [11200/12672]\n",
      "0.5300729406356746\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 26.459980  [    0/12672]\n",
      "loss: 6.006625  [ 1600/12672]\n",
      "loss: 21.169167  [ 3200/12672]\n",
      "loss: 15.270145  [ 4800/12672]\n",
      "loss: 12.387165  [ 6400/12672]\n",
      "loss: 4.556038  [ 8000/12672]\n",
      "loss: 16.126827  [ 9600/12672]\n",
      "loss: 11.099679  [11200/12672]\n",
      "0.5867022299598179\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 15.703855  [    0/12672]\n",
      "loss: 8.645652  [ 1600/12672]\n",
      "loss: 27.466805  [ 3200/12672]\n",
      "loss: 20.035881  [ 4800/12672]\n",
      "loss: 19.534075  [ 6400/12672]\n",
      "loss: 24.710691  [ 8000/12672]\n",
      "loss: 23.055910  [ 9600/12672]\n",
      "loss: 17.431627  [11200/12672]\n",
      "0.6094365791607996\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 16.201805  [    0/12672]\n",
      "loss: 15.310843  [ 1600/12672]\n",
      "loss: 9.172530  [ 3200/12672]\n",
      "loss: 8.149611  [ 4800/12672]\n",
      "loss: 22.119211  [ 6400/12672]\n",
      "loss: 12.050218  [ 8000/12672]\n",
      "loss: 19.463655  [ 9600/12672]\n",
      "loss: 20.066685  [11200/12672]\n",
      "0.594886547062693\n",
      "Done!\n",
      "##############################\n",
      "SESSION 8\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.755820  [    0/12672]\n",
      "loss: 7.502923  [ 1600/12672]\n",
      "loss: 12.690956  [ 3200/12672]\n",
      "loss: 19.866735  [ 4800/12672]\n",
      "loss: 18.006834  [ 6400/12672]\n",
      "loss: 7.734943  [ 8000/12672]\n",
      "loss: 7.291768  [ 9600/12672]\n",
      "loss: 15.260139  [11200/12672]\n",
      "0.5335400918992796\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 15.870392  [    0/12672]\n",
      "loss: 13.908164  [ 1600/12672]\n",
      "loss: 7.589430  [ 3200/12672]\n",
      "loss: 6.268807  [ 4800/12672]\n",
      "loss: 6.839711  [ 6400/12672]\n",
      "loss: 3.557145  [ 8000/12672]\n",
      "loss: 13.227451  [ 9600/12672]\n",
      "loss: 21.022627  [11200/12672]\n",
      "0.5847432636486365\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 14.968101  [    0/12672]\n",
      "loss: 25.588781  [ 1600/12672]\n",
      "loss: 9.799331  [ 3200/12672]\n",
      "loss: 15.926330  [ 4800/12672]\n",
      "loss: 10.820270  [ 6400/12672]\n",
      "loss: 8.863890  [ 8000/12672]\n",
      "loss: 17.837257  [ 9600/12672]\n",
      "loss: 6.138944  [11200/12672]\n",
      "0.5484241269097584\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 26.777342  [    0/12672]\n",
      "loss: 13.720957  [ 1600/12672]\n",
      "loss: 20.871635  [ 3200/12672]\n",
      "loss: 17.080368  [ 4800/12672]\n",
      "loss: 10.766607  [ 6400/12672]\n",
      "loss: 5.213923  [ 8000/12672]\n",
      "loss: 15.383943  [ 9600/12672]\n",
      "loss: 10.980563  [11200/12672]\n",
      "0.5058653191054194\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 25.129173  [    0/12672]\n",
      "loss: 14.125596  [ 1600/12672]\n",
      "loss: 9.058868  [ 3200/12672]\n",
      "loss: 23.105118  [ 4800/12672]\n",
      "loss: 8.640703  [ 6400/12672]\n",
      "loss: 11.322698  [ 8000/12672]\n",
      "loss: 5.358362  [ 9600/12672]\n",
      "loss: 13.624813  [11200/12672]\n",
      "0.62380752024204\n",
      "Done!\n",
      "##############################\n",
      "SESSION 9\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.740626  [    0/12672]\n",
      "loss: 21.194595  [ 1600/12672]\n",
      "loss: 11.523496  [ 3200/12672]\n",
      "loss: 30.947725  [ 4800/12672]\n",
      "loss: 17.604111  [ 6400/12672]\n",
      "loss: 10.947662  [ 8000/12672]\n",
      "loss: 26.989180  [ 9600/12672]\n",
      "loss: 17.951004  [11200/12672]\n",
      "0.531057256153458\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 25.693657  [    0/12672]\n",
      "loss: 12.409550  [ 1600/12672]\n",
      "loss: 6.225671  [ 3200/12672]\n",
      "loss: 8.302035  [ 4800/12672]\n",
      "loss: 13.462088  [ 6400/12672]\n",
      "loss: 17.809620  [ 8000/12672]\n",
      "loss: 20.589182  [ 9600/12672]\n",
      "loss: 14.637908  [11200/12672]\n",
      "0.5412894060397277\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 24.150942  [    0/12672]\n",
      "loss: 18.615974  [ 1600/12672]\n",
      "loss: 15.143686  [ 3200/12672]\n",
      "loss: 4.313818  [ 4800/12672]\n",
      "loss: 14.988662  [ 6400/12672]\n",
      "loss: 13.387102  [ 8000/12672]\n",
      "loss: 6.658466  [ 9600/12672]\n",
      "loss: 12.863069  [11200/12672]\n",
      "0.5225417869795286\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 12.924648  [    0/12672]\n",
      "loss: 6.963737  [ 1600/12672]\n",
      "loss: 12.347708  [ 3200/12672]\n",
      "loss: 19.357975  [ 4800/12672]\n",
      "loss: 19.771944  [ 6400/12672]\n",
      "loss: 22.710318  [ 8000/12672]\n",
      "loss: 8.942837  [ 9600/12672]\n",
      "loss: 9.645513  [11200/12672]\n",
      "0.5866061601946493\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 19.846104  [    0/12672]\n",
      "loss: 14.714376  [ 1600/12672]\n",
      "loss: 8.119453  [ 3200/12672]\n",
      "loss: 23.495024  [ 4800/12672]\n",
      "loss: 14.723088  [ 6400/12672]\n",
      "loss: 12.396424  [ 8000/12672]\n",
      "loss: 16.327621  [ 9600/12672]\n",
      "loss: 10.646694  [11200/12672]\n",
      "0.5950090350226007\n",
      "Done!\n",
      "##############################\n",
      "SESSION 10\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.716263  [    0/12672]\n",
      "loss: 12.904934  [ 1600/12672]\n",
      "loss: 19.282761  [ 3200/12672]\n",
      "loss: 16.916548  [ 4800/12672]\n",
      "loss: 18.806736  [ 6400/12672]\n",
      "loss: 7.536162  [ 8000/12672]\n",
      "loss: 30.598984  [ 9600/12672]\n",
      "loss: 14.516104  [11200/12672]\n",
      "0.5036464563605478\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 20.042921  [    0/12672]\n",
      "loss: 14.242086  [ 1600/12672]\n",
      "loss: 8.290759  [ 3200/12672]\n",
      "loss: 20.609629  [ 4800/12672]\n",
      "loss: 19.586464  [ 6400/12672]\n",
      "loss: 16.464071  [ 8000/12672]\n",
      "loss: 5.059455  [ 9600/12672]\n",
      "loss: 22.964067  [11200/12672]\n",
      "0.5245522096932064\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 21.063593  [    0/12672]\n",
      "loss: 15.237995  [ 1600/12672]\n",
      "loss: 18.265152  [ 3200/12672]\n",
      "loss: 11.629599  [ 4800/12672]\n",
      "loss: 23.490286  [ 6400/12672]\n",
      "loss: 15.031594  [ 8000/12672]\n",
      "loss: 4.834925  [ 9600/12672]\n",
      "loss: 17.276646  [11200/12672]\n",
      "0.5487274324008047\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 16.264826  [    0/12672]\n",
      "loss: 20.149578  [ 1600/12672]\n",
      "loss: 16.476257  [ 3200/12672]\n",
      "loss: 12.116885  [ 4800/12672]\n",
      "loss: 18.075146  [ 6400/12672]\n",
      "loss: 29.492592  [ 8000/12672]\n",
      "loss: 19.571472  [ 9600/12672]\n",
      "loss: 19.157001  [11200/12672]\n",
      "0.5500894345951561\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 13.178333  [    0/12672]\n",
      "loss: 11.413595  [ 1600/12672]\n",
      "loss: 12.319040  [ 3200/12672]\n",
      "loss: 24.991316  [ 4800/12672]\n",
      "loss: 20.794178  [ 6400/12672]\n",
      "loss: 7.961787  [ 8000/12672]\n",
      "loss: 15.308331  [ 9600/12672]\n",
      "loss: 21.533676  [11200/12672]\n",
      "0.6021729471872725\n",
      "Done!\n",
      "##############################\n",
      "SESSION 11\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.711277  [    0/12672]\n",
      "loss: 27.304083  [ 1600/12672]\n",
      "loss: 13.102261  [ 3200/12672]\n",
      "loss: 12.274546  [ 4800/12672]\n",
      "loss: 22.425423  [ 6400/12672]\n",
      "loss: 9.331828  [ 8000/12672]\n",
      "loss: 10.223370  [ 9600/12672]\n",
      "loss: 20.661455  [11200/12672]\n",
      "0.5902088795166087\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 4.165588  [    0/12672]\n",
      "loss: 9.570224  [ 1600/12672]\n",
      "loss: 13.488276  [ 3200/12672]\n",
      "loss: 16.511879  [ 4800/12672]\n",
      "loss: 5.476103  [ 6400/12672]\n",
      "loss: 21.575359  [ 8000/12672]\n",
      "loss: 17.410376  [ 9600/12672]\n",
      "loss: 10.029428  [11200/12672]\n",
      "0.5234573898682913\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 19.962250  [    0/12672]\n",
      "loss: 18.017298  [ 1600/12672]\n",
      "loss: 8.300734  [ 3200/12672]\n",
      "loss: 19.899712  [ 4800/12672]\n",
      "loss: 6.978439  [ 6400/12672]\n",
      "loss: 24.763063  [ 8000/12672]\n",
      "loss: 5.589539  [ 9600/12672]\n",
      "loss: 10.012862  [11200/12672]\n",
      "0.6190233332766943\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 6.429974  [    0/12672]\n",
      "loss: 17.192030  [ 1600/12672]\n",
      "loss: 16.907953  [ 3200/12672]\n",
      "loss: 15.249927  [ 4800/12672]\n",
      "loss: 10.306474  [ 6400/12672]\n",
      "loss: 19.630167  [ 8000/12672]\n",
      "loss: 7.691747  [ 9600/12672]\n",
      "loss: 27.766479  [11200/12672]\n",
      "0.5506761423979151\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 28.373968  [    0/12672]\n",
      "loss: 9.979584  [ 1600/12672]\n",
      "loss: 10.585919  [ 3200/12672]\n",
      "loss: 5.692106  [ 4800/12672]\n",
      "loss: 9.847588  [ 6400/12672]\n",
      "loss: 19.075371  [ 8000/12672]\n",
      "loss: 25.857302  [ 9600/12672]\n",
      "loss: 8.866459  [11200/12672]\n",
      "0.5437489043793481\n",
      "Done!\n",
      "##############################\n",
      "SESSION 12\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.727428  [    0/12672]\n",
      "loss: 19.263752  [ 1600/12672]\n",
      "loss: 23.215904  [ 3200/12672]\n",
      "loss: 10.996237  [ 4800/12672]\n",
      "loss: 8.802182  [ 6400/12672]\n",
      "loss: 19.265198  [ 8000/12672]\n",
      "loss: 22.428001  [ 9600/12672]\n",
      "loss: 12.741953  [11200/12672]\n",
      "0.528757197275327\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 19.104647  [    0/12672]\n",
      "loss: 29.885801  [ 1600/12672]\n",
      "loss: 5.426745  [ 3200/12672]\n",
      "loss: 30.141529  [ 4800/12672]\n",
      "loss: 12.489265  [ 6400/12672]\n",
      "loss: 22.974808  [ 8000/12672]\n",
      "loss: 24.886106  [ 9600/12672]\n",
      "loss: 6.433866  [11200/12672]\n",
      "0.534026891114835\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 12.725032  [    0/12672]\n",
      "loss: 19.791502  [ 1600/12672]\n",
      "loss: 10.899297  [ 3200/12672]\n",
      "loss: 25.117945  [ 4800/12672]\n",
      "loss: 16.523046  [ 6400/12672]\n",
      "loss: 18.496574  [ 8000/12672]\n",
      "loss: 13.308790  [ 9600/12672]\n",
      "loss: 16.325171  [11200/12672]\n",
      "0.5809975306437046\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 7.397907  [    0/12672]\n",
      "loss: 21.506018  [ 1600/12672]\n",
      "loss: 17.758656  [ 3200/12672]\n",
      "loss: 21.481762  [ 4800/12672]\n",
      "loss: 5.203710  [ 6400/12672]\n",
      "loss: 18.420200  [ 8000/12672]\n",
      "loss: 18.640467  [ 9600/12672]\n",
      "loss: 8.806350  [11200/12672]\n",
      "0.6287767751097668\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 8.034558  [    0/12672]\n",
      "loss: 16.264244  [ 1600/12672]\n",
      "loss: 15.765620  [ 3200/12672]\n",
      "loss: 8.156822  [ 4800/12672]\n",
      "loss: 11.240257  [ 6400/12672]\n",
      "loss: 14.830077  [ 8000/12672]\n",
      "loss: 18.401789  [ 9600/12672]\n",
      "loss: 16.763081  [11200/12672]\n",
      "0.6412582449098158\n",
      "Done!\n",
      "##############################\n",
      "SESSION 13\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.701871  [    0/12672]\n",
      "loss: 8.854977  [ 1600/12672]\n",
      "loss: 6.318783  [ 3200/12672]\n",
      "loss: 9.447016  [ 4800/12672]\n",
      "loss: 13.177249  [ 6400/12672]\n",
      "loss: 23.894342  [ 8000/12672]\n",
      "loss: 4.853671  [ 9600/12672]\n",
      "loss: 19.581436  [11200/12672]\n",
      "0.5369343258851119\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 18.909468  [    0/12672]\n",
      "loss: 5.663638  [ 1600/12672]\n",
      "loss: 23.975231  [ 3200/12672]\n",
      "loss: 7.801884  [ 4800/12672]\n",
      "loss: 19.532967  [ 6400/12672]\n",
      "loss: 12.757672  [ 8000/12672]\n",
      "loss: 12.589581  [ 9600/12672]\n",
      "loss: 17.913776  [11200/12672]\n",
      "0.5257897129586317\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 20.685596  [    0/12672]\n",
      "loss: 9.417695  [ 1600/12672]\n",
      "loss: 10.621064  [ 3200/12672]\n",
      "loss: 21.332241  [ 4800/12672]\n",
      "loss: 4.528465  [ 6400/12672]\n",
      "loss: 13.645063  [ 8000/12672]\n",
      "loss: 24.459658  [ 9600/12672]\n",
      "loss: 18.871550  [11200/12672]\n",
      "0.5910224192685345\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 14.536521  [    0/12672]\n",
      "loss: 25.711571  [ 1600/12672]\n",
      "loss: 23.467358  [ 3200/12672]\n",
      "loss: 15.207658  [ 4800/12672]\n",
      "loss: 13.621499  [ 6400/12672]\n",
      "loss: 22.013453  [ 8000/12672]\n",
      "loss: 24.546612  [ 9600/12672]\n",
      "loss: 10.750130  [11200/12672]\n",
      "0.5533018974642232\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 13.583744  [    0/12672]\n",
      "loss: 9.256139  [ 1600/12672]\n",
      "loss: 22.098972  [ 3200/12672]\n",
      "loss: 18.100431  [ 4800/12672]\n",
      "loss: 17.591013  [ 6400/12672]\n",
      "loss: 14.677464  [ 8000/12672]\n",
      "loss: 13.659981  [ 9600/12672]\n",
      "loss: 11.656924  [11200/12672]\n",
      "0.5795469578024698\n",
      "Done!\n",
      "##############################\n",
      "SESSION 14\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.763769  [    0/12672]\n",
      "loss: 28.039692  [ 1600/12672]\n",
      "loss: 9.121747  [ 3200/12672]\n",
      "loss: 21.038618  [ 4800/12672]\n",
      "loss: 21.749084  [ 6400/12672]\n",
      "loss: 3.019026  [ 8000/12672]\n",
      "loss: 3.813885  [ 9600/12672]\n",
      "loss: 16.788465  [11200/12672]\n",
      "0.5677017956396134\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 13.247034  [    0/12672]\n",
      "loss: 14.924617  [ 1600/12672]\n",
      "loss: 11.809961  [ 3200/12672]\n",
      "loss: 7.545813  [ 4800/12672]\n",
      "loss: 4.497390  [ 6400/12672]\n",
      "loss: 11.759605  [ 8000/12672]\n",
      "loss: 15.383797  [ 9600/12672]\n",
      "loss: 26.267534  [11200/12672]\n",
      "0.5989606254857076\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 6.748317  [    0/12672]\n",
      "loss: 16.574694  [ 1600/12672]\n",
      "loss: 17.384666  [ 3200/12672]\n",
      "loss: 20.193645  [ 4800/12672]\n",
      "loss: 17.665033  [ 6400/12672]\n",
      "loss: 14.649830  [ 8000/12672]\n",
      "loss: 21.603699  [ 9600/12672]\n",
      "loss: 5.251456  [11200/12672]\n",
      "0.5620327176814435\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 22.396606  [    0/12672]\n",
      "loss: 10.964687  [ 1600/12672]\n",
      "loss: 18.311392  [ 3200/12672]\n",
      "loss: 9.255136  [ 4800/12672]\n",
      "loss: 17.701897  [ 6400/12672]\n",
      "loss: 14.008071  [ 8000/12672]\n",
      "loss: 15.783722  [ 9600/12672]\n",
      "loss: 9.761492  [11200/12672]\n",
      "0.5801546503328152\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 17.146446  [    0/12672]\n",
      "loss: 6.931120  [ 1600/12672]\n",
      "loss: 7.918205  [ 3200/12672]\n",
      "loss: 11.122717  [ 4800/12672]\n",
      "loss: 21.873590  [ 6400/12672]\n",
      "loss: 13.659857  [ 8000/12672]\n",
      "loss: 13.109371  [ 9600/12672]\n",
      "loss: 13.940041  [11200/12672]\n",
      "0.5610250461422152\n",
      "Done!\n",
      "##############################\n",
      "SESSION 15\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.756134  [    0/12672]\n",
      "loss: 20.457605  [ 1600/12672]\n",
      "loss: 21.200336  [ 3200/12672]\n",
      "loss: 24.077341  [ 4800/12672]\n",
      "loss: 16.549292  [ 6400/12672]\n",
      "loss: 20.510571  [ 8000/12672]\n",
      "loss: 7.237989  [ 9600/12672]\n",
      "loss: 19.431816  [11200/12672]\n",
      "0.5378923714489932\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 10.780047  [    0/12672]\n",
      "loss: 9.164552  [ 1600/12672]\n",
      "loss: 6.425621  [ 3200/12672]\n",
      "loss: 14.721494  [ 4800/12672]\n",
      "loss: 19.234159  [ 6400/12672]\n",
      "loss: 21.617952  [ 8000/12672]\n",
      "loss: 10.392173  [ 9600/12672]\n",
      "loss: 12.488229  [11200/12672]\n",
      "0.5065692135252493\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 25.358719  [    0/12672]\n",
      "loss: 22.170864  [ 1600/12672]\n",
      "loss: 23.788860  [ 3200/12672]\n",
      "loss: 15.765247  [ 4800/12672]\n",
      "loss: 17.863813  [ 6400/12672]\n",
      "loss: 14.250141  [ 8000/12672]\n",
      "loss: 27.288166  [ 9600/12672]\n",
      "loss: 14.151366  [11200/12672]\n",
      "0.5777710666011108\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 13.493119  [    0/12672]\n",
      "loss: 14.240608  [ 1600/12672]\n",
      "loss: 17.172867  [ 3200/12672]\n",
      "loss: 13.115134  [ 4800/12672]\n",
      "loss: 16.034077  [ 6400/12672]\n",
      "loss: 26.772520  [ 8000/12672]\n",
      "loss: 14.643780  [ 9600/12672]\n",
      "loss: 7.961404  [11200/12672]\n",
      "0.5544968773250963\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 14.884203  [    0/12672]\n",
      "loss: 9.577732  [ 1600/12672]\n",
      "loss: 16.281546  [ 3200/12672]\n",
      "loss: 16.367693  [ 4800/12672]\n",
      "loss: 10.840758  [ 6400/12672]\n",
      "loss: 20.023178  [ 8000/12672]\n",
      "loss: 20.625856  [ 9600/12672]\n",
      "loss: 10.089197  [11200/12672]\n",
      "0.5556564473509509\n",
      "Done!\n",
      "##############################\n",
      "SESSION 16\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.763591  [    0/12672]\n",
      "loss: 29.910873  [ 1600/12672]\n",
      "loss: 24.332457  [ 3200/12672]\n",
      "loss: 7.807477  [ 4800/12672]\n",
      "loss: 17.715981  [ 6400/12672]\n",
      "loss: 10.782772  [ 8000/12672]\n",
      "loss: 21.316401  [ 9600/12672]\n",
      "loss: 4.299181  [11200/12672]\n",
      "0.5256219562392968\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 24.430073  [    0/12672]\n",
      "loss: 17.628046  [ 1600/12672]\n",
      "loss: 9.566704  [ 3200/12672]\n",
      "loss: 17.602394  [ 4800/12672]\n",
      "loss: 11.836028  [ 6400/12672]\n",
      "loss: 5.474169  [ 8000/12672]\n",
      "loss: 9.038499  [ 9600/12672]\n",
      "loss: 12.486952  [11200/12672]\n",
      "0.5723888141018035\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 18.925154  [    0/12672]\n",
      "loss: 15.284008  [ 1600/12672]\n",
      "loss: 11.237732  [ 3200/12672]\n",
      "loss: 17.667198  [ 4800/12672]\n",
      "loss: 29.608614  [ 6400/12672]\n",
      "loss: 20.192814  [ 8000/12672]\n",
      "loss: 18.351919  [ 9600/12672]\n",
      "loss: 13.869170  [11200/12672]\n",
      "0.6042225172034512\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 13.785480  [    0/12672]\n",
      "loss: 5.676741  [ 1600/12672]\n",
      "loss: 22.681990  [ 3200/12672]\n",
      "loss: 12.781325  [ 4800/12672]\n",
      "loss: 26.298975  [ 6400/12672]\n",
      "loss: 6.442808  [ 8000/12672]\n",
      "loss: 5.064954  [ 9600/12672]\n",
      "loss: 24.520123  [11200/12672]\n",
      "0.6005128726332407\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 8.307605  [    0/12672]\n",
      "loss: 13.981294  [ 1600/12672]\n",
      "loss: 8.907139  [ 3200/12672]\n",
      "loss: 19.333738  [ 4800/12672]\n",
      "loss: 15.001623  [ 6400/12672]\n",
      "loss: 17.594137  [ 8000/12672]\n",
      "loss: 25.193483  [ 9600/12672]\n",
      "loss: 14.891169  [11200/12672]\n",
      "0.5506721474847384\n",
      "Done!\n",
      "##############################\n",
      "SESSION 17\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.751090  [    0/12672]\n",
      "loss: 15.847932  [ 1600/12672]\n",
      "loss: 17.545259  [ 3200/12672]\n",
      "loss: 13.784625  [ 4800/12672]\n",
      "loss: 6.666979  [ 6400/12672]\n",
      "loss: 21.108992  [ 8000/12672]\n",
      "loss: 13.500223  [ 9600/12672]\n",
      "loss: 20.476974  [11200/12672]\n",
      "0.5649067613628477\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 10.262718  [    0/12672]\n",
      "loss: 17.045792  [ 1600/12672]\n",
      "loss: 22.085938  [ 3200/12672]\n",
      "loss: 16.250244  [ 4800/12672]\n",
      "loss: 27.318687  [ 6400/12672]\n",
      "loss: 11.530122  [ 8000/12672]\n",
      "loss: 12.017854  [ 9600/12672]\n",
      "loss: 18.620298  [11200/12672]\n",
      "0.5758606081217792\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 17.543058  [    0/12672]\n",
      "loss: 12.095687  [ 1600/12672]\n",
      "loss: 24.023777  [ 3200/12672]\n",
      "loss: 17.782578  [ 4800/12672]\n",
      "loss: 15.576221  [ 6400/12672]\n",
      "loss: 22.466473  [ 8000/12672]\n",
      "loss: 25.816362  [ 9600/12672]\n",
      "loss: 20.412148  [11200/12672]\n",
      "0.59634297678404\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 10.153034  [    0/12672]\n",
      "loss: 9.620951  [ 1600/12672]\n",
      "loss: 20.651489  [ 3200/12672]\n",
      "loss: 10.617221  [ 4800/12672]\n",
      "loss: 18.668354  [ 6400/12672]\n",
      "loss: 7.576048  [ 8000/12672]\n",
      "loss: 19.666702  [ 9600/12672]\n",
      "loss: 4.463674  [11200/12672]\n",
      "0.5868246030033395\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 12.458539  [    0/12672]\n",
      "loss: 9.298082  [ 1600/12672]\n",
      "loss: 21.407656  [ 3200/12672]\n",
      "loss: 18.323671  [ 4800/12672]\n",
      "loss: 11.300006  [ 6400/12672]\n",
      "loss: 18.342045  [ 8000/12672]\n",
      "loss: 20.255016  [ 9600/12672]\n",
      "loss: 13.498471  [11200/12672]\n",
      "0.5174213951660023\n",
      "Done!\n",
      "##############################\n",
      "SESSION 18\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.725391  [    0/12672]\n",
      "loss: 27.523439  [ 1600/12672]\n",
      "loss: 25.365860  [ 3200/12672]\n",
      "loss: 20.152111  [ 4800/12672]\n",
      "loss: 14.833099  [ 6400/12672]\n",
      "loss: 22.938972  [ 8000/12672]\n",
      "loss: 30.688145  [ 9600/12672]\n",
      "loss: 8.874393  [11200/12672]\n",
      "0.5572940175191304\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 23.090559  [    0/12672]\n",
      "loss: 24.508066  [ 1600/12672]\n",
      "loss: 10.827437  [ 3200/12672]\n",
      "loss: 10.603734  [ 4800/12672]\n",
      "loss: 12.528838  [ 6400/12672]\n",
      "loss: 11.673834  [ 8000/12672]\n",
      "loss: 14.309003  [ 9600/12672]\n",
      "loss: 20.864294  [11200/12672]\n",
      "0.5481718825647544\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 21.447063  [    0/12672]\n",
      "loss: 17.469263  [ 1600/12672]\n",
      "loss: 15.224264  [ 3200/12672]\n",
      "loss: 16.263025  [ 4800/12672]\n",
      "loss: 13.857861  [ 6400/12672]\n",
      "loss: 15.380840  [ 8000/12672]\n",
      "loss: 10.326393  [ 9600/12672]\n",
      "loss: 17.844110  [11200/12672]\n",
      "0.539540235247048\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 19.484106  [    0/12672]\n",
      "loss: 18.871838  [ 1600/12672]\n",
      "loss: 10.769226  [ 3200/12672]\n",
      "loss: 5.623437  [ 4800/12672]\n",
      "loss: 12.465913  [ 6400/12672]\n",
      "loss: 15.836410  [ 8000/12672]\n",
      "loss: 8.535559  [ 9600/12672]\n",
      "loss: 15.871321  [11200/12672]\n",
      "0.5304613349647987\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 22.095015  [    0/12672]\n",
      "loss: 13.692898  [ 1600/12672]\n",
      "loss: 27.309219  [ 3200/12672]\n",
      "loss: 15.333143  [ 4800/12672]\n",
      "loss: 14.154431  [ 6400/12672]\n",
      "loss: 16.872797  [ 8000/12672]\n",
      "loss: 7.899141  [ 9600/12672]\n",
      "loss: 11.209040  [11200/12672]\n",
      "0.5790330331806934\n",
      "Done!\n",
      "##############################\n",
      "SESSION 19\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.684443  [    0/12672]\n",
      "loss: 21.434221  [ 1600/12672]\n",
      "loss: 12.348323  [ 3200/12672]\n",
      "loss: 18.838049  [ 4800/12672]\n",
      "loss: 6.281024  [ 6400/12672]\n",
      "loss: 20.293688  [ 8000/12672]\n",
      "loss: 5.773409  [ 9600/12672]\n",
      "loss: 10.698848  [11200/12672]\n",
      "0.5600220755744797\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 17.916889  [    0/12672]\n",
      "loss: 17.218641  [ 1600/12672]\n",
      "loss: 21.065952  [ 3200/12672]\n",
      "loss: 13.413302  [ 4800/12672]\n",
      "loss: 15.132978  [ 6400/12672]\n",
      "loss: 7.384303  [ 8000/12672]\n",
      "loss: 18.692535  [ 9600/12672]\n",
      "loss: 18.469765  [11200/12672]\n",
      "0.5416734223248226\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 10.391348  [    0/12672]\n",
      "loss: 10.813724  [ 1600/12672]\n",
      "loss: 5.278337  [ 3200/12672]\n",
      "loss: 8.415112  [ 4800/12672]\n",
      "loss: 23.600258  [ 6400/12672]\n",
      "loss: 7.219780  [ 8000/12672]\n",
      "loss: 7.831767  [ 9600/12672]\n",
      "loss: 7.409264  [11200/12672]\n",
      "0.5170651485256719\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 19.529234  [    0/12672]\n",
      "loss: 21.614408  [ 1600/12672]\n",
      "loss: 14.987229  [ 3200/12672]\n",
      "loss: 19.364866  [ 4800/12672]\n",
      "loss: 15.294590  [ 6400/12672]\n",
      "loss: 21.993963  [ 8000/12672]\n",
      "loss: 21.548491  [ 9600/12672]\n",
      "loss: 13.255752  [11200/12672]\n",
      "0.5497290238264723\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 25.244848  [    0/12672]\n",
      "loss: 20.305618  [ 1600/12672]\n",
      "loss: 10.249121  [ 3200/12672]\n",
      "loss: 14.497790  [ 4800/12672]\n",
      "loss: 16.720865  [ 6400/12672]\n",
      "loss: 23.792431  [ 8000/12672]\n",
      "loss: 15.933721  [ 9600/12672]\n",
      "loss: 20.829187  [11200/12672]\n",
      "0.5806662241073248\n",
      "Done!\n",
      "##############################\n",
      "SESSION 20\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.746212  [    0/12672]\n",
      "loss: 15.977869  [ 1600/12672]\n",
      "loss: 8.747810  [ 3200/12672]\n",
      "loss: 28.591259  [ 4800/12672]\n",
      "loss: 30.184893  [ 6400/12672]\n",
      "loss: 21.031252  [ 8000/12672]\n",
      "loss: 9.624247  [ 9600/12672]\n",
      "loss: 15.218056  [11200/12672]\n",
      "0.5684906298380648\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 17.409901  [    0/12672]\n",
      "loss: 10.434740  [ 1600/12672]\n",
      "loss: 21.914095  [ 3200/12672]\n",
      "loss: 21.386633  [ 4800/12672]\n",
      "loss: 7.270551  [ 6400/12672]\n",
      "loss: 22.227598  [ 8000/12672]\n",
      "loss: 17.377497  [ 9600/12672]\n",
      "loss: 12.426970  [11200/12672]\n",
      "0.5575147622515788\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 18.670887  [    0/12672]\n",
      "loss: 10.950458  [ 1600/12672]\n",
      "loss: 7.204300  [ 3200/12672]\n",
      "loss: 8.625788  [ 4800/12672]\n",
      "loss: 17.520929  [ 6400/12672]\n",
      "loss: 13.331217  [ 8000/12672]\n",
      "loss: 13.120353  [ 9600/12672]\n",
      "loss: 7.898067  [11200/12672]\n",
      "0.5483679293040062\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 20.817253  [    0/12672]\n",
      "loss: 9.449024  [ 1600/12672]\n",
      "loss: 14.564731  [ 3200/12672]\n",
      "loss: 14.015445  [ 4800/12672]\n",
      "loss: 7.195291  [ 6400/12672]\n",
      "loss: 11.703623  [ 8000/12672]\n",
      "loss: 15.605017  [ 9600/12672]\n",
      "loss: 7.673749  [11200/12672]\n",
      "0.619804212374091\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 8.022472  [    0/12672]\n",
      "loss: 14.994068  [ 1600/12672]\n",
      "loss: 5.523987  [ 3200/12672]\n",
      "loss: 11.098734  [ 4800/12672]\n",
      "loss: 13.053980  [ 6400/12672]\n",
      "loss: 15.947422  [ 8000/12672]\n",
      "loss: 11.946120  [ 9600/12672]\n",
      "loss: 17.359495  [11200/12672]\n",
      "0.6262668558785093\n",
      "Done!\n",
      "##############################\n",
      "SESSION 21\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.722362  [    0/12672]\n",
      "loss: 20.394714  [ 1600/12672]\n",
      "loss: 14.620893  [ 3200/12672]\n",
      "loss: 8.950678  [ 4800/12672]\n",
      "loss: 26.035805  [ 6400/12672]\n",
      "loss: 8.646745  [ 8000/12672]\n",
      "loss: 7.578521  [ 9600/12672]\n",
      "loss: 23.340124  [11200/12672]\n",
      "0.5225151504593845\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 29.233807  [    0/12672]\n",
      "loss: 22.346802  [ 1600/12672]\n",
      "loss: 6.373038  [ 3200/12672]\n",
      "loss: 23.770874  [ 4800/12672]\n",
      "loss: 18.423613  [ 6400/12672]\n",
      "loss: 13.098525  [ 8000/12672]\n",
      "loss: 11.480954  [ 9600/12672]\n",
      "loss: 15.471126  [11200/12672]\n",
      "0.6114046588488449\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 9.671913  [    0/12672]\n",
      "loss: 17.579531  [ 1600/12672]\n",
      "loss: 6.913132  [ 3200/12672]\n",
      "loss: 16.274866  [ 4800/12672]\n",
      "loss: 22.097084  [ 6400/12672]\n",
      "loss: 21.903452  [ 8000/12672]\n",
      "loss: 19.709255  [ 9600/12672]\n",
      "loss: 15.991201  [11200/12672]\n",
      "0.5614129363904228\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 18.188854  [    0/12672]\n",
      "loss: 6.764827  [ 1600/12672]\n",
      "loss: 13.970637  [ 3200/12672]\n",
      "loss: 24.728300  [ 4800/12672]\n",
      "loss: 19.214315  [ 6400/12672]\n",
      "loss: 11.898140  [ 8000/12672]\n",
      "loss: 7.922932  [ 9600/12672]\n",
      "loss: 18.046669  [11200/12672]\n",
      "0.5377890806448291\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 13.426827  [    0/12672]\n",
      "loss: 16.458429  [ 1600/12672]\n",
      "loss: 7.491028  [ 3200/12672]\n",
      "loss: 23.598133  [ 4800/12672]\n",
      "loss: 18.452175  [ 6400/12672]\n",
      "loss: 23.520601  [ 8000/12672]\n",
      "loss: 13.383593  [ 9600/12672]\n",
      "loss: 21.481407  [11200/12672]\n",
      "0.5212631673505804\n",
      "Done!\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 0.778039  [    0/12672]\n",
      "loss: 13.803291  [ 1600/12672]\n",
      "loss: 10.510843  [ 3200/12672]\n",
      "loss: 13.486895  [ 4800/12672]\n",
      "loss: 7.484552  [ 6400/12672]\n",
      "loss: 8.297561  [ 8000/12672]\n",
      "loss: 18.911419  [ 9600/12672]\n",
      "loss: 10.806732  [11200/12672]\n",
      "0.498017968045671\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 28.343931  [    0/12672]\n",
      "loss: 9.595251  [ 1600/12672]\n",
      "loss: 18.962517  [ 3200/12672]\n",
      "loss: 11.239657  [ 4800/12672]\n",
      "loss: 16.044868  [ 6400/12672]\n",
      "loss: 27.725103  [ 8000/12672]\n",
      "loss: 24.827156  [ 9600/12672]\n",
      "loss: 8.096144  [11200/12672]\n",
      "0.5600977873998427\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 17.591551  [    0/12672]\n",
      "loss: 19.017786  [ 1600/12672]\n",
      "loss: 23.272018  [ 3200/12672]\n",
      "loss: 13.850866  [ 4800/12672]\n",
      "loss: 11.977016  [ 6400/12672]\n",
      "loss: 4.047642  [ 8000/12672]\n",
      "loss: 12.271650  [ 9600/12672]\n",
      "loss: 12.416839  [11200/12672]\n",
      "0.5549377572675869\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 21.426956  [    0/12672]\n",
      "loss: 25.145842  [ 1600/12672]\n",
      "loss: 22.213682  [ 3200/12672]\n",
      "loss: 12.173513  [ 4800/12672]\n",
      "loss: 8.561211  [ 6400/12672]\n",
      "loss: 11.818450  [ 8000/12672]\n",
      "loss: 21.469450  [ 9600/12672]\n",
      "loss: 20.750393  [11200/12672]\n",
      "0.5407098145589649\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "None\n",
      "yay training\n",
      "loss: 16.245665  [    0/12672]\n",
      "loss: 12.236158  [ 1600/12672]\n",
      "loss: 7.822185  [ 3200/12672]\n",
      "loss: 10.494864  [ 4800/12672]\n",
      "loss: 18.413298  [ 6400/12672]\n",
      "loss: 13.121839  [ 8000/12672]\n",
      "loss: 15.589358  [ 9600/12672]\n",
      "loss: 10.682901  [11200/12672]\n",
      "0.5392256743490204\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "for i, (train_idx, test_idx) in enumerate(logo.split(X, y, groups=sessions)):\n",
    "    print(f\"{'#'*30}\\nSESSION {i}\\n{'#'*30}\")\n",
    "    ## create model ##\n",
    "    model = LogisticRegressionTorch(X.shape[-1])\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    weight_decay = 1e-4\n",
    "    lr = 5e-1\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "    ## data ##\n",
    "    train_set = SimpleDataset(X[train_idx], y[train_idx])\n",
    "    test_set = SimpleDataset(X[test_idx], y[test_idx])\n",
    "    ## class balancing ##\n",
    "    cls_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(train_set.y.detach().numpy()),\n",
    "        y=train_set.y.detach().numpy()\n",
    "    )\n",
    "    weights = cls_weights[train_set.y.detach().numpy().astype(int)]\n",
    "    sampler = WeightedRandomSampler(weights, len(train_set.y.detach().numpy()),\n",
    "                                    replacement=True)\n",
    "    \n",
    "    train_dataloader = DataLoader(train_set, batch_size=200, sampler=sampler)\n",
    "    \n",
    "    ## training epochs ##\n",
    "    epochs = 5\n",
    "    for t in range(epochs):\n",
    "        print(print(f\"{'-'*30}\\nEpoch {t+1}\\n{'-'*30}\"))\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, print_nth_batch=8)\n",
    "        out = test_auc_score(train_set, model)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9d6e5d-6764-4dda-9216-e6fc426e9dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
