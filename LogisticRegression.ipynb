{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55035abd-0f92-4b00-90a5-90048fe5149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x116add0b0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import skorch\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "#### PYTORCH CONFIGURATION SETTINGS ######\n",
    "device = \"cpu\"#\"mps\" if torch.has_mps else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "torch.manual_seed(56)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b18eb46-9cb5-4f79-bd67-59faf8336e78",
   "metadata": {},
   "source": [
    "### Logistic Regression PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d5cd46b-8998-4508-91e7-d08c09b4d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "def train_loop(dataloader, model, loss_fn, optimizer, print_nth_batch=4):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(\"yay training\")\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # print(y)\n",
    "        # Compute prediction and loss\n",
    "        pred = torch.squeeze(model(X))\n",
    "        # print(pred)\n",
    "        # regularization, computing largest singular value\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % print_nth_batch == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    # we don't want to track gradients here because we're just doing\n",
    "    # a forward pass to evaluate predictions\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = torch.squeeze(model(X))\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # round predicted probs to get label prediction, compute n correct\n",
    "            correct += (pred.round() == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "def test_auc_score(dataset, model):\n",
    "    X, y = dataset[:]\n",
    "    pred = model(X)\n",
    "    pred, y = pred.detach().numpy(), y.detach().numpy()\n",
    "    score = roc_auc_score(y_true=y, y_score=pred)\n",
    "    print(\"AUC:\", score)\n",
    "    return score\n",
    "    \n",
    "    \n",
    "    \n",
    "class LogisticRegressionTorch(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, output_dim, bias=True),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # logits = torch.sigmoid(self.linear(x))\n",
    "        probs = self.linear(x)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88a7ed46-6f7e-494b-9f74-1571075c8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = torch.rand(1000, 128) + .2\n",
    "y_1 = torch.zeros(1000,)\n",
    "X_2 = torch.rand(1000, 128) - .1\n",
    "y_2 = torch.ones(1000,)\n",
    "\n",
    "X = torch.cat((X_1, X_2))\n",
    "y = torch.cat((y_1, y_2))\n",
    "\n",
    "dataset = SimpleDataset(X, y)\n",
    "\n",
    "train_prop = .8\n",
    "train_num = int(train_prop * len(dataset))\n",
    "test_num = len(X) - train_num\n",
    "\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset,\n",
    "                                                    [train_num, test_num])\n",
    "\n",
    "\n",
    "target = train_set.dataset.y[train_set.indices]\n",
    "cls_weights = torch.from_numpy(\n",
    "    compute_class_weight(class_weight='balanced',\n",
    "                         classes=np.unique(target.numpy()),\n",
    "                         y=target.numpy())\n",
    ")\n",
    "weights = cls_weights[target.numpy()]\n",
    "sampler = WeightedRandomSampler(weights, len(target.numpy()), replacement=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=100, sampler=sampler)\n",
    "test_dataloader = DataLoader(test_set, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "29b0b7b5-d1b1-44fd-b644-842d55fdd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionTorch(128)\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "weight_decay = 0\n",
    "lr = 5e-1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr,weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d62a1cb0-0e75-4aed-8389-5f90c13bb18c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.448199  [    0/12672]\n",
      "loss: 19.501102  [  400/12672]\n",
      "loss: 11.889750  [  800/12672]\n",
      "loss: 10.050867  [ 1200/12672]\n",
      "loss: 4.415318  [ 1600/12672]\n",
      "loss: 20.606892  [ 2000/12672]\n",
      "loss: 23.693605  [ 2400/12672]\n",
      "loss: 6.622545  [ 2800/12672]\n",
      "loss: 22.063299  [ 3200/12672]\n",
      "loss: 9.026217  [ 3600/12672]\n",
      "loss: 12.450096  [ 4000/12672]\n",
      "loss: 13.314853  [ 4400/12672]\n",
      "loss: 10.686407  [ 4800/12672]\n",
      "loss: 17.673269  [ 5200/12672]\n",
      "loss: 13.940762  [ 5600/12672]\n",
      "loss: 16.124735  [ 6000/12672]\n",
      "loss: 14.201858  [ 6400/12672]\n",
      "loss: 16.571611  [ 6800/12672]\n",
      "loss: 10.381608  [ 7200/12672]\n",
      "loss: 7.627161  [ 7600/12672]\n",
      "loss: 9.125181  [ 8000/12672]\n",
      "loss: 8.847769  [ 8400/12672]\n",
      "loss: 13.405208  [ 8800/12672]\n",
      "loss: 19.638165  [ 9200/12672]\n",
      "loss: 14.617030  [ 9600/12672]\n",
      "loss: 7.883918  [10000/12672]\n",
      "loss: 15.535515  [10400/12672]\n",
      "loss: 13.479076  [10800/12672]\n",
      "loss: 19.351795  [11200/12672]\n",
      "loss: 16.983923  [11600/12672]\n",
      "loss: 5.988626  [12000/12672]\n",
      "loss: 7.569260  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Avg loss: 18.016468 \n",
      "\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.649567  [    0/12672]\n",
      "loss: 13.509387  [  400/12672]\n",
      "loss: 9.397038  [  800/12672]\n",
      "loss: 16.511402  [ 1200/12672]\n",
      "loss: 13.522353  [ 1600/12672]\n",
      "loss: 10.554843  [ 2000/12672]\n",
      "loss: 20.753071  [ 2400/12672]\n",
      "loss: 8.883151  [ 2800/12672]\n",
      "loss: 20.038437  [ 3200/12672]\n",
      "loss: 11.116176  [ 3600/12672]\n",
      "loss: 13.765134  [ 4000/12672]\n",
      "loss: 25.880190  [ 4400/12672]\n",
      "loss: 11.455889  [ 4800/12672]\n",
      "loss: 31.708776  [ 5200/12672]\n",
      "loss: 13.507155  [ 5600/12672]\n",
      "loss: 12.163624  [ 6000/12672]\n",
      "loss: 11.731881  [ 6400/12672]\n",
      "loss: 17.309862  [ 6800/12672]\n",
      "loss: 17.024408  [ 7200/12672]\n",
      "loss: 12.313538  [ 7600/12672]\n",
      "loss: 22.971514  [ 8000/12672]\n",
      "loss: 24.267168  [ 8400/12672]\n",
      "loss: 17.090376  [ 8800/12672]\n",
      "loss: 11.761583  [ 9200/12672]\n",
      "loss: 12.240818  [ 9600/12672]\n",
      "loss: 14.493343  [10000/12672]\n",
      "loss: 7.649945  [10400/12672]\n",
      "loss: 12.533072  [10800/12672]\n",
      "loss: 14.956245  [11200/12672]\n",
      "loss: 10.299414  [11600/12672]\n",
      "loss: 19.823963  [12000/12672]\n",
      "loss: 7.478351  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Avg loss: 16.509207 \n",
      "\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.266824  [    0/12672]\n",
      "loss: 10.595530  [  400/12672]\n",
      "loss: 12.792263  [  800/12672]\n",
      "loss: 12.264021  [ 1200/12672]\n",
      "loss: 16.692595  [ 1600/12672]\n",
      "loss: 14.937693  [ 2000/12672]\n",
      "loss: 14.701788  [ 2400/12672]\n",
      "loss: 9.137777  [ 2800/12672]\n",
      "loss: 11.993700  [ 3200/12672]\n",
      "loss: 11.910131  [ 3600/12672]\n",
      "loss: 18.288343  [ 4000/12672]\n",
      "loss: 12.494102  [ 4400/12672]\n",
      "loss: 20.259302  [ 4800/12672]\n",
      "loss: 14.861511  [ 5200/12672]\n",
      "loss: 17.148876  [ 5600/12672]\n",
      "loss: 11.321703  [ 6000/12672]\n",
      "loss: 15.139669  [ 6400/12672]\n",
      "loss: 6.948258  [ 6800/12672]\n",
      "loss: 10.369998  [ 7200/12672]\n",
      "loss: 14.284306  [ 7600/12672]\n",
      "loss: 18.567276  [ 8000/12672]\n",
      "loss: 12.247922  [ 8400/12672]\n",
      "loss: 12.163288  [ 8800/12672]\n",
      "loss: 8.944297  [ 9200/12672]\n",
      "loss: 17.060509  [ 9600/12672]\n",
      "loss: 11.167281  [10000/12672]\n",
      "loss: 8.378854  [10400/12672]\n",
      "loss: 15.077158  [10800/12672]\n",
      "loss: 12.965886  [11200/12672]\n",
      "loss: 10.021579  [11600/12672]\n",
      "loss: 18.610182  [12000/12672]\n",
      "loss: 16.985973  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Avg loss: 12.171034 \n",
      "\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.183802  [    0/12672]\n",
      "loss: 15.160548  [  400/12672]\n",
      "loss: 10.933621  [  800/12672]\n",
      "loss: 12.116112  [ 1200/12672]\n",
      "loss: 14.420932  [ 1600/12672]\n",
      "loss: 14.012866  [ 2000/12672]\n",
      "loss: 18.241829  [ 2400/12672]\n",
      "loss: 10.419930  [ 2800/12672]\n",
      "loss: 12.127881  [ 3200/12672]\n",
      "loss: 14.237103  [ 3600/12672]\n",
      "loss: 12.597299  [ 4000/12672]\n",
      "loss: 15.388819  [ 4400/12672]\n",
      "loss: 13.214099  [ 4800/12672]\n",
      "loss: 11.429836  [ 5200/12672]\n",
      "loss: 11.230478  [ 5600/12672]\n",
      "loss: 12.669957  [ 6000/12672]\n",
      "loss: 13.378399  [ 6400/12672]\n",
      "loss: 6.739186  [ 6800/12672]\n",
      "loss: 9.320815  [ 7200/12672]\n",
      "loss: 12.317379  [ 7600/12672]\n",
      "loss: 11.085850  [ 8000/12672]\n",
      "loss: 8.723577  [ 8400/12672]\n",
      "loss: 15.511212  [ 8800/12672]\n",
      "loss: 7.381125  [ 9200/12672]\n",
      "loss: 16.983154  [ 9600/12672]\n",
      "loss: 16.036646  [10000/12672]\n",
      "loss: 14.431424  [10400/12672]\n",
      "loss: 15.025547  [10800/12672]\n",
      "loss: 24.832644  [11200/12672]\n",
      "loss: 18.524570  [11600/12672]\n",
      "loss: 15.385938  [12000/12672]\n",
      "loss: 9.860323  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Avg loss: 12.374711 \n",
      "\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.690251  [    0/12672]\n",
      "loss: 11.427903  [  400/12672]\n",
      "loss: 15.970511  [  800/12672]\n",
      "loss: 17.453661  [ 1200/12672]\n",
      "loss: 10.413849  [ 1600/12672]\n",
      "loss: 20.263115  [ 2000/12672]\n",
      "loss: 17.261705  [ 2400/12672]\n",
      "loss: 19.123299  [ 2800/12672]\n",
      "loss: 8.567987  [ 3200/12672]\n",
      "loss: 18.131485  [ 3600/12672]\n",
      "loss: 16.139654  [ 4000/12672]\n",
      "loss: 11.765730  [ 4400/12672]\n",
      "loss: 8.238675  [ 4800/12672]\n",
      "loss: 14.039829  [ 5200/12672]\n",
      "loss: 14.159582  [ 5600/12672]\n",
      "loss: 7.805453  [ 6000/12672]\n",
      "loss: 7.981040  [ 6400/12672]\n",
      "loss: 14.287803  [ 6800/12672]\n",
      "loss: 13.123188  [ 7200/12672]\n",
      "loss: 12.064352  [ 7600/12672]\n",
      "loss: 16.017061  [ 8000/12672]\n",
      "loss: 22.324369  [ 8400/12672]\n",
      "loss: 9.934460  [ 8800/12672]\n",
      "loss: 4.155950  [ 9200/12672]\n",
      "loss: 19.120373  [ 9600/12672]\n",
      "loss: 12.057743  [10000/12672]\n",
      "loss: 10.811903  [10400/12672]\n",
      "loss: 7.482214  [10800/12672]\n",
      "loss: 19.884293  [11200/12672]\n",
      "loss: 15.217796  [11600/12672]\n",
      "loss: 16.182228  [12000/12672]\n",
      "loss: 9.579833  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Avg loss: 23.999401 \n",
      "\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.469971  [    0/12672]\n",
      "loss: 8.185697  [  400/12672]\n",
      "loss: 10.853790  [  800/12672]\n",
      "loss: 12.540036  [ 1200/12672]\n",
      "loss: 15.900827  [ 1600/12672]\n",
      "loss: 17.563251  [ 2000/12672]\n",
      "loss: 12.235682  [ 2400/12672]\n",
      "loss: 16.347824  [ 2800/12672]\n",
      "loss: 7.042830  [ 3200/12672]\n",
      "loss: 8.950843  [ 3600/12672]\n",
      "loss: 13.864532  [ 4000/12672]\n",
      "loss: 14.991700  [ 4400/12672]\n",
      "loss: 10.055911  [ 4800/12672]\n",
      "loss: 17.945389  [ 5200/12672]\n",
      "loss: 13.379511  [ 5600/12672]\n",
      "loss: 13.228340  [ 6000/12672]\n",
      "loss: 14.096674  [ 6400/12672]\n",
      "loss: 15.256887  [ 6800/12672]\n",
      "loss: 12.325086  [ 7200/12672]\n",
      "loss: 16.655842  [ 7600/12672]\n",
      "loss: 6.907852  [ 8000/12672]\n",
      "loss: 10.289501  [ 8400/12672]\n",
      "loss: 15.458017  [ 8800/12672]\n",
      "loss: 13.156487  [ 9200/12672]\n",
      "loss: 9.132521  [ 9600/12672]\n",
      "loss: 18.210533  [10000/12672]\n",
      "loss: 10.921343  [10400/12672]\n",
      "loss: 16.609474  [10800/12672]\n",
      "loss: 25.303988  [11200/12672]\n",
      "loss: 9.111837  [11600/12672]\n",
      "loss: 12.410911  [12000/12672]\n",
      "loss: 17.120338  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Avg loss: 11.639173 \n",
      "\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.995352  [    0/12672]\n",
      "loss: 20.579924  [  400/12672]\n",
      "loss: 12.873269  [  800/12672]\n",
      "loss: 12.389066  [ 1200/12672]\n",
      "loss: 17.971207  [ 1600/12672]\n",
      "loss: 12.104944  [ 2000/12672]\n",
      "loss: 12.234894  [ 2400/12672]\n",
      "loss: 13.213955  [ 2800/12672]\n",
      "loss: 31.964170  [ 3200/12672]\n",
      "loss: 18.850887  [ 3600/12672]\n",
      "loss: 9.032213  [ 4000/12672]\n",
      "loss: 12.687379  [ 4400/12672]\n",
      "loss: 11.031095  [ 4800/12672]\n",
      "loss: 13.497263  [ 5200/12672]\n",
      "loss: 13.893620  [ 5600/12672]\n",
      "loss: 13.919647  [ 6000/12672]\n",
      "loss: 19.508148  [ 6400/12672]\n",
      "loss: 24.821772  [ 6800/12672]\n",
      "loss: 22.795626  [ 7200/12672]\n",
      "loss: 10.880245  [ 7600/12672]\n",
      "loss: 9.302649  [ 8000/12672]\n",
      "loss: 19.752058  [ 8400/12672]\n",
      "loss: 13.427460  [ 8800/12672]\n",
      "loss: 12.387313  [ 9200/12672]\n",
      "loss: 13.814828  [ 9600/12672]\n",
      "loss: 14.291482  [10000/12672]\n",
      "loss: 17.464123  [10400/12672]\n",
      "loss: 12.016816  [10800/12672]\n",
      "loss: 21.615026  [11200/12672]\n",
      "loss: 10.352825  [11600/12672]\n",
      "loss: 8.616590  [12000/12672]\n",
      "loss: 16.116655  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Avg loss: 12.515601 \n",
      "\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.303319  [    0/12672]\n",
      "loss: 8.861413  [  400/12672]\n",
      "loss: 12.084609  [  800/12672]\n",
      "loss: 14.408398  [ 1200/12672]\n",
      "loss: 26.308914  [ 1600/12672]\n",
      "loss: 9.421227  [ 2000/12672]\n",
      "loss: 14.875638  [ 2400/12672]\n",
      "loss: 15.288671  [ 2800/12672]\n",
      "loss: 16.106947  [ 3200/12672]\n",
      "loss: 12.155720  [ 3600/12672]\n",
      "loss: 10.166431  [ 4000/12672]\n",
      "loss: 6.269804  [ 4400/12672]\n",
      "loss: 9.990419  [ 4800/12672]\n",
      "loss: 11.910367  [ 5200/12672]\n",
      "loss: 23.577917  [ 5600/12672]\n",
      "loss: 20.741671  [ 6000/12672]\n",
      "loss: 15.915788  [ 6400/12672]\n",
      "loss: 17.503172  [ 6800/12672]\n",
      "loss: 12.012377  [ 7200/12672]\n",
      "loss: 14.923263  [ 7600/12672]\n",
      "loss: 18.974543  [ 8000/12672]\n",
      "loss: 15.195709  [ 8400/12672]\n",
      "loss: 10.174289  [ 8800/12672]\n",
      "loss: 6.401777  [ 9200/12672]\n",
      "loss: 21.268303  [ 9600/12672]\n",
      "loss: 16.403627  [10000/12672]\n",
      "loss: 16.186075  [10400/12672]\n",
      "loss: 11.771916  [10800/12672]\n",
      "loss: 8.495223  [11200/12672]\n",
      "loss: 13.724661  [11600/12672]\n",
      "loss: 16.110214  [12000/12672]\n",
      "loss: 15.796165  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 17.372392 \n",
      "\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.784058  [    0/12672]\n",
      "loss: 15.786152  [  400/12672]\n",
      "loss: 16.158751  [  800/12672]\n",
      "loss: 19.234642  [ 1200/12672]\n",
      "loss: 17.091730  [ 1600/12672]\n",
      "loss: 13.708818  [ 2000/12672]\n",
      "loss: 6.746881  [ 2400/12672]\n",
      "loss: 8.083761  [ 2800/12672]\n",
      "loss: 12.905032  [ 3200/12672]\n",
      "loss: 16.447067  [ 3600/12672]\n",
      "loss: 17.348621  [ 4000/12672]\n",
      "loss: 20.821470  [ 4400/12672]\n",
      "loss: 8.115767  [ 4800/12672]\n",
      "loss: 10.315418  [ 5200/12672]\n",
      "loss: 15.641731  [ 5600/12672]\n",
      "loss: 10.003778  [ 6000/12672]\n",
      "loss: 17.689465  [ 6400/12672]\n",
      "loss: 11.771424  [ 6800/12672]\n",
      "loss: 9.236159  [ 7200/12672]\n",
      "loss: 13.616966  [ 7600/12672]\n",
      "loss: 13.914340  [ 8000/12672]\n",
      "loss: 24.049797  [ 8400/12672]\n",
      "loss: 12.333526  [ 8800/12672]\n",
      "loss: 9.009104  [ 9200/12672]\n",
      "loss: 9.193885  [ 9600/12672]\n",
      "loss: 12.357063  [10000/12672]\n",
      "loss: 8.423096  [10400/12672]\n",
      "loss: 22.022852  [10800/12672]\n",
      "loss: 13.279924  [11200/12672]\n",
      "loss: 18.116661  [11600/12672]\n",
      "loss: 12.920464  [12000/12672]\n",
      "loss: 10.147761  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Avg loss: 13.401397 \n",
      "\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.949485  [    0/12672]\n",
      "loss: 8.294992  [  400/12672]\n",
      "loss: 17.950354  [  800/12672]\n",
      "loss: 7.952792  [ 1200/12672]\n",
      "loss: 21.010412  [ 1600/12672]\n",
      "loss: 12.966318  [ 2000/12672]\n",
      "loss: 14.722090  [ 2400/12672]\n",
      "loss: 13.706103  [ 2800/12672]\n",
      "loss: 13.002720  [ 3200/12672]\n",
      "loss: 20.948454  [ 3600/12672]\n",
      "loss: 12.959817  [ 4000/12672]\n",
      "loss: 12.776285  [ 4400/12672]\n",
      "loss: 9.594580  [ 4800/12672]\n",
      "loss: 6.740043  [ 5200/12672]\n",
      "loss: 10.858948  [ 5600/12672]\n",
      "loss: 12.081985  [ 6000/12672]\n",
      "loss: 12.982862  [ 6400/12672]\n",
      "loss: 11.654093  [ 6800/12672]\n",
      "loss: 16.540211  [ 7200/12672]\n",
      "loss: 8.533106  [ 7600/12672]\n",
      "loss: 18.562454  [ 8000/12672]\n",
      "loss: 19.952366  [ 8400/12672]\n",
      "loss: 13.183201  [ 8800/12672]\n",
      "loss: 13.489519  [ 9200/12672]\n",
      "loss: 15.693146  [ 9600/12672]\n",
      "loss: 14.538270  [10000/12672]\n",
      "loss: 18.448578  [10400/12672]\n",
      "loss: 9.776441  [10800/12672]\n",
      "loss: 14.023926  [11200/12672]\n",
      "loss: 6.790817  [11600/12672]\n",
      "loss: 5.435196  [12000/12672]\n",
      "loss: 11.927088  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Avg loss: 9.422605 \n",
      "\n",
      "------------------------------\n",
      "Epoch 11\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.481699  [    0/12672]\n",
      "loss: 17.628441  [  400/12672]\n",
      "loss: 15.440799  [  800/12672]\n",
      "loss: 13.526672  [ 1200/12672]\n",
      "loss: 8.630747  [ 1600/12672]\n",
      "loss: 17.374836  [ 2000/12672]\n",
      "loss: 20.244081  [ 2400/12672]\n",
      "loss: 8.898341  [ 2800/12672]\n",
      "loss: 9.706555  [ 3200/12672]\n",
      "loss: 12.080648  [ 3600/12672]\n",
      "loss: 6.360147  [ 4000/12672]\n",
      "loss: 16.686150  [ 4400/12672]\n",
      "loss: 16.738291  [ 4800/12672]\n",
      "loss: 9.030946  [ 5200/12672]\n",
      "loss: 14.754167  [ 5600/12672]\n",
      "loss: 14.072790  [ 6000/12672]\n",
      "loss: 15.118283  [ 6400/12672]\n",
      "loss: 21.216120  [ 6800/12672]\n",
      "loss: 17.123138  [ 7200/12672]\n",
      "loss: 9.326149  [ 7600/12672]\n",
      "loss: 23.808596  [ 8000/12672]\n",
      "loss: 12.617850  [ 8400/12672]\n",
      "loss: 15.786592  [ 8800/12672]\n",
      "loss: 13.495055  [ 9200/12672]\n",
      "loss: 11.627166  [ 9600/12672]\n",
      "loss: 13.130214  [10000/12672]\n",
      "loss: 13.557033  [10400/12672]\n",
      "loss: 28.438143  [10800/12672]\n",
      "loss: 12.197895  [11200/12672]\n",
      "loss: 8.645572  [11600/12672]\n",
      "loss: 17.772137  [12000/12672]\n",
      "loss: 9.375681  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Avg loss: 14.795683 \n",
      "\n",
      "------------------------------\n",
      "Epoch 12\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.015186  [    0/12672]\n",
      "loss: 18.378218  [  400/12672]\n",
      "loss: 10.905426  [  800/12672]\n",
      "loss: 14.407141  [ 1200/12672]\n",
      "loss: 17.095621  [ 1600/12672]\n",
      "loss: 24.367382  [ 2000/12672]\n",
      "loss: 13.952176  [ 2400/12672]\n",
      "loss: 14.862652  [ 2800/12672]\n",
      "loss: 14.088938  [ 3200/12672]\n",
      "loss: 8.695137  [ 3600/12672]\n",
      "loss: 17.837742  [ 4000/12672]\n",
      "loss: 10.056675  [ 4400/12672]\n",
      "loss: 15.008612  [ 4800/12672]\n",
      "loss: 13.683960  [ 5200/12672]\n",
      "loss: 8.771029  [ 5600/12672]\n",
      "loss: 24.330189  [ 6000/12672]\n",
      "loss: 10.008524  [ 6400/12672]\n",
      "loss: 13.807898  [ 6800/12672]\n",
      "loss: 9.575724  [ 7200/12672]\n",
      "loss: 6.328605  [ 7600/12672]\n",
      "loss: 8.506556  [ 8000/12672]\n",
      "loss: 13.222223  [ 8400/12672]\n",
      "loss: 11.756744  [ 8800/12672]\n",
      "loss: 19.167419  [ 9200/12672]\n",
      "loss: 14.429133  [ 9600/12672]\n",
      "loss: 13.435739  [10000/12672]\n",
      "loss: 11.294412  [10400/12672]\n",
      "loss: 13.987287  [10800/12672]\n",
      "loss: 7.798900  [11200/12672]\n",
      "loss: 16.432383  [11600/12672]\n",
      "loss: 14.301639  [12000/12672]\n",
      "loss: 17.070663  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Avg loss: 24.183126 \n",
      "\n",
      "------------------------------\n",
      "Epoch 13\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 24.290478  [    0/12672]\n",
      "loss: 12.562439  [  400/12672]\n",
      "loss: 20.123627  [  800/12672]\n",
      "loss: 16.138966  [ 1200/12672]\n",
      "loss: 12.933722  [ 1600/12672]\n",
      "loss: 14.098883  [ 2000/12672]\n",
      "loss: 14.329038  [ 2400/12672]\n",
      "loss: 19.382090  [ 2800/12672]\n",
      "loss: 14.456590  [ 3200/12672]\n",
      "loss: 9.724588  [ 3600/12672]\n",
      "loss: 21.644138  [ 4000/12672]\n",
      "loss: 5.628462  [ 4400/12672]\n",
      "loss: 13.378937  [ 4800/12672]\n",
      "loss: 11.573664  [ 5200/12672]\n",
      "loss: 20.805998  [ 5600/12672]\n",
      "loss: 12.693164  [ 6000/12672]\n",
      "loss: 10.590127  [ 6400/12672]\n",
      "loss: 11.371552  [ 6800/12672]\n",
      "loss: 9.033211  [ 7200/12672]\n",
      "loss: 10.337649  [ 7600/12672]\n",
      "loss: 16.199575  [ 8000/12672]\n",
      "loss: 9.331702  [ 8400/12672]\n",
      "loss: 6.877408  [ 8800/12672]\n",
      "loss: 11.971825  [ 9200/12672]\n",
      "loss: 6.992371  [ 9600/12672]\n",
      "loss: 10.481929  [10000/12672]\n",
      "loss: 16.138960  [10400/12672]\n",
      "loss: 12.549189  [10800/12672]\n",
      "loss: 11.699098  [11200/12672]\n",
      "loss: 6.820444  [11600/12672]\n",
      "loss: 10.163265  [12000/12672]\n",
      "loss: 7.893771  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Avg loss: 14.835085 \n",
      "\n",
      "------------------------------\n",
      "Epoch 14\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.221777  [    0/12672]\n",
      "loss: 18.183083  [  400/12672]\n",
      "loss: 12.206839  [  800/12672]\n",
      "loss: 8.189983  [ 1200/12672]\n",
      "loss: 7.791025  [ 1600/12672]\n",
      "loss: 15.121408  [ 2000/12672]\n",
      "loss: 10.265409  [ 2400/12672]\n",
      "loss: 11.727020  [ 2800/12672]\n",
      "loss: 10.553771  [ 3200/12672]\n",
      "loss: 11.364791  [ 3600/12672]\n",
      "loss: 18.377714  [ 4000/12672]\n",
      "loss: 12.504489  [ 4400/12672]\n",
      "loss: 11.490378  [ 4800/12672]\n",
      "loss: 13.047885  [ 5200/12672]\n",
      "loss: 12.386039  [ 5600/12672]\n",
      "loss: 12.195374  [ 6000/12672]\n",
      "loss: 15.164351  [ 6400/12672]\n",
      "loss: 20.665071  [ 6800/12672]\n",
      "loss: 17.923084  [ 7200/12672]\n",
      "loss: 14.874378  [ 7600/12672]\n",
      "loss: 18.247149  [ 8000/12672]\n",
      "loss: 9.630839  [ 8400/12672]\n",
      "loss: 15.790625  [ 8800/12672]\n",
      "loss: 10.902757  [ 9200/12672]\n",
      "loss: 14.080828  [ 9600/12672]\n",
      "loss: 8.631372  [10000/12672]\n",
      "loss: 9.469222  [10400/12672]\n",
      "loss: 11.847634  [10800/12672]\n",
      "loss: 11.521795  [11200/12672]\n",
      "loss: 9.161792  [11600/12672]\n",
      "loss: 12.765641  [12000/12672]\n",
      "loss: 19.950039  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Avg loss: 11.334845 \n",
      "\n",
      "------------------------------\n",
      "Epoch 15\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.034005  [    0/12672]\n",
      "loss: 24.054441  [  400/12672]\n",
      "loss: 18.953207  [  800/12672]\n",
      "loss: 17.820242  [ 1200/12672]\n",
      "loss: 15.984304  [ 1600/12672]\n",
      "loss: 9.668208  [ 2000/12672]\n",
      "loss: 18.507774  [ 2400/12672]\n",
      "loss: 14.505383  [ 2800/12672]\n",
      "loss: 13.899640  [ 3200/12672]\n",
      "loss: 15.674949  [ 3600/12672]\n",
      "loss: 14.519579  [ 4000/12672]\n",
      "loss: 9.527836  [ 4400/12672]\n",
      "loss: 9.200832  [ 4800/12672]\n",
      "loss: 11.880459  [ 5200/12672]\n",
      "loss: 13.229123  [ 5600/12672]\n",
      "loss: 15.025188  [ 6000/12672]\n",
      "loss: 14.049607  [ 6400/12672]\n",
      "loss: 12.471589  [ 6800/12672]\n",
      "loss: 11.801819  [ 7200/12672]\n",
      "loss: 7.602612  [ 7600/12672]\n",
      "loss: 7.913931  [ 8000/12672]\n",
      "loss: 7.411782  [ 8400/12672]\n",
      "loss: 20.745857  [ 8800/12672]\n",
      "loss: 16.161396  [ 9200/12672]\n",
      "loss: 10.264732  [ 9600/12672]\n",
      "loss: 10.733447  [10000/12672]\n",
      "loss: 10.308656  [10400/12672]\n",
      "loss: 9.667546  [10800/12672]\n",
      "loss: 16.173214  [11200/12672]\n",
      "loss: 6.567777  [11600/12672]\n",
      "loss: 17.029745  [12000/12672]\n",
      "loss: 16.983877  [12400/12672]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 12.249934 \n",
      "\n",
      "Done!\n",
      "CPU times: user 1.88 s, sys: 182 ms, total: 2.06 s\n",
      "Wall time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epochs = 15\n",
    "for t in range(epochs):\n",
    "    print(f\"{'-'*30}\\nEpoch {t+1}\\n{'-'*30}\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(train_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "fa642a60-58bb-4586-846f-ab3748391f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.2556009 , -0.11014569, -0.34770104, -0.05363564, -0.32045805,\n",
       "        -0.20047373,  0.04354731, -0.07021297, -0.2549383 , -0.08638977,\n",
       "        -0.12272437, -0.26145813, -0.132995  , -0.5590581 , -0.13800876,\n",
       "        -0.39361808, -0.18007791, -0.14691398, -0.11113048,  0.02109414,\n",
       "        -0.02971033, -0.06245779, -0.23700692, -0.0312883 , -0.05845731,\n",
       "         0.14545608, -0.22800045,  0.13870333, -0.2055197 , -0.5122873 ,\n",
       "        -0.02531125,  0.07469609,  0.00397547, -0.01759478, -0.16063288,\n",
       "        -0.04735122, -0.12375901, -0.19227822, -0.05999327, -0.04843619,\n",
       "        -0.2967028 ,  0.00551004,  0.03163636, -0.06197273,  0.01000183,\n",
       "        -0.21084203, -0.10228419, -0.07310296,  0.11168385, -0.23657438,\n",
       "        -0.2252271 , -0.30507943, -0.02275092, -0.31863666, -0.14079633,\n",
       "        -0.06563782,  0.15927301, -0.18031234,  0.01695587, -0.09139688,\n",
       "        -0.18292908, -0.11702715, -0.5671819 ,  0.15823328, -0.11645681,\n",
       "         0.01410398, -0.08147889, -0.09058848, -0.22371317, -0.10227858,\n",
       "        -0.07714693, -0.1457704 , -0.22057438, -0.20044486,  0.01164881,\n",
       "        -0.33475474, -0.00665838, -0.0153681 , -0.18155982, -0.24766387,\n",
       "        -0.22887512, -0.16248038, -0.16140792, -0.24882337, -0.21806884,\n",
       "        -0.01768024, -0.05802217,  0.03899268, -0.1101112 , -0.02605398,\n",
       "         0.005957  , -0.12131958, -0.03348484, -0.08030755, -0.18201545,\n",
       "        -0.22871569,  0.05486714, -0.02430068, -0.13328029, -0.23076108,\n",
       "        -0.13126285, -0.0348118 , -0.25455052,  0.01738402, -0.08646844,\n",
       "        -0.13921174, -0.01897634, -0.18064915,  0.03457617, -0.2869255 ,\n",
       "         0.08858623, -0.23258016, -0.15405336, -0.05003629,  0.04085288,\n",
       "        -0.26723537,  0.03861602, -0.3729332 ,  0.02717902, -0.2995317 ,\n",
       "        -0.17331788, -0.04101282, -0.07718829, -0.08079364, -0.07807177,\n",
       "        -0.27141556, -0.29208753,  0.01224815]], dtype=float32)"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p for p in model.parameters()][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ce08d-e451-45d9-b3fc-a28c2bedf854",
   "metadata": {},
   "source": [
    "## Compare to sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23c67416-86b6-4434-a7e7-1e8c1a238572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "523260ef-dec1-468c-ab02-fe635822b437",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimpleDataset' object has no attribute 'indices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:3\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SimpleDataset' object has no attribute 'indices'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sk_model = LogisticRegression(penalty='l2', C=1., class_weight='balanced', fit_intercept=True)\n",
    "\n",
    "train_idx = train_set.indices\n",
    "test_idx = test_set.indices\n",
    "sk_train_X = train_set.dataset.X[train_idx].detach().numpy()\n",
    "sk_train_y = train_set.dataset.y[train_idx].detach().numpy()\n",
    "sk_test_X = test_set.dataset.X[test_idx].detach().numpy()\n",
    "sk_test_y = test_set.dataset.y[test_idx].detach().numpy()\n",
    "fit = sk_model.fit(sk_train_X, sk_train_y)\n",
    "\n",
    "sk_pred = fit.predict(sk_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "5f6c6a99-81a3-4ea5-9786-93a96a6a24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "58151025-efbe-4cce-afff-9a5c7e397716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(sk_test_y, sk_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1afe66-561c-4162-bf56-5b951363d036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.37388629, -0.32047521, -0.4095046 , -0.32130418, -0.39439804,\n",
       "        -0.35823253, -0.33484078, -0.33933441, -0.37543385, -0.36355242,\n",
       "        -0.34292282, -0.37185879, -0.3706311 , -0.44209721, -0.36840507,\n",
       "        -0.34789623, -0.41266399, -0.36281521, -0.36685014, -0.3846179 ,\n",
       "        -0.31708437, -0.31090994, -0.37258387, -0.37406865, -0.36461087,\n",
       "        -0.28830832, -0.41198973, -0.3104652 , -0.35061659, -0.40379479,\n",
       "        -0.33055622, -0.33311162, -0.35796505, -0.35008685, -0.35916163,\n",
       "        -0.35937264, -0.39998844, -0.3649274 , -0.37894635, -0.35347019,\n",
       "        -0.37401855, -0.3663856 , -0.30936565, -0.37154138, -0.32723201,\n",
       "        -0.36783979, -0.39499198, -0.3303809 , -0.33748294, -0.36837342,\n",
       "        -0.39150569, -0.36852701, -0.3572353 , -0.37949994, -0.33189788,\n",
       "        -0.40627196, -0.35212517, -0.37479089, -0.33094589, -0.37000063,\n",
       "        -0.3751616 , -0.33788822, -0.37261937, -0.31014997, -0.34313105,\n",
       "        -0.35801088, -0.36816738, -0.34082888, -0.3879882 , -0.3457667 ,\n",
       "        -0.36235728, -0.32733371, -0.39650571, -0.39248756, -0.32821235,\n",
       "        -0.3990739 , -0.30894712, -0.34339028, -0.34547438, -0.35086806,\n",
       "        -0.40395907, -0.37280974, -0.31477501, -0.42060536, -0.38097354,\n",
       "        -0.3758629 , -0.35815151, -0.34744084, -0.32995486, -0.35151104,\n",
       "        -0.32959605, -0.37197039, -0.3986891 , -0.3753245 , -0.39787029,\n",
       "        -0.39702534, -0.35969944, -0.32618243, -0.37155474, -0.40618902,\n",
       "        -0.38696675, -0.34085002, -0.37607757, -0.38614888, -0.39346724,\n",
       "        -0.38347606, -0.33018366, -0.37635166, -0.3128696 , -0.39317084,\n",
       "        -0.31900011, -0.37878595, -0.37034107, -0.33262278, -0.28697747,\n",
       "        -0.34756017, -0.35172575, -0.38714172, -0.33827419, -0.41180401,\n",
       "        -0.31481693, -0.33421305, -0.31779403, -0.30730132, -0.3548753 ,\n",
       "        -0.42106957, -0.38216169, -0.32547378]])"
      ]
     },
     "execution_count": 555,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a23101-0bd3-4961-b112-e36be91814f4",
   "metadata": {},
   "source": [
    "## Using lab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c55e7286-4c5d-48d6-85ac-6bc3d166cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptsa.data.timeseries import TimeSeries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, LeaveOneGroupOut\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be80b2f1-34c7-458e-88a0-a1c8ae83f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeSeries.from_hdf(\n",
    "    \"/Users/jrudoler/rhino_mount/scratch/jrudoler/scalp_features/LTP093_feats.h5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3edface1-79d9-4147-87e1-79869a0c0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_npy = ts.data\n",
    "y_npy = ts.recalled.data\n",
    "X = torch.tensor(ts.data).float()\n",
    "y = torch.tensor(ts.recalled.data).float()\n",
    "dataset = SimpleDataset(X, y)\n",
    "sessions = ts.session.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cbc2f825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.y.detach().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e42c29f-5d38-4187-88cb-36414f76b390",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "SESSION 0\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.715226  [    0/12672]\n",
      "loss: 28.113054  [ 1600/12672]\n",
      "loss: 23.311895  [ 3200/12672]\n",
      "loss: 7.153251  [ 4800/12672]\n",
      "loss: 7.607974  [ 6400/12672]\n",
      "loss: 11.595755  [ 8000/12672]\n",
      "loss: 11.874519  [ 9600/12672]\n",
      "loss: 8.548401  [11200/12672]\n",
      "AUC: 0.5834685577193572\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.464243  [    0/12672]\n",
      "loss: 16.580606  [ 1600/12672]\n",
      "loss: 5.655796  [ 3200/12672]\n",
      "loss: 10.774112  [ 4800/12672]\n",
      "loss: 25.277502  [ 6400/12672]\n",
      "loss: 12.422085  [ 8000/12672]\n",
      "loss: 30.589161  [ 9600/12672]\n",
      "loss: 18.302435  [11200/12672]\n",
      "AUC: 0.505155262818357\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.837206  [    0/12672]\n",
      "loss: 13.396523  [ 1600/12672]\n",
      "loss: 26.100977  [ 3200/12672]\n",
      "loss: 18.889591  [ 4800/12672]\n",
      "loss: 15.965723  [ 6400/12672]\n",
      "loss: 18.339470  [ 8000/12672]\n",
      "loss: 13.517723  [ 9600/12672]\n",
      "loss: 14.113721  [11200/12672]\n",
      "AUC: 0.5362083023992182\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.240176  [    0/12672]\n",
      "loss: 7.189338  [ 1600/12672]\n",
      "loss: 16.177618  [ 3200/12672]\n",
      "loss: 13.554495  [ 4800/12672]\n",
      "loss: 7.776077  [ 6400/12672]\n",
      "loss: 21.111191  [ 8000/12672]\n",
      "loss: 25.304558  [ 9600/12672]\n",
      "loss: 9.754707  [11200/12672]\n",
      "AUC: 0.5844544702704646\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 16.474455  [    0/12672]\n",
      "loss: 15.202898  [ 1600/12672]\n",
      "loss: 12.523672  [ 3200/12672]\n",
      "loss: 18.449915  [ 4800/12672]\n",
      "loss: 9.754363  [ 6400/12672]\n",
      "loss: 13.124402  [ 8000/12672]\n",
      "loss: 15.596818  [ 9600/12672]\n",
      "loss: 9.380241  [11200/12672]\n",
      "AUC: 0.547717670412603\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 16.381584  [    0/12672]\n",
      "loss: 10.442935  [ 1600/12672]\n",
      "loss: 15.563458  [ 3200/12672]\n",
      "loss: 19.044828  [ 4800/12672]\n",
      "loss: 13.598057  [ 6400/12672]\n",
      "loss: 11.278760  [ 8000/12672]\n",
      "loss: 8.022305  [ 9600/12672]\n",
      "loss: 13.727253  [11200/12672]\n",
      "AUC: 0.5789375426996546\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.127572  [    0/12672]\n",
      "loss: 19.210915  [ 1600/12672]\n",
      "loss: 24.479389  [ 3200/12672]\n",
      "loss: 7.513110  [ 4800/12672]\n",
      "loss: 16.104588  [ 6400/12672]\n",
      "loss: 13.843170  [ 8000/12672]\n",
      "loss: 23.482447  [ 9600/12672]\n",
      "loss: 6.864529  [11200/12672]\n",
      "AUC: 0.6230505690819601\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.617122  [    0/12672]\n",
      "loss: 10.495083  [ 1600/12672]\n",
      "loss: 23.673851  [ 3200/12672]\n",
      "loss: 19.174730  [ 4800/12672]\n",
      "loss: 8.016410  [ 6400/12672]\n",
      "loss: 8.067251  [ 8000/12672]\n",
      "loss: 12.718494  [ 9600/12672]\n",
      "loss: 14.298000  [11200/12672]\n",
      "AUC: 0.5706037731159801\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 19.086563  [    0/12672]\n",
      "loss: 20.397110  [ 1600/12672]\n",
      "loss: 12.639317  [ 3200/12672]\n",
      "loss: 14.813726  [ 4800/12672]\n",
      "loss: 9.557766  [ 6400/12672]\n",
      "loss: 15.268246  [ 8000/12672]\n",
      "loss: 10.677037  [ 9600/12672]\n",
      "loss: 11.283381  [11200/12672]\n",
      "AUC: 0.5765379939675832\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.134035  [    0/12672]\n",
      "loss: 23.093513  [ 1600/12672]\n",
      "loss: 14.361946  [ 3200/12672]\n",
      "loss: 11.977224  [ 4800/12672]\n",
      "loss: 20.828341  [ 6400/12672]\n",
      "loss: 4.740025  [ 8000/12672]\n",
      "loss: 14.051177  [ 9600/12672]\n",
      "loss: 7.109088  [11200/12672]\n",
      "AUC: 0.6301973940255383\n",
      "sklearn AUC: 0.6258230711233979\n",
      "Done!\n",
      "##############################\n",
      "SESSION 1\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.749624  [    0/12672]\n",
      "loss: 23.469131  [ 1600/12672]\n",
      "loss: 7.999800  [ 3200/12672]\n",
      "loss: 27.909992  [ 4800/12672]\n",
      "loss: 12.847679  [ 6400/12672]\n",
      "loss: 21.141069  [ 8000/12672]\n",
      "loss: 8.568604  [ 9600/12672]\n",
      "loss: 17.806395  [11200/12672]\n",
      "AUC: 0.5346537966844512\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.601988  [    0/12672]\n",
      "loss: 9.055809  [ 1600/12672]\n",
      "loss: 4.381711  [ 3200/12672]\n",
      "loss: 20.570337  [ 4800/12672]\n",
      "loss: 23.096739  [ 6400/12672]\n",
      "loss: 18.118895  [ 8000/12672]\n",
      "loss: 12.379315  [ 9600/12672]\n",
      "loss: 20.174086  [11200/12672]\n",
      "AUC: 0.5211902682503388\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.157509  [    0/12672]\n",
      "loss: 22.300537  [ 1600/12672]\n",
      "loss: 14.524863  [ 3200/12672]\n",
      "loss: 9.280479  [ 4800/12672]\n",
      "loss: 19.492266  [ 6400/12672]\n",
      "loss: 7.211594  [ 8000/12672]\n",
      "loss: 25.774275  [ 9600/12672]\n",
      "loss: 7.319165  [11200/12672]\n",
      "AUC: 0.5733829686441395\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.495289  [    0/12672]\n",
      "loss: 19.689335  [ 1600/12672]\n",
      "loss: 9.804257  [ 3200/12672]\n",
      "loss: 13.496001  [ 4800/12672]\n",
      "loss: 11.819093  [ 6400/12672]\n",
      "loss: 8.819474  [ 8000/12672]\n",
      "loss: 6.297133  [ 9600/12672]\n",
      "loss: 11.797564  [11200/12672]\n",
      "AUC: 0.608432352536416\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 7.316751  [    0/12672]\n",
      "loss: 17.209570  [ 1600/12672]\n",
      "loss: 23.397190  [ 3200/12672]\n",
      "loss: 25.707237  [ 4800/12672]\n",
      "loss: 11.238980  [ 6400/12672]\n",
      "loss: 22.779579  [ 8000/12672]\n",
      "loss: 12.992916  [ 9600/12672]\n",
      "loss: 9.276409  [11200/12672]\n",
      "AUC: 0.5724436160865515\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.183422  [    0/12672]\n",
      "loss: 14.182988  [ 1600/12672]\n",
      "loss: 15.810595  [ 3200/12672]\n",
      "loss: 21.424837  [ 4800/12672]\n",
      "loss: 19.975296  [ 6400/12672]\n",
      "loss: 16.218168  [ 8000/12672]\n",
      "loss: 6.258354  [ 9600/12672]\n",
      "loss: 7.725555  [11200/12672]\n",
      "AUC: 0.5489429703379065\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.807389  [    0/12672]\n",
      "loss: 12.550035  [ 1600/12672]\n",
      "loss: 17.176163  [ 3200/12672]\n",
      "loss: 18.852913  [ 4800/12672]\n",
      "loss: 8.919753  [ 6400/12672]\n",
      "loss: 6.963645  [ 8000/12672]\n",
      "loss: 17.222332  [ 9600/12672]\n",
      "loss: 15.748470  [11200/12672]\n",
      "AUC: 0.5112027994791667\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.342876  [    0/12672]\n",
      "loss: 5.807655  [ 1600/12672]\n",
      "loss: 19.350801  [ 3200/12672]\n",
      "loss: 7.709547  [ 4800/12672]\n",
      "loss: 15.655282  [ 6400/12672]\n",
      "loss: 9.446871  [ 8000/12672]\n",
      "loss: 18.404024  [ 9600/12672]\n",
      "loss: 24.483940  [11200/12672]\n",
      "AUC: 0.6440369347052846\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.552084  [    0/12672]\n",
      "loss: 7.673619  [ 1600/12672]\n",
      "loss: 7.583701  [ 3200/12672]\n",
      "loss: 16.329905  [ 4800/12672]\n",
      "loss: 8.515779  [ 6400/12672]\n",
      "loss: 5.967239  [ 8000/12672]\n",
      "loss: 11.004559  [ 9600/12672]\n",
      "loss: 6.925168  [11200/12672]\n",
      "AUC: 0.5352269118394308\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 20.743896  [    0/12672]\n",
      "loss: 13.897046  [ 1600/12672]\n",
      "loss: 9.148745  [ 3200/12672]\n",
      "loss: 23.646694  [ 4800/12672]\n",
      "loss: 15.465236  [ 6400/12672]\n",
      "loss: 15.636580  [ 8000/12672]\n",
      "loss: 18.694195  [ 9600/12672]\n",
      "loss: 13.224422  [11200/12672]\n",
      "AUC: 0.602440149157351\n",
      "sklearn AUC: 0.5777453357747315\n",
      "Done!\n",
      "##############################\n",
      "SESSION 2\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.690923  [    0/12672]\n",
      "loss: 6.538356  [ 1600/12672]\n",
      "loss: 16.460297  [ 3200/12672]\n",
      "loss: 24.004908  [ 4800/12672]\n",
      "loss: 8.778023  [ 6400/12672]\n",
      "loss: 16.801935  [ 8000/12672]\n",
      "loss: 19.934235  [ 9600/12672]\n",
      "loss: 6.439700  [11200/12672]\n",
      "AUC: 0.5458341297525474\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.906136  [    0/12672]\n",
      "loss: 21.288078  [ 1600/12672]\n",
      "loss: 21.233942  [ 3200/12672]\n",
      "loss: 19.319937  [ 4800/12672]\n",
      "loss: 17.001904  [ 6400/12672]\n",
      "loss: 14.758583  [ 8000/12672]\n",
      "loss: 10.941689  [ 9600/12672]\n",
      "loss: 12.456892  [11200/12672]\n",
      "AUC: 0.5206783996422393\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.602015  [    0/12672]\n",
      "loss: 22.515921  [ 1600/12672]\n",
      "loss: 10.395482  [ 3200/12672]\n",
      "loss: 18.578819  [ 4800/12672]\n",
      "loss: 11.882608  [ 6400/12672]\n",
      "loss: 10.774096  [ 8000/12672]\n",
      "loss: 16.199385  [ 9600/12672]\n",
      "loss: 12.784676  [11200/12672]\n",
      "AUC: 0.5780482936549831\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.735964  [    0/12672]\n",
      "loss: 5.958128  [ 1600/12672]\n",
      "loss: 13.911795  [ 3200/12672]\n",
      "loss: 21.706314  [ 4800/12672]\n",
      "loss: 13.208634  [ 6400/12672]\n",
      "loss: 13.579856  [ 8000/12672]\n",
      "loss: 13.910383  [ 9600/12672]\n",
      "loss: 11.310794  [11200/12672]\n",
      "AUC: 0.5746472998089446\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.198223  [    0/12672]\n",
      "loss: 8.503131  [ 1600/12672]\n",
      "loss: 9.463654  [ 3200/12672]\n",
      "loss: 16.570501  [ 4800/12672]\n",
      "loss: 8.026132  [ 6400/12672]\n",
      "loss: 16.098434  [ 8000/12672]\n",
      "loss: 22.448723  [ 9600/12672]\n",
      "loss: 10.163258  [11200/12672]\n",
      "AUC: 0.5848494432444352\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 20.844170  [    0/12672]\n",
      "loss: 8.568468  [ 1600/12672]\n",
      "loss: 10.610126  [ 3200/12672]\n",
      "loss: 24.882957  [ 4800/12672]\n",
      "loss: 15.679089  [ 6400/12672]\n",
      "loss: 14.268633  [ 8000/12672]\n",
      "loss: 13.015006  [ 9600/12672]\n",
      "loss: 6.895624  [11200/12672]\n",
      "AUC: 0.5792003255775937\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.250518  [    0/12672]\n",
      "loss: 12.924414  [ 1600/12672]\n",
      "loss: 15.381177  [ 3200/12672]\n",
      "loss: 11.731762  [ 4800/12672]\n",
      "loss: 16.031584  [ 6400/12672]\n",
      "loss: 9.410464  [ 8000/12672]\n",
      "loss: 11.341056  [ 9600/12672]\n",
      "loss: 15.720920  [11200/12672]\n",
      "AUC: 0.6164856622192063\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 8.019010  [    0/12672]\n",
      "loss: 21.129414  [ 1600/12672]\n",
      "loss: 10.560328  [ 3200/12672]\n",
      "loss: 12.745606  [ 4800/12672]\n",
      "loss: 15.788404  [ 6400/12672]\n",
      "loss: 10.897015  [ 8000/12672]\n",
      "loss: 19.224237  [ 9600/12672]\n",
      "loss: 9.912601  [11200/12672]\n",
      "AUC: 0.567176905318562\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.524294  [    0/12672]\n",
      "loss: 14.733440  [ 1600/12672]\n",
      "loss: 10.277185  [ 3200/12672]\n",
      "loss: 11.263863  [ 4800/12672]\n",
      "loss: 9.830439  [ 6400/12672]\n",
      "loss: 6.257985  [ 8000/12672]\n",
      "loss: 11.399999  [ 9600/12672]\n",
      "loss: 14.656161  [11200/12672]\n",
      "AUC: 0.5636970234000381\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 22.853836  [    0/12672]\n",
      "loss: 9.018070  [ 1600/12672]\n",
      "loss: 9.511609  [ 3200/12672]\n",
      "loss: 12.193716  [ 4800/12672]\n",
      "loss: 9.993305  [ 6400/12672]\n",
      "loss: 15.347598  [ 8000/12672]\n",
      "loss: 11.460405  [ 9600/12672]\n",
      "loss: 13.216292  [11200/12672]\n",
      "AUC: 0.5461346760145352\n",
      "sklearn AUC: 0.5289760348583878\n",
      "Done!\n",
      "##############################\n",
      "SESSION 3\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.725551  [    0/12672]\n",
      "loss: 13.753274  [ 1600/12672]\n",
      "loss: 32.381802  [ 3200/12672]\n",
      "loss: 21.081680  [ 4800/12672]\n",
      "loss: 5.770885  [ 6400/12672]\n",
      "loss: 16.092821  [ 8000/12672]\n",
      "loss: 10.303390  [ 9600/12672]\n",
      "loss: 4.767251  [11200/12672]\n",
      "AUC: 0.5150016972955294\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.702995  [    0/12672]\n",
      "loss: 19.535910  [ 1600/12672]\n",
      "loss: 6.303327  [ 3200/12672]\n",
      "loss: 17.092955  [ 4800/12672]\n",
      "loss: 22.373669  [ 6400/12672]\n",
      "loss: 8.368511  [ 8000/12672]\n",
      "loss: 21.726870  [ 9600/12672]\n",
      "loss: 14.924796  [11200/12672]\n",
      "AUC: 0.5670556527465824\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.091181  [    0/12672]\n",
      "loss: 17.569817  [ 1600/12672]\n",
      "loss: 11.479671  [ 3200/12672]\n",
      "loss: 8.689132  [ 4800/12672]\n",
      "loss: 12.706155  [ 6400/12672]\n",
      "loss: 31.610762  [ 8000/12672]\n",
      "loss: 5.399578  [ 9600/12672]\n",
      "loss: 10.852558  [11200/12672]\n",
      "AUC: 0.5385839971579971\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 38.162888  [    0/12672]\n",
      "loss: 10.978498  [ 1600/12672]\n",
      "loss: 25.212042  [ 3200/12672]\n",
      "loss: 4.493764  [ 4800/12672]\n",
      "loss: 18.415285  [ 6400/12672]\n",
      "loss: 8.503175  [ 8000/12672]\n",
      "loss: 5.201546  [ 9600/12672]\n",
      "loss: 9.491375  [11200/12672]\n",
      "AUC: 0.5691048602213807\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.661177  [    0/12672]\n",
      "loss: 22.868975  [ 1600/12672]\n",
      "loss: 18.609627  [ 3200/12672]\n",
      "loss: 24.811098  [ 4800/12672]\n",
      "loss: 19.432941  [ 6400/12672]\n",
      "loss: 17.692045  [ 8000/12672]\n",
      "loss: 18.365456  [ 9600/12672]\n",
      "loss: 19.166594  [11200/12672]\n",
      "AUC: 0.5280756552922607\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 26.503340  [    0/12672]\n",
      "loss: 17.055309  [ 1600/12672]\n",
      "loss: 21.122383  [ 3200/12672]\n",
      "loss: 17.948242  [ 4800/12672]\n",
      "loss: 17.883657  [ 6400/12672]\n",
      "loss: 28.461040  [ 8000/12672]\n",
      "loss: 24.728802  [ 9600/12672]\n",
      "loss: 13.306130  [11200/12672]\n",
      "AUC: 0.6019798133642602\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.942687  [    0/12672]\n",
      "loss: 6.851149  [ 1600/12672]\n",
      "loss: 6.767670  [ 3200/12672]\n",
      "loss: 12.836329  [ 4800/12672]\n",
      "loss: 9.914505  [ 6400/12672]\n",
      "loss: 12.157206  [ 8000/12672]\n",
      "loss: 6.227417  [ 9600/12672]\n",
      "loss: 8.825964  [11200/12672]\n",
      "AUC: 0.6260451641702873\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 8.674194  [    0/12672]\n",
      "loss: 23.155312  [ 1600/12672]\n",
      "loss: 7.148142  [ 3200/12672]\n",
      "loss: 7.645627  [ 4800/12672]\n",
      "loss: 23.432741  [ 6400/12672]\n",
      "loss: 10.964618  [ 8000/12672]\n",
      "loss: 18.441154  [ 9600/12672]\n",
      "loss: 9.716373  [11200/12672]\n",
      "AUC: 0.5726030085636323\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.917759  [    0/12672]\n",
      "loss: 9.849727  [ 1600/12672]\n",
      "loss: 13.141308  [ 3200/12672]\n",
      "loss: 9.867755  [ 4800/12672]\n",
      "loss: 13.255713  [ 6400/12672]\n",
      "loss: 17.888725  [ 8000/12672]\n",
      "loss: 14.131803  [ 9600/12672]\n",
      "loss: 15.806078  [11200/12672]\n",
      "AUC: 0.672526871372978\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 6.523248  [    0/12672]\n",
      "loss: 9.248798  [ 1600/12672]\n",
      "loss: 13.588970  [ 3200/12672]\n",
      "loss: 10.926445  [ 4800/12672]\n",
      "loss: 18.551498  [ 6400/12672]\n",
      "loss: 31.682411  [ 8000/12672]\n",
      "loss: 11.085688  [ 9600/12672]\n",
      "loss: 12.161393  [11200/12672]\n",
      "AUC: 0.6596657641979062\n",
      "sklearn AUC: 0.5931030273437499\n",
      "Done!\n",
      "##############################\n",
      "SESSION 4\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.714576  [    0/12672]\n",
      "loss: 18.770544  [ 1600/12672]\n",
      "loss: 9.537768  [ 3200/12672]\n",
      "loss: 15.620822  [ 4800/12672]\n",
      "loss: 12.627920  [ 6400/12672]\n",
      "loss: 23.854038  [ 8000/12672]\n",
      "loss: 21.503252  [ 9600/12672]\n",
      "loss: 16.649857  [11200/12672]\n",
      "AUC: 0.5620765909013403\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 8.242129  [    0/12672]\n",
      "loss: 26.282642  [ 1600/12672]\n",
      "loss: 23.750765  [ 3200/12672]\n",
      "loss: 14.751141  [ 4800/12672]\n",
      "loss: 17.180140  [ 6400/12672]\n",
      "loss: 6.250386  [ 8000/12672]\n",
      "loss: 23.342567  [ 9600/12672]\n",
      "loss: 19.600052  [11200/12672]\n",
      "AUC: 0.5222336135456809\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 29.904749  [    0/12672]\n",
      "loss: 15.405723  [ 1600/12672]\n",
      "loss: 10.535813  [ 3200/12672]\n",
      "loss: 22.415386  [ 4800/12672]\n",
      "loss: 8.264988  [ 6400/12672]\n",
      "loss: 15.780605  [ 8000/12672]\n",
      "loss: 11.571835  [ 9600/12672]\n",
      "loss: 14.287642  [11200/12672]\n",
      "AUC: 0.5363147236171264\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.685223  [    0/12672]\n",
      "loss: 23.653872  [ 1600/12672]\n",
      "loss: 16.668941  [ 3200/12672]\n",
      "loss: 21.662560  [ 4800/12672]\n",
      "loss: 17.299477  [ 6400/12672]\n",
      "loss: 19.046021  [ 8000/12672]\n",
      "loss: 12.168999  [ 9600/12672]\n",
      "loss: 11.589901  [11200/12672]\n",
      "AUC: 0.54125118827371\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 24.690044  [    0/12672]\n",
      "loss: 9.838947  [ 1600/12672]\n",
      "loss: 9.766959  [ 3200/12672]\n",
      "loss: 17.312593  [ 4800/12672]\n",
      "loss: 11.772295  [ 6400/12672]\n",
      "loss: 8.284677  [ 8000/12672]\n",
      "loss: 17.039129  [ 9600/12672]\n",
      "loss: 14.024576  [11200/12672]\n",
      "AUC: 0.5847795992054019\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.161228  [    0/12672]\n",
      "loss: 11.377128  [ 1600/12672]\n",
      "loss: 6.620643  [ 3200/12672]\n",
      "loss: 24.603693  [ 4800/12672]\n",
      "loss: 17.868244  [ 6400/12672]\n",
      "loss: 19.523682  [ 8000/12672]\n",
      "loss: 14.949017  [ 9600/12672]\n",
      "loss: 16.386690  [11200/12672]\n",
      "AUC: 0.5795589213245576\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.108191  [    0/12672]\n",
      "loss: 5.909182  [ 1600/12672]\n",
      "loss: 16.508150  [ 3200/12672]\n",
      "loss: 13.935666  [ 4800/12672]\n",
      "loss: 20.058388  [ 6400/12672]\n",
      "loss: 15.092589  [ 8000/12672]\n",
      "loss: 10.699802  [ 9600/12672]\n",
      "loss: 18.370010  [11200/12672]\n",
      "AUC: 0.64954483255465\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.005939  [    0/12672]\n",
      "loss: 22.727732  [ 1600/12672]\n",
      "loss: 17.085924  [ 3200/12672]\n",
      "loss: 13.297688  [ 4800/12672]\n",
      "loss: 10.611917  [ 6400/12672]\n",
      "loss: 9.372180  [ 8000/12672]\n",
      "loss: 13.361320  [ 9600/12672]\n",
      "loss: 21.240433  [11200/12672]\n",
      "AUC: 0.6618948995882072\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 6.792980  [    0/12672]\n",
      "loss: 14.779840  [ 1600/12672]\n",
      "loss: 8.468869  [ 3200/12672]\n",
      "loss: 8.450678  [ 4800/12672]\n",
      "loss: 16.578688  [ 6400/12672]\n",
      "loss: 24.908581  [ 8000/12672]\n",
      "loss: 18.339159  [ 9600/12672]\n",
      "loss: 9.774585  [11200/12672]\n",
      "AUC: 0.5605641396045766\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 16.269983  [    0/12672]\n",
      "loss: 9.780233  [ 1600/12672]\n",
      "loss: 7.276145  [ 3200/12672]\n",
      "loss: 6.697405  [ 4800/12672]\n",
      "loss: 16.178972  [ 6400/12672]\n",
      "loss: 8.584454  [ 8000/12672]\n",
      "loss: 17.420238  [ 9600/12672]\n",
      "loss: 8.893877  [11200/12672]\n",
      "AUC: 0.6233398404196364\n",
      "sklearn AUC: 0.5008803247420159\n",
      "Done!\n",
      "##############################\n",
      "SESSION 5\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.835740  [    0/12672]\n",
      "loss: 14.778576  [ 1600/12672]\n",
      "loss: 20.509859  [ 3200/12672]\n",
      "loss: 23.007534  [ 4800/12672]\n",
      "loss: 28.925190  [ 6400/12672]\n",
      "loss: 6.881963  [ 8000/12672]\n",
      "loss: 19.650215  [ 9600/12672]\n",
      "loss: 12.584612  [11200/12672]\n",
      "AUC: 0.5874600688831602\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 5.041436  [    0/12672]\n",
      "loss: 18.623955  [ 1600/12672]\n",
      "loss: 12.155103  [ 3200/12672]\n",
      "loss: 13.028391  [ 4800/12672]\n",
      "loss: 22.804434  [ 6400/12672]\n",
      "loss: 25.320086  [ 8000/12672]\n",
      "loss: 10.865601  [ 9600/12672]\n",
      "loss: 7.228955  [11200/12672]\n",
      "AUC: 0.5901274373373979\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.155194  [    0/12672]\n",
      "loss: 7.389863  [ 1600/12672]\n",
      "loss: 7.559058  [ 3200/12672]\n",
      "loss: 11.451647  [ 4800/12672]\n",
      "loss: 20.760500  [ 6400/12672]\n",
      "loss: 12.521010  [ 8000/12672]\n",
      "loss: 13.408195  [ 9600/12672]\n",
      "loss: 15.275612  [11200/12672]\n",
      "AUC: 0.5423372059834152\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.617966  [    0/12672]\n",
      "loss: 13.734327  [ 1600/12672]\n",
      "loss: 10.428301  [ 3200/12672]\n",
      "loss: 9.792829  [ 4800/12672]\n",
      "loss: 15.700708  [ 6400/12672]\n",
      "loss: 17.683029  [ 8000/12672]\n",
      "loss: 14.613111  [ 9600/12672]\n",
      "loss: 8.863370  [11200/12672]\n",
      "AUC: 0.5284165648596862\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 26.107492  [    0/12672]\n",
      "loss: 11.166456  [ 1600/12672]\n",
      "loss: 16.993731  [ 3200/12672]\n",
      "loss: 16.955246  [ 4800/12672]\n",
      "loss: 13.086208  [ 6400/12672]\n",
      "loss: 9.779654  [ 8000/12672]\n",
      "loss: 17.458427  [ 9600/12672]\n",
      "loss: 20.337070  [11200/12672]\n",
      "AUC: 0.5677008044832028\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 19.901438  [    0/12672]\n",
      "loss: 19.616655  [ 1600/12672]\n",
      "loss: 19.279282  [ 3200/12672]\n",
      "loss: 8.787535  [ 4800/12672]\n",
      "loss: 18.360435  [ 6400/12672]\n",
      "loss: 16.885714  [ 8000/12672]\n",
      "loss: 19.096668  [ 9600/12672]\n",
      "loss: 11.475866  [11200/12672]\n",
      "AUC: 0.6542359356142611\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 8.910748  [    0/12672]\n",
      "loss: 15.209587  [ 1600/12672]\n",
      "loss: 5.323133  [ 3200/12672]\n",
      "loss: 13.462834  [ 4800/12672]\n",
      "loss: 26.658226  [ 6400/12672]\n",
      "loss: 16.460793  [ 8000/12672]\n",
      "loss: 12.236510  [ 9600/12672]\n",
      "loss: 21.180164  [11200/12672]\n",
      "AUC: 0.6289764362507045\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.224801  [    0/12672]\n",
      "loss: 5.648539  [ 1600/12672]\n",
      "loss: 12.268641  [ 3200/12672]\n",
      "loss: 35.858742  [ 4800/12672]\n",
      "loss: 17.565971  [ 6400/12672]\n",
      "loss: 11.085686  [ 8000/12672]\n",
      "loss: 16.441811  [ 9600/12672]\n",
      "loss: 17.256180  [11200/12672]\n",
      "AUC: 0.5469959068092685\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 25.231880  [    0/12672]\n",
      "loss: 14.948264  [ 1600/12672]\n",
      "loss: 8.167850  [ 3200/12672]\n",
      "loss: 17.031185  [ 4800/12672]\n",
      "loss: 19.152136  [ 6400/12672]\n",
      "loss: 11.947417  [ 8000/12672]\n",
      "loss: 27.807348  [ 9600/12672]\n",
      "loss: 11.476431  [11200/12672]\n",
      "AUC: 0.5353996042116986\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 24.158432  [    0/12672]\n",
      "loss: 15.874881  [ 1600/12672]\n",
      "loss: 7.008649  [ 3200/12672]\n",
      "loss: 11.690351  [ 4800/12672]\n",
      "loss: 14.813700  [ 6400/12672]\n",
      "loss: 18.301111  [ 8000/12672]\n",
      "loss: 6.410427  [ 9600/12672]\n",
      "loss: 12.049978  [11200/12672]\n",
      "AUC: 0.617825411360855\n",
      "sklearn AUC: 0.5709376718148649\n",
      "Done!\n",
      "##############################\n",
      "SESSION 6\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.703948  [    0/12672]\n",
      "loss: 28.659904  [ 1600/12672]\n",
      "loss: 15.354025  [ 3200/12672]\n",
      "loss: 3.226684  [ 4800/12672]\n",
      "loss: 14.031034  [ 6400/12672]\n",
      "loss: 29.484783  [ 8000/12672]\n",
      "loss: 19.502907  [ 9600/12672]\n",
      "loss: 15.576056  [11200/12672]\n",
      "AUC: 0.5460400483663084\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 23.329966  [    0/12672]\n",
      "loss: 10.885580  [ 1600/12672]\n",
      "loss: 21.727295  [ 3200/12672]\n",
      "loss: 21.897461  [ 4800/12672]\n",
      "loss: 9.233095  [ 6400/12672]\n",
      "loss: 24.006382  [ 8000/12672]\n",
      "loss: 26.303007  [ 9600/12672]\n",
      "loss: 9.205705  [11200/12672]\n",
      "AUC: 0.5250692010257909\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.611168  [    0/12672]\n",
      "loss: 20.552988  [ 1600/12672]\n",
      "loss: 18.016697  [ 3200/12672]\n",
      "loss: 13.420577  [ 4800/12672]\n",
      "loss: 9.091130  [ 6400/12672]\n",
      "loss: 18.251806  [ 8000/12672]\n",
      "loss: 20.655954  [ 9600/12672]\n",
      "loss: 18.104443  [11200/12672]\n",
      "AUC: 0.5640452038958794\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 23.565990  [    0/12672]\n",
      "loss: 24.710180  [ 1600/12672]\n",
      "loss: 25.985380  [ 3200/12672]\n",
      "loss: 23.708441  [ 4800/12672]\n",
      "loss: 6.535677  [ 6400/12672]\n",
      "loss: 6.400998  [ 8000/12672]\n",
      "loss: 19.867105  [ 9600/12672]\n",
      "loss: 5.500887  [11200/12672]\n",
      "AUC: 0.5464370772930812\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 27.251572  [    0/12672]\n",
      "loss: 12.279294  [ 1600/12672]\n",
      "loss: 18.620804  [ 3200/12672]\n",
      "loss: 19.605932  [ 4800/12672]\n",
      "loss: 5.275739  [ 6400/12672]\n",
      "loss: 22.639637  [ 8000/12672]\n",
      "loss: 19.955812  [ 9600/12672]\n",
      "loss: 13.008784  [11200/12672]\n",
      "AUC: 0.5480048366308349\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.565699  [    0/12672]\n",
      "loss: 24.438728  [ 1600/12672]\n",
      "loss: 8.583156  [ 3200/12672]\n",
      "loss: 14.372184  [ 4800/12672]\n",
      "loss: 11.785434  [ 6400/12672]\n",
      "loss: 16.830097  [ 8000/12672]\n",
      "loss: 12.319265  [ 9600/12672]\n",
      "loss: 7.414976  [11200/12672]\n",
      "AUC: 0.6026467930745825\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.968916  [    0/12672]\n",
      "loss: 21.822598  [ 1600/12672]\n",
      "loss: 5.979042  [ 3200/12672]\n",
      "loss: 16.466778  [ 4800/12672]\n",
      "loss: 26.626814  [ 6400/12672]\n",
      "loss: 8.700656  [ 8000/12672]\n",
      "loss: 7.807556  [ 9600/12672]\n",
      "loss: 10.973026  [11200/12672]\n",
      "AUC: 0.5755328532135691\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 19.400068  [    0/12672]\n",
      "loss: 13.993604  [ 1600/12672]\n",
      "loss: 5.914932  [ 3200/12672]\n",
      "loss: 15.880405  [ 4800/12672]\n",
      "loss: 16.304123  [ 6400/12672]\n",
      "loss: 4.709900  [ 8000/12672]\n",
      "loss: 12.647182  [ 9600/12672]\n",
      "loss: 11.095409  [11200/12672]\n",
      "AUC: 0.5820890126097875\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 22.582571  [    0/12672]\n",
      "loss: 9.957154  [ 1600/12672]\n",
      "loss: 7.205410  [ 3200/12672]\n",
      "loss: 15.578811  [ 4800/12672]\n",
      "loss: 8.183404  [ 6400/12672]\n",
      "loss: 19.112202  [ 8000/12672]\n",
      "loss: 20.374636  [ 9600/12672]\n",
      "loss: 18.258764  [11200/12672]\n",
      "AUC: 0.5864179433688994\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.782632  [    0/12672]\n",
      "loss: 18.560524  [ 1600/12672]\n",
      "loss: 19.321522  [ 3200/12672]\n",
      "loss: 19.579638  [ 4800/12672]\n",
      "loss: 14.133487  [ 6400/12672]\n",
      "loss: 11.216288  [ 8000/12672]\n",
      "loss: 15.449846  [ 9600/12672]\n",
      "loss: 10.637396  [11200/12672]\n",
      "AUC: 0.6549885329329382\n",
      "sklearn AUC: 0.5266134478101486\n",
      "Done!\n",
      "##############################\n",
      "SESSION 7\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.730983  [    0/12672]\n",
      "loss: 11.408667  [ 1600/12672]\n",
      "loss: 9.203747  [ 3200/12672]\n",
      "loss: 5.712543  [ 4800/12672]\n",
      "loss: 22.639797  [ 6400/12672]\n",
      "loss: 11.034225  [ 8000/12672]\n",
      "loss: 12.702047  [ 9600/12672]\n",
      "loss: 14.475477  [11200/12672]\n",
      "AUC: 0.4873528704689494\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 24.290710  [    0/12672]\n",
      "loss: 9.074094  [ 1600/12672]\n",
      "loss: 13.736438  [ 3200/12672]\n",
      "loss: 22.530563  [ 4800/12672]\n",
      "loss: 8.163447  [ 6400/12672]\n",
      "loss: 10.380048  [ 8000/12672]\n",
      "loss: 15.973310  [ 9600/12672]\n",
      "loss: 15.402196  [11200/12672]\n",
      "AUC: 0.5764688912547851\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.936970  [    0/12672]\n",
      "loss: 17.273666  [ 1600/12672]\n",
      "loss: 22.968882  [ 3200/12672]\n",
      "loss: 14.950196  [ 4800/12672]\n",
      "loss: 5.161747  [ 6400/12672]\n",
      "loss: 8.741815  [ 8000/12672]\n",
      "loss: 26.023117  [ 9600/12672]\n",
      "loss: 6.798104  [11200/12672]\n",
      "AUC: 0.6179174272905235\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 4.771158  [    0/12672]\n",
      "loss: 27.928190  [ 1600/12672]\n",
      "loss: 17.273331  [ 3200/12672]\n",
      "loss: 12.631698  [ 4800/12672]\n",
      "loss: 15.458752  [ 6400/12672]\n",
      "loss: 17.398746  [ 8000/12672]\n",
      "loss: 19.282747  [ 9600/12672]\n",
      "loss: 24.451275  [11200/12672]\n",
      "AUC: 0.6153431063041739\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.313521  [    0/12672]\n",
      "loss: 13.768154  [ 1600/12672]\n",
      "loss: 10.526414  [ 3200/12672]\n",
      "loss: 13.696037  [ 4800/12672]\n",
      "loss: 17.103926  [ 6400/12672]\n",
      "loss: 11.101836  [ 8000/12672]\n",
      "loss: 16.219210  [ 9600/12672]\n",
      "loss: 11.225063  [11200/12672]\n",
      "AUC: 0.630317837300906\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 8.985841  [    0/12672]\n",
      "loss: 19.845959  [ 1600/12672]\n",
      "loss: 16.566893  [ 3200/12672]\n",
      "loss: 5.481979  [ 4800/12672]\n",
      "loss: 11.655946  [ 6400/12672]\n",
      "loss: 19.447865  [ 8000/12672]\n",
      "loss: 19.265589  [ 9600/12672]\n",
      "loss: 8.708639  [11200/12672]\n",
      "AUC: 0.5725356711674464\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.178394  [    0/12672]\n",
      "loss: 15.558893  [ 1600/12672]\n",
      "loss: 21.119795  [ 3200/12672]\n",
      "loss: 16.896975  [ 4800/12672]\n",
      "loss: 14.028307  [ 6400/12672]\n",
      "loss: 12.491866  [ 8000/12672]\n",
      "loss: 23.384083  [ 9600/12672]\n",
      "loss: 16.145750  [11200/12672]\n",
      "AUC: 0.6214409804320432\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.387446  [    0/12672]\n",
      "loss: 17.664715  [ 1600/12672]\n",
      "loss: 18.876837  [ 3200/12672]\n",
      "loss: 15.873753  [ 4800/12672]\n",
      "loss: 15.512942  [ 6400/12672]\n",
      "loss: 8.730056  [ 8000/12672]\n",
      "loss: 15.061324  [ 9600/12672]\n",
      "loss: 5.485956  [11200/12672]\n",
      "AUC: 0.576698663503528\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 28.110754  [    0/12672]\n",
      "loss: 11.725294  [ 1600/12672]\n",
      "loss: 15.934571  [ 3200/12672]\n",
      "loss: 15.394678  [ 4800/12672]\n",
      "loss: 8.979072  [ 6400/12672]\n",
      "loss: 6.485947  [ 8000/12672]\n",
      "loss: 18.098320  [ 9600/12672]\n",
      "loss: 20.603155  [11200/12672]\n",
      "AUC: 0.5461700750540913\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.618868  [    0/12672]\n",
      "loss: 9.729581  [ 1600/12672]\n",
      "loss: 17.295929  [ 3200/12672]\n",
      "loss: 15.030861  [ 4800/12672]\n",
      "loss: 6.511067  [ 6400/12672]\n",
      "loss: 11.635743  [ 8000/12672]\n",
      "loss: 18.416574  [ 9600/12672]\n",
      "loss: 12.519290  [11200/12672]\n",
      "AUC: 0.5803441894501581\n",
      "sklearn AUC: 0.5945863125638406\n",
      "Done!\n",
      "##############################\n",
      "SESSION 8\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.699427  [    0/12672]\n",
      "loss: 26.794331  [ 1600/12672]\n",
      "loss: 27.050941  [ 3200/12672]\n",
      "loss: 15.741442  [ 4800/12672]\n",
      "loss: 13.324316  [ 6400/12672]\n",
      "loss: 5.610389  [ 8000/12672]\n",
      "loss: 8.517386  [ 9600/12672]\n",
      "loss: 25.187674  [11200/12672]\n",
      "AUC: 0.5833611081749136\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 4.796586  [    0/12672]\n",
      "loss: 12.436770  [ 1600/12672]\n",
      "loss: 12.818495  [ 3200/12672]\n",
      "loss: 20.997282  [ 4800/12672]\n",
      "loss: 23.374495  [ 6400/12672]\n",
      "loss: 19.334913  [ 8000/12672]\n",
      "loss: 21.598909  [ 9600/12672]\n",
      "loss: 16.832016  [11200/12672]\n",
      "AUC: 0.5658908121776333\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.012219  [    0/12672]\n",
      "loss: 5.130245  [ 1600/12672]\n",
      "loss: 5.270789  [ 3200/12672]\n",
      "loss: 15.832432  [ 4800/12672]\n",
      "loss: 26.111485  [ 6400/12672]\n",
      "loss: 8.820922  [ 8000/12672]\n",
      "loss: 10.844215  [ 9600/12672]\n",
      "loss: 9.010416  [11200/12672]\n",
      "AUC: 0.5525124381031042\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.316682  [    0/12672]\n",
      "loss: 18.846024  [ 1600/12672]\n",
      "loss: 22.070503  [ 3200/12672]\n",
      "loss: 7.224919  [ 4800/12672]\n",
      "loss: 21.049278  [ 6400/12672]\n",
      "loss: 16.991402  [ 8000/12672]\n",
      "loss: 6.192137  [ 9600/12672]\n",
      "loss: 10.233068  [11200/12672]\n",
      "AUC: 0.5488136492237488\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.567781  [    0/12672]\n",
      "loss: 17.920795  [ 1600/12672]\n",
      "loss: 7.145936  [ 3200/12672]\n",
      "loss: 10.432762  [ 4800/12672]\n",
      "loss: 10.726642  [ 6400/12672]\n",
      "loss: 16.240030  [ 8000/12672]\n",
      "loss: 12.807950  [ 9600/12672]\n",
      "loss: 15.821495  [11200/12672]\n",
      "AUC: 0.6335029900013744\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 7.663378  [    0/12672]\n",
      "loss: 12.066004  [ 1600/12672]\n",
      "loss: 18.415510  [ 3200/12672]\n",
      "loss: 10.929230  [ 4800/12672]\n",
      "loss: 23.401024  [ 6400/12672]\n",
      "loss: 18.134083  [ 8000/12672]\n",
      "loss: 15.284388  [ 9600/12672]\n",
      "loss: 8.684454  [11200/12672]\n",
      "AUC: 0.5707403259697731\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.095409  [    0/12672]\n",
      "loss: 8.144356  [ 1600/12672]\n",
      "loss: 13.063616  [ 3200/12672]\n",
      "loss: 10.784899  [ 4800/12672]\n",
      "loss: 18.968391  [ 6400/12672]\n",
      "loss: 9.073371  [ 8000/12672]\n",
      "loss: 19.495340  [ 9600/12672]\n",
      "loss: 16.373096  [11200/12672]\n",
      "AUC: 0.6097781567853303\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 7.207883  [    0/12672]\n",
      "loss: 17.543434  [ 1600/12672]\n",
      "loss: 16.922586  [ 3200/12672]\n",
      "loss: 7.665846  [ 4800/12672]\n",
      "loss: 22.459496  [ 6400/12672]\n",
      "loss: 11.943058  [ 8000/12672]\n",
      "loss: 11.584685  [ 9600/12672]\n",
      "loss: 11.658813  [11200/12672]\n",
      "AUC: 0.5870267391838683\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 16.319639  [    0/12672]\n",
      "loss: 15.814374  [ 1600/12672]\n",
      "loss: 9.337164  [ 3200/12672]\n",
      "loss: 11.065622  [ 4800/12672]\n",
      "loss: 10.702702  [ 6400/12672]\n",
      "loss: 17.648809  [ 8000/12672]\n",
      "loss: 9.101930  [ 9600/12672]\n",
      "loss: 14.005950  [11200/12672]\n",
      "AUC: 0.5828806827722657\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.573914  [    0/12672]\n",
      "loss: 15.535492  [ 1600/12672]\n",
      "loss: 12.812706  [ 3200/12672]\n",
      "loss: 12.384785  [ 4800/12672]\n",
      "loss: 16.195555  [ 6400/12672]\n",
      "loss: 16.722179  [ 8000/12672]\n",
      "loss: 16.893114  [ 9600/12672]\n",
      "loss: 12.220022  [11200/12672]\n",
      "AUC: 0.6408543028096818\n",
      "sklearn AUC: 0.5866151545363908\n",
      "Done!\n",
      "##############################\n",
      "SESSION 9\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.741609  [    0/12672]\n",
      "loss: 6.437707  [ 1600/12672]\n",
      "loss: 15.786274  [ 3200/12672]\n",
      "loss: 20.650923  [ 4800/12672]\n",
      "loss: 25.043543  [ 6400/12672]\n",
      "loss: 14.129792  [ 8000/12672]\n",
      "loss: 2.017876  [ 9600/12672]\n",
      "loss: 14.506269  [11200/12672]\n",
      "AUC: 0.5596404219514064\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.489084  [    0/12672]\n",
      "loss: 10.101367  [ 1600/12672]\n",
      "loss: 19.336004  [ 3200/12672]\n",
      "loss: 5.553060  [ 4800/12672]\n",
      "loss: 15.099717  [ 6400/12672]\n",
      "loss: 10.325469  [ 8000/12672]\n",
      "loss: 25.779322  [ 9600/12672]\n",
      "loss: 17.571379  [11200/12672]\n",
      "AUC: 0.5448163803374659\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 19.552719  [    0/12672]\n",
      "loss: 5.237051  [ 1600/12672]\n",
      "loss: 11.366002  [ 3200/12672]\n",
      "loss: 13.206023  [ 4800/12672]\n",
      "loss: 25.888111  [ 6400/12672]\n",
      "loss: 22.819237  [ 8000/12672]\n",
      "loss: 10.648618  [ 9600/12672]\n",
      "loss: 16.372694  [11200/12672]\n",
      "AUC: 0.5697728167343412\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.027639  [    0/12672]\n",
      "loss: 8.320257  [ 1600/12672]\n",
      "loss: 13.616543  [ 3200/12672]\n",
      "loss: 15.769488  [ 4800/12672]\n",
      "loss: 8.919346  [ 6400/12672]\n",
      "loss: 16.179237  [ 8000/12672]\n",
      "loss: 6.495877  [ 9600/12672]\n",
      "loss: 6.362478  [11200/12672]\n",
      "AUC: 0.5953679658254234\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.910358  [    0/12672]\n",
      "loss: 6.548028  [ 1600/12672]\n",
      "loss: 23.421370  [ 3200/12672]\n",
      "loss: 21.218618  [ 4800/12672]\n",
      "loss: 10.745000  [ 6400/12672]\n",
      "loss: 16.219475  [ 8000/12672]\n",
      "loss: 21.485615  [ 9600/12672]\n",
      "loss: 13.575279  [11200/12672]\n",
      "AUC: 0.6446679100830324\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 6.135295  [    0/12672]\n",
      "loss: 18.224468  [ 1600/12672]\n",
      "loss: 14.960008  [ 3200/12672]\n",
      "loss: 8.636724  [ 4800/12672]\n",
      "loss: 25.452595  [ 6400/12672]\n",
      "loss: 21.355396  [ 8000/12672]\n",
      "loss: 19.398483  [ 9600/12672]\n",
      "loss: 17.107328  [11200/12672]\n",
      "AUC: 0.5384134288967497\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 22.425623  [    0/12672]\n",
      "loss: 5.094303  [ 1600/12672]\n",
      "loss: 22.415316  [ 3200/12672]\n",
      "loss: 11.005216  [ 4800/12672]\n",
      "loss: 12.769171  [ 6400/12672]\n",
      "loss: 17.158625  [ 8000/12672]\n",
      "loss: 9.004897  [ 9600/12672]\n",
      "loss: 10.715900  [11200/12672]\n",
      "AUC: 0.552296916732492\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 24.128996  [    0/12672]\n",
      "loss: 15.553130  [ 1600/12672]\n",
      "loss: 16.551306  [ 3200/12672]\n",
      "loss: 24.778648  [ 4800/12672]\n",
      "loss: 8.044316  [ 6400/12672]\n",
      "loss: 24.058998  [ 8000/12672]\n",
      "loss: 12.960175  [ 9600/12672]\n",
      "loss: 14.915596  [11200/12672]\n",
      "AUC: 0.6469209726122576\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.824552  [    0/12672]\n",
      "loss: 17.743021  [ 1600/12672]\n",
      "loss: 15.359730  [ 3200/12672]\n",
      "loss: 19.368650  [ 4800/12672]\n",
      "loss: 9.462295  [ 6400/12672]\n",
      "loss: 18.400719  [ 8000/12672]\n",
      "loss: 19.228453  [ 9600/12672]\n",
      "loss: 12.919958  [11200/12672]\n",
      "AUC: 0.6132634924325081\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.965210  [    0/12672]\n",
      "loss: 12.637832  [ 1600/12672]\n",
      "loss: 23.771177  [ 3200/12672]\n",
      "loss: 23.999584  [ 4800/12672]\n",
      "loss: 9.035501  [ 6400/12672]\n",
      "loss: 9.634811  [ 8000/12672]\n",
      "loss: 7.962169  [ 9600/12672]\n",
      "loss: 9.689489  [11200/12672]\n",
      "AUC: 0.604508476277782\n",
      "sklearn AUC: 0.5421475995914198\n",
      "Done!\n",
      "##############################\n",
      "SESSION 10\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.748003  [    0/12672]\n",
      "loss: 20.105310  [ 1600/12672]\n",
      "loss: 6.794889  [ 3200/12672]\n",
      "loss: 2.691418  [ 4800/12672]\n",
      "loss: 7.792188  [ 6400/12672]\n",
      "loss: 19.256191  [ 8000/12672]\n",
      "loss: 10.085819  [ 9600/12672]\n",
      "loss: 8.427179  [11200/12672]\n",
      "AUC: 0.5118001670127453\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.468869  [    0/12672]\n",
      "loss: 12.183510  [ 1600/12672]\n",
      "loss: 14.447410  [ 3200/12672]\n",
      "loss: 6.439718  [ 4800/12672]\n",
      "loss: 10.741225  [ 6400/12672]\n",
      "loss: 3.991885  [ 8000/12672]\n",
      "loss: 13.662029  [ 9600/12672]\n",
      "loss: 12.874258  [11200/12672]\n",
      "AUC: 0.5884748266116813\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 8.895940  [    0/12672]\n",
      "loss: 20.949026  [ 1600/12672]\n",
      "loss: 16.862011  [ 3200/12672]\n",
      "loss: 24.933613  [ 4800/12672]\n",
      "loss: 8.152390  [ 6400/12672]\n",
      "loss: 19.853691  [ 8000/12672]\n",
      "loss: 5.959798  [ 9600/12672]\n",
      "loss: 12.001484  [11200/12672]\n",
      "AUC: 0.5603290915862804\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 16.579851  [    0/12672]\n",
      "loss: 4.829428  [ 1600/12672]\n",
      "loss: 15.318978  [ 3200/12672]\n",
      "loss: 13.800672  [ 4800/12672]\n",
      "loss: 9.696162  [ 6400/12672]\n",
      "loss: 11.639353  [ 8000/12672]\n",
      "loss: 20.124260  [ 9600/12672]\n",
      "loss: 3.883030  [11200/12672]\n",
      "AUC: 0.6085380449968819\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 6.379859  [    0/12672]\n",
      "loss: 19.332026  [ 1600/12672]\n",
      "loss: 20.015327  [ 3200/12672]\n",
      "loss: 14.225347  [ 4800/12672]\n",
      "loss: 8.056193  [ 6400/12672]\n",
      "loss: 11.259387  [ 8000/12672]\n",
      "loss: 10.250655  [ 9600/12672]\n",
      "loss: 10.300326  [11200/12672]\n",
      "AUC: 0.5834272726220984\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 16.391180  [    0/12672]\n",
      "loss: 17.031448  [ 1600/12672]\n",
      "loss: 18.801611  [ 3200/12672]\n",
      "loss: 19.677046  [ 4800/12672]\n",
      "loss: 5.993849  [ 6400/12672]\n",
      "loss: 11.852325  [ 8000/12672]\n",
      "loss: 27.174633  [ 9600/12672]\n",
      "loss: 16.235411  [11200/12672]\n",
      "AUC: 0.563487243727581\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.446526  [    0/12672]\n",
      "loss: 20.132158  [ 1600/12672]\n",
      "loss: 10.474835  [ 3200/12672]\n",
      "loss: 17.295174  [ 4800/12672]\n",
      "loss: 9.811358  [ 6400/12672]\n",
      "loss: 20.245914  [ 8000/12672]\n",
      "loss: 18.713646  [ 9600/12672]\n",
      "loss: 18.898777  [11200/12672]\n",
      "AUC: 0.5594705600571892\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 19.012640  [    0/12672]\n",
      "loss: 8.237240  [ 1600/12672]\n",
      "loss: 13.450026  [ 3200/12672]\n",
      "loss: 11.197494  [ 4800/12672]\n",
      "loss: 15.244925  [ 6400/12672]\n",
      "loss: 5.502294  [ 8000/12672]\n",
      "loss: 21.353832  [ 9600/12672]\n",
      "loss: 17.141649  [11200/12672]\n",
      "AUC: 0.6602780922703384\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 7.668980  [    0/12672]\n",
      "loss: 5.007180  [ 1600/12672]\n",
      "loss: 20.028877  [ 3200/12672]\n",
      "loss: 8.498558  [ 4800/12672]\n",
      "loss: 16.627356  [ 6400/12672]\n",
      "loss: 10.808450  [ 8000/12672]\n",
      "loss: 18.306202  [ 9600/12672]\n",
      "loss: 7.957561  [11200/12672]\n",
      "AUC: 0.5640993303915259\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.715775  [    0/12672]\n",
      "loss: 21.100204  [ 1600/12672]\n",
      "loss: 6.498729  [ 3200/12672]\n",
      "loss: 8.147429  [ 4800/12672]\n",
      "loss: 23.678589  [ 6400/12672]\n",
      "loss: 8.228806  [ 8000/12672]\n",
      "loss: 12.139259  [ 9600/12672]\n",
      "loss: 9.644828  [11200/12672]\n",
      "AUC: 0.5570075081418515\n",
      "sklearn AUC: 0.589434395635946\n",
      "Done!\n",
      "##############################\n",
      "SESSION 11\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.723848  [    0/12672]\n",
      "loss: 13.546357  [ 1600/12672]\n",
      "loss: 13.738575  [ 3200/12672]\n",
      "loss: 23.974306  [ 4800/12672]\n",
      "loss: 25.367256  [ 6400/12672]\n",
      "loss: 24.793364  [ 8000/12672]\n",
      "loss: 7.570745  [ 9600/12672]\n",
      "loss: 24.998623  [11200/12672]\n",
      "AUC: 0.5390605186811962\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.389927  [    0/12672]\n",
      "loss: 17.987188  [ 1600/12672]\n",
      "loss: 23.504162  [ 3200/12672]\n",
      "loss: 15.717345  [ 4800/12672]\n",
      "loss: 23.984983  [ 6400/12672]\n",
      "loss: 19.818813  [ 8000/12672]\n",
      "loss: 10.758551  [ 9600/12672]\n",
      "loss: 18.437262  [11200/12672]\n",
      "AUC: 0.5682547128442254\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 16.556120  [    0/12672]\n",
      "loss: 14.256536  [ 1600/12672]\n",
      "loss: 16.412714  [ 3200/12672]\n",
      "loss: 12.513978  [ 4800/12672]\n",
      "loss: 28.564976  [ 6400/12672]\n",
      "loss: 19.875353  [ 8000/12672]\n",
      "loss: 24.142017  [ 9600/12672]\n",
      "loss: 15.361609  [11200/12672]\n",
      "AUC: 0.6034747678234064\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.403384  [    0/12672]\n",
      "loss: 18.741192  [ 1600/12672]\n",
      "loss: 18.191442  [ 3200/12672]\n",
      "loss: 4.360630  [ 4800/12672]\n",
      "loss: 17.182907  [ 6400/12672]\n",
      "loss: 19.423782  [ 8000/12672]\n",
      "loss: 11.432384  [ 9600/12672]\n",
      "loss: 18.435291  [11200/12672]\n",
      "AUC: 0.5394506282332352\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.334989  [    0/12672]\n",
      "loss: 14.753114  [ 1600/12672]\n",
      "loss: 19.538769  [ 3200/12672]\n",
      "loss: 10.017764  [ 4800/12672]\n",
      "loss: 12.146071  [ 6400/12672]\n",
      "loss: 15.669550  [ 8000/12672]\n",
      "loss: 11.658184  [ 9600/12672]\n",
      "loss: 8.555348  [11200/12672]\n",
      "AUC: 0.6099887438005416\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.294205  [    0/12672]\n",
      "loss: 16.231236  [ 1600/12672]\n",
      "loss: 5.741362  [ 3200/12672]\n",
      "loss: 12.239128  [ 4800/12672]\n",
      "loss: 22.727192  [ 6400/12672]\n",
      "loss: 13.919248  [ 8000/12672]\n",
      "loss: 14.049460  [ 9600/12672]\n",
      "loss: 26.725483  [11200/12672]\n",
      "AUC: 0.550162922286895\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.490410  [    0/12672]\n",
      "loss: 7.875259  [ 1600/12672]\n",
      "loss: 28.941706  [ 3200/12672]\n",
      "loss: 22.615992  [ 4800/12672]\n",
      "loss: 14.134048  [ 6400/12672]\n",
      "loss: 11.211079  [ 8000/12672]\n",
      "loss: 13.203217  [ 9600/12672]\n",
      "loss: 14.760903  [11200/12672]\n",
      "AUC: 0.5688229976223251\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 16.772701  [    0/12672]\n",
      "loss: 7.515588  [ 1600/12672]\n",
      "loss: 6.477191  [ 3200/12672]\n",
      "loss: 22.490711  [ 4800/12672]\n",
      "loss: 13.742382  [ 6400/12672]\n",
      "loss: 13.891301  [ 8000/12672]\n",
      "loss: 13.413137  [ 9600/12672]\n",
      "loss: 12.763073  [11200/12672]\n",
      "AUC: 0.5784901843471125\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 24.357023  [    0/12672]\n",
      "loss: 18.898598  [ 1600/12672]\n",
      "loss: 13.932483  [ 3200/12672]\n",
      "loss: 13.348150  [ 4800/12672]\n",
      "loss: 11.274212  [ 6400/12672]\n",
      "loss: 6.229387  [ 8000/12672]\n",
      "loss: 18.716717  [ 9600/12672]\n",
      "loss: 13.751573  [11200/12672]\n",
      "AUC: 0.5560929674161759\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 16.113546  [    0/12672]\n",
      "loss: 8.630154  [ 1600/12672]\n",
      "loss: 16.347811  [ 3200/12672]\n",
      "loss: 8.820858  [ 4800/12672]\n",
      "loss: 20.942688  [ 6400/12672]\n",
      "loss: 16.357946  [ 8000/12672]\n",
      "loss: 17.717297  [ 9600/12672]\n",
      "loss: 9.436784  [11200/12672]\n",
      "AUC: 0.6557657930583798\n",
      "sklearn AUC: 0.6383749736453721\n",
      "Done!\n",
      "##############################\n",
      "SESSION 12\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.725661  [    0/12672]\n",
      "loss: 23.212309  [ 1600/12672]\n",
      "loss: 26.409330  [ 3200/12672]\n",
      "loss: 13.579319  [ 4800/12672]\n",
      "loss: 22.492569  [ 6400/12672]\n",
      "loss: 14.637273  [ 8000/12672]\n",
      "loss: 15.224468  [ 9600/12672]\n",
      "loss: 27.553827  [11200/12672]\n",
      "AUC: 0.5307208126392535\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.043546  [    0/12672]\n",
      "loss: 30.999207  [ 1600/12672]\n",
      "loss: 6.294425  [ 3200/12672]\n",
      "loss: 17.697224  [ 4800/12672]\n",
      "loss: 5.325332  [ 6400/12672]\n",
      "loss: 4.759689  [ 8000/12672]\n",
      "loss: 15.863871  [ 9600/12672]\n",
      "loss: 17.648727  [11200/12672]\n",
      "AUC: 0.6224178184840512\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 6.224808  [    0/12672]\n",
      "loss: 21.558910  [ 1600/12672]\n",
      "loss: 12.651443  [ 3200/12672]\n",
      "loss: 12.331152  [ 4800/12672]\n",
      "loss: 17.746576  [ 6400/12672]\n",
      "loss: 14.327782  [ 8000/12672]\n",
      "loss: 23.545252  [ 9600/12672]\n",
      "loss: 18.872873  [11200/12672]\n",
      "AUC: 0.5553199169619043\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 20.325861  [    0/12672]\n",
      "loss: 21.008904  [ 1600/12672]\n",
      "loss: 16.911694  [ 3200/12672]\n",
      "loss: 5.500634  [ 4800/12672]\n",
      "loss: 16.594250  [ 6400/12672]\n",
      "loss: 17.030714  [ 8000/12672]\n",
      "loss: 17.546764  [ 9600/12672]\n",
      "loss: 8.350959  [11200/12672]\n",
      "AUC: 0.6126443518937221\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.483021  [    0/12672]\n",
      "loss: 12.445712  [ 1600/12672]\n",
      "loss: 18.206396  [ 3200/12672]\n",
      "loss: 14.809958  [ 4800/12672]\n",
      "loss: 21.331480  [ 6400/12672]\n",
      "loss: 19.793255  [ 8000/12672]\n",
      "loss: 21.200788  [ 9600/12672]\n",
      "loss: 11.745696  [11200/12672]\n",
      "AUC: 0.5393994220224275\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.643190  [    0/12672]\n",
      "loss: 14.219326  [ 1600/12672]\n",
      "loss: 16.017126  [ 3200/12672]\n",
      "loss: 17.827387  [ 4800/12672]\n",
      "loss: 15.331871  [ 6400/12672]\n",
      "loss: 15.285255  [ 8000/12672]\n",
      "loss: 21.655499  [ 9600/12672]\n",
      "loss: 16.382332  [11200/12672]\n",
      "AUC: 0.6212202561527019\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.641533  [    0/12672]\n",
      "loss: 13.445116  [ 1600/12672]\n",
      "loss: 15.254999  [ 3200/12672]\n",
      "loss: 15.640520  [ 4800/12672]\n",
      "loss: 21.396406  [ 6400/12672]\n",
      "loss: 23.215218  [ 8000/12672]\n",
      "loss: 22.424343  [ 9600/12672]\n",
      "loss: 15.421082  [11200/12672]\n",
      "AUC: 0.6049165037951285\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.452389  [    0/12672]\n",
      "loss: 15.106262  [ 1600/12672]\n",
      "loss: 12.252976  [ 3200/12672]\n",
      "loss: 19.219048  [ 4800/12672]\n",
      "loss: 6.365993  [ 6400/12672]\n",
      "loss: 7.481085  [ 8000/12672]\n",
      "loss: 6.375320  [ 9600/12672]\n",
      "loss: 15.288064  [11200/12672]\n",
      "AUC: 0.6085371233107488\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 7.836332  [    0/12672]\n",
      "loss: 7.634324  [ 1600/12672]\n",
      "loss: 18.044527  [ 3200/12672]\n",
      "loss: 10.706826  [ 4800/12672]\n",
      "loss: 13.109821  [ 6400/12672]\n",
      "loss: 15.657162  [ 8000/12672]\n",
      "loss: 15.218387  [ 9600/12672]\n",
      "loss: 18.820322  [11200/12672]\n",
      "AUC: 0.5670678447039411\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.353539  [    0/12672]\n",
      "loss: 11.208879  [ 1600/12672]\n",
      "loss: 15.343733  [ 3200/12672]\n",
      "loss: 20.785385  [ 4800/12672]\n",
      "loss: 9.785069  [ 6400/12672]\n",
      "loss: 14.361026  [ 8000/12672]\n",
      "loss: 17.382648  [ 9600/12672]\n",
      "loss: 17.609987  [11200/12672]\n",
      "AUC: 0.5241282625501861\n",
      "sklearn AUC: 0.5950669115717659\n",
      "Done!\n",
      "##############################\n",
      "SESSION 13\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.731578  [    0/12672]\n",
      "loss: 10.547381  [ 1600/12672]\n",
      "loss: 15.393559  [ 3200/12672]\n",
      "loss: 13.782981  [ 4800/12672]\n",
      "loss: 20.212883  [ 6400/12672]\n",
      "loss: 8.785830  [ 8000/12672]\n",
      "loss: 23.929811  [ 9600/12672]\n",
      "loss: 5.985617  [11200/12672]\n",
      "AUC: 0.5374076643502825\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 23.716089  [    0/12672]\n",
      "loss: 23.940313  [ 1600/12672]\n",
      "loss: 10.683292  [ 3200/12672]\n",
      "loss: 21.451303  [ 4800/12672]\n",
      "loss: 16.626358  [ 6400/12672]\n",
      "loss: 25.511362  [ 8000/12672]\n",
      "loss: 10.224868  [ 9600/12672]\n",
      "loss: 12.915930  [11200/12672]\n",
      "AUC: 0.5256518033020756\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 28.970335  [    0/12672]\n",
      "loss: 21.620152  [ 1600/12672]\n",
      "loss: 23.318710  [ 3200/12672]\n",
      "loss: 15.396038  [ 4800/12672]\n",
      "loss: 10.757993  [ 6400/12672]\n",
      "loss: 20.420197  [ 8000/12672]\n",
      "loss: 8.076278  [ 9600/12672]\n",
      "loss: 20.970510  [11200/12672]\n",
      "AUC: 0.5242542668637915\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.210405  [    0/12672]\n",
      "loss: 26.513287  [ 1600/12672]\n",
      "loss: 9.723071  [ 3200/12672]\n",
      "loss: 23.979666  [ 4800/12672]\n",
      "loss: 19.050631  [ 6400/12672]\n",
      "loss: 10.891618  [ 8000/12672]\n",
      "loss: 9.938545  [ 9600/12672]\n",
      "loss: 10.459746  [11200/12672]\n",
      "AUC: 0.5498552516189931\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 19.669178  [    0/12672]\n",
      "loss: 7.732504  [ 1600/12672]\n",
      "loss: 18.522860  [ 3200/12672]\n",
      "loss: 11.769667  [ 4800/12672]\n",
      "loss: 5.978258  [ 6400/12672]\n",
      "loss: 8.829080  [ 8000/12672]\n",
      "loss: 18.897930  [ 9600/12672]\n",
      "loss: 7.846152  [11200/12672]\n",
      "AUC: 0.5267510018711521\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 28.802368  [    0/12672]\n",
      "loss: 10.667776  [ 1600/12672]\n",
      "loss: 4.841714  [ 3200/12672]\n",
      "loss: 15.328748  [ 4800/12672]\n",
      "loss: 14.578509  [ 6400/12672]\n",
      "loss: 14.156299  [ 8000/12672]\n",
      "loss: 8.468318  [ 9600/12672]\n",
      "loss: 15.555068  [11200/12672]\n",
      "AUC: 0.5948194226954997\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.584192  [    0/12672]\n",
      "loss: 12.361427  [ 1600/12672]\n",
      "loss: 14.572391  [ 3200/12672]\n",
      "loss: 10.026151  [ 4800/12672]\n",
      "loss: 19.336845  [ 6400/12672]\n",
      "loss: 8.213459  [ 8000/12672]\n",
      "loss: 17.213528  [ 9600/12672]\n",
      "loss: 23.033028  [11200/12672]\n",
      "AUC: 0.5906398810069866\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.737919  [    0/12672]\n",
      "loss: 17.439863  [ 1600/12672]\n",
      "loss: 6.119363  [ 3200/12672]\n",
      "loss: 11.280373  [ 4800/12672]\n",
      "loss: 15.408303  [ 6400/12672]\n",
      "loss: 13.524254  [ 8000/12672]\n",
      "loss: 11.041612  [ 9600/12672]\n",
      "loss: 16.562929  [11200/12672]\n",
      "AUC: 0.5747012179634932\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.686993  [    0/12672]\n",
      "loss: 16.646988  [ 1600/12672]\n",
      "loss: 12.081640  [ 3200/12672]\n",
      "loss: 13.189969  [ 4800/12672]\n",
      "loss: 11.359114  [ 6400/12672]\n",
      "loss: 11.133898  [ 8000/12672]\n",
      "loss: 13.577725  [ 9600/12672]\n",
      "loss: 8.528421  [11200/12672]\n",
      "AUC: 0.5528994013733575\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.666149  [    0/12672]\n",
      "loss: 24.889572  [ 1600/12672]\n",
      "loss: 6.870800  [ 3200/12672]\n",
      "loss: 11.141799  [ 4800/12672]\n",
      "loss: 10.929980  [ 6400/12672]\n",
      "loss: 14.428798  [ 8000/12672]\n",
      "loss: 11.222881  [ 9600/12672]\n",
      "loss: 7.065122  [11200/12672]\n",
      "AUC: 0.594745847999324\n",
      "sklearn AUC: 0.6029994156982406\n",
      "Done!\n",
      "##############################\n",
      "SESSION 14\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.690221  [    0/12672]\n",
      "loss: 18.854216  [ 1600/12672]\n",
      "loss: 6.602238  [ 3200/12672]\n",
      "loss: 10.286724  [ 4800/12672]\n",
      "loss: 8.496753  [ 6400/12672]\n",
      "loss: 14.712668  [ 8000/12672]\n",
      "loss: 9.746068  [ 9600/12672]\n",
      "loss: 11.103184  [11200/12672]\n",
      "AUC: 0.5548236036964455\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.354965  [    0/12672]\n",
      "loss: 24.626621  [ 1600/12672]\n",
      "loss: 24.343985  [ 3200/12672]\n",
      "loss: 14.963000  [ 4800/12672]\n",
      "loss: 8.159698  [ 6400/12672]\n",
      "loss: 29.535500  [ 8000/12672]\n",
      "loss: 20.506531  [ 9600/12672]\n",
      "loss: 7.944493  [11200/12672]\n",
      "AUC: 0.5770099184644885\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 6.541085  [    0/12672]\n",
      "loss: 6.328579  [ 1600/12672]\n",
      "loss: 9.865671  [ 3200/12672]\n",
      "loss: 14.581257  [ 4800/12672]\n",
      "loss: 18.339270  [ 6400/12672]\n",
      "loss: 17.992619  [ 8000/12672]\n",
      "loss: 13.264608  [ 9600/12672]\n",
      "loss: 22.777075  [11200/12672]\n",
      "AUC: 0.57653802984356\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.481651  [    0/12672]\n",
      "loss: 22.088568  [ 1600/12672]\n",
      "loss: 18.226126  [ 3200/12672]\n",
      "loss: 10.125577  [ 4800/12672]\n",
      "loss: 6.156887  [ 6400/12672]\n",
      "loss: 15.211320  [ 8000/12672]\n",
      "loss: 20.918734  [ 9600/12672]\n",
      "loss: 19.867565  [11200/12672]\n",
      "AUC: 0.5499003243681578\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 22.157381  [    0/12672]\n",
      "loss: 13.401253  [ 1600/12672]\n",
      "loss: 18.068375  [ 3200/12672]\n",
      "loss: 8.616406  [ 4800/12672]\n",
      "loss: 24.297609  [ 6400/12672]\n",
      "loss: 23.580271  [ 8000/12672]\n",
      "loss: 24.697857  [ 9600/12672]\n",
      "loss: 16.627306  [11200/12672]\n",
      "AUC: 0.6070647083854237\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.673397  [    0/12672]\n",
      "loss: 19.007708  [ 1600/12672]\n",
      "loss: 15.923985  [ 3200/12672]\n",
      "loss: 7.277585  [ 4800/12672]\n",
      "loss: 17.044563  [ 6400/12672]\n",
      "loss: 6.475521  [ 8000/12672]\n",
      "loss: 17.035002  [ 9600/12672]\n",
      "loss: 6.213050  [11200/12672]\n",
      "AUC: 0.6042220522832478\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.738195  [    0/12672]\n",
      "loss: 14.263115  [ 1600/12672]\n",
      "loss: 11.415241  [ 3200/12672]\n",
      "loss: 10.317801  [ 4800/12672]\n",
      "loss: 21.620127  [ 6400/12672]\n",
      "loss: 22.027563  [ 8000/12672]\n",
      "loss: 12.052627  [ 9600/12672]\n",
      "loss: 3.850566  [11200/12672]\n",
      "AUC: 0.5725053243048047\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.472361  [    0/12672]\n",
      "loss: 9.110947  [ 1600/12672]\n",
      "loss: 18.612852  [ 3200/12672]\n",
      "loss: 20.804541  [ 4800/12672]\n",
      "loss: 4.505875  [ 6400/12672]\n",
      "loss: 12.334153  [ 8000/12672]\n",
      "loss: 9.813238  [ 9600/12672]\n",
      "loss: 19.211981  [11200/12672]\n",
      "AUC: 0.6093714099878362\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.555646  [    0/12672]\n",
      "loss: 18.025475  [ 1600/12672]\n",
      "loss: 9.565141  [ 3200/12672]\n",
      "loss: 21.542526  [ 4800/12672]\n",
      "loss: 14.964809  [ 6400/12672]\n",
      "loss: 22.153669  [ 8000/12672]\n",
      "loss: 12.178234  [ 9600/12672]\n",
      "loss: 13.415990  [11200/12672]\n",
      "AUC: 0.5942922634181985\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.889992  [    0/12672]\n",
      "loss: 12.764138  [ 1600/12672]\n",
      "loss: 12.314181  [ 3200/12672]\n",
      "loss: 10.919102  [ 4800/12672]\n",
      "loss: 15.307146  [ 6400/12672]\n",
      "loss: 21.851038  [ 8000/12672]\n",
      "loss: 20.675346  [ 9600/12672]\n",
      "loss: 7.339979  [11200/12672]\n",
      "AUC: 0.5945430099295512\n",
      "sklearn AUC: 0.5934686442883164\n",
      "Done!\n",
      "##############################\n",
      "SESSION 15\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.726846  [    0/12672]\n",
      "loss: 25.401735  [ 1600/12672]\n",
      "loss: 3.083575  [ 3200/12672]\n",
      "loss: 11.736698  [ 4800/12672]\n",
      "loss: 10.962615  [ 6400/12672]\n",
      "loss: 8.385248  [ 8000/12672]\n",
      "loss: 16.124723  [ 9600/12672]\n",
      "loss: 10.988921  [11200/12672]\n",
      "AUC: 0.543899111896411\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 19.268461  [    0/12672]\n",
      "loss: 14.746660  [ 1600/12672]\n",
      "loss: 15.911724  [ 3200/12672]\n",
      "loss: 23.804970  [ 4800/12672]\n",
      "loss: 13.854095  [ 6400/12672]\n",
      "loss: 25.731163  [ 8000/12672]\n",
      "loss: 6.439664  [ 9600/12672]\n",
      "loss: 7.343188  [11200/12672]\n",
      "AUC: 0.6064078683997588\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 5.903854  [    0/12672]\n",
      "loss: 27.525711  [ 1600/12672]\n",
      "loss: 23.373989  [ 3200/12672]\n",
      "loss: 3.922321  [ 4800/12672]\n",
      "loss: 8.902591  [ 6400/12672]\n",
      "loss: 7.188335  [ 8000/12672]\n",
      "loss: 8.240084  [ 9600/12672]\n",
      "loss: 8.088897  [11200/12672]\n",
      "AUC: 0.5587646978394889\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.590509  [    0/12672]\n",
      "loss: 23.168011  [ 1600/12672]\n",
      "loss: 11.030804  [ 3200/12672]\n",
      "loss: 16.413288  [ 4800/12672]\n",
      "loss: 8.710851  [ 6400/12672]\n",
      "loss: 10.957684  [ 8000/12672]\n",
      "loss: 14.583182  [ 9600/12672]\n",
      "loss: 9.858164  [11200/12672]\n",
      "AUC: 0.5056221728838665\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 22.065071  [    0/12672]\n",
      "loss: 23.096584  [ 1600/12672]\n",
      "loss: 15.854267  [ 3200/12672]\n",
      "loss: 11.162869  [ 4800/12672]\n",
      "loss: 11.079398  [ 6400/12672]\n",
      "loss: 17.934114  [ 8000/12672]\n",
      "loss: 17.434784  [ 9600/12672]\n",
      "loss: 14.107584  [11200/12672]\n",
      "AUC: 0.5172507170345091\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 25.240654  [    0/12672]\n",
      "loss: 19.465963  [ 1600/12672]\n",
      "loss: 21.468193  [ 3200/12672]\n",
      "loss: 6.182639  [ 4800/12672]\n",
      "loss: 14.136052  [ 6400/12672]\n",
      "loss: 10.126191  [ 8000/12672]\n",
      "loss: 23.943043  [ 9600/12672]\n",
      "loss: 22.593296  [11200/12672]\n",
      "AUC: 0.5827259191048741\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.985387  [    0/12672]\n",
      "loss: 19.441507  [ 1600/12672]\n",
      "loss: 22.387363  [ 3200/12672]\n",
      "loss: 11.976575  [ 4800/12672]\n",
      "loss: 8.116262  [ 6400/12672]\n",
      "loss: 10.349859  [ 8000/12672]\n",
      "loss: 11.355695  [ 9600/12672]\n",
      "loss: 14.449654  [11200/12672]\n",
      "AUC: 0.5442756629660264\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.826797  [    0/12672]\n",
      "loss: 9.603263  [ 1600/12672]\n",
      "loss: 18.431648  [ 3200/12672]\n",
      "loss: 16.024769  [ 4800/12672]\n",
      "loss: 12.937621  [ 6400/12672]\n",
      "loss: 17.096092  [ 8000/12672]\n",
      "loss: 19.475451  [ 9600/12672]\n",
      "loss: 10.459816  [11200/12672]\n",
      "AUC: 0.5645511165538886\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.899166  [    0/12672]\n",
      "loss: 10.492885  [ 1600/12672]\n",
      "loss: 14.456100  [ 3200/12672]\n",
      "loss: 14.741863  [ 4800/12672]\n",
      "loss: 15.479671  [ 6400/12672]\n",
      "loss: 18.970810  [ 8000/12672]\n",
      "loss: 18.349745  [ 9600/12672]\n",
      "loss: 10.888836  [11200/12672]\n",
      "AUC: 0.5509352070833108\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 20.013445  [    0/12672]\n",
      "loss: 8.963397  [ 1600/12672]\n",
      "loss: 23.083307  [ 3200/12672]\n",
      "loss: 19.620319  [ 4800/12672]\n",
      "loss: 17.242638  [ 6400/12672]\n",
      "loss: 16.678513  [ 8000/12672]\n",
      "loss: 14.967408  [ 9600/12672]\n",
      "loss: 12.287814  [11200/12672]\n",
      "AUC: 0.6679787082975854\n",
      "sklearn AUC: 0.6175347896548492\n",
      "Done!\n",
      "##############################\n",
      "SESSION 16\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.742520  [    0/12672]\n",
      "loss: 26.305758  [ 1600/12672]\n",
      "loss: 24.627169  [ 3200/12672]\n",
      "loss: 22.160728  [ 4800/12672]\n",
      "loss: 6.405270  [ 6400/12672]\n",
      "loss: 14.653537  [ 8000/12672]\n",
      "loss: 11.108584  [ 9600/12672]\n",
      "loss: 10.039407  [11200/12672]\n",
      "AUC: 0.550965015894037\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.771686  [    0/12672]\n",
      "loss: 12.509565  [ 1600/12672]\n",
      "loss: 23.036421  [ 3200/12672]\n",
      "loss: 18.525509  [ 4800/12672]\n",
      "loss: 27.141674  [ 6400/12672]\n",
      "loss: 12.449482  [ 8000/12672]\n",
      "loss: 7.509860  [ 9600/12672]\n",
      "loss: 4.922252  [11200/12672]\n",
      "AUC: 0.6044704282049556\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.058035  [    0/12672]\n",
      "loss: 21.945999  [ 1600/12672]\n",
      "loss: 17.148497  [ 3200/12672]\n",
      "loss: 17.605066  [ 4800/12672]\n",
      "loss: 11.878587  [ 6400/12672]\n",
      "loss: 13.382899  [ 8000/12672]\n",
      "loss: 18.518007  [ 9600/12672]\n",
      "loss: 7.330307  [11200/12672]\n",
      "AUC: 0.5591187937213753\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.023896  [    0/12672]\n",
      "loss: 23.289301  [ 1600/12672]\n",
      "loss: 11.224774  [ 3200/12672]\n",
      "loss: 9.240203  [ 4800/12672]\n",
      "loss: 5.991430  [ 6400/12672]\n",
      "loss: 20.742826  [ 8000/12672]\n",
      "loss: 12.493839  [ 9600/12672]\n",
      "loss: 11.371565  [11200/12672]\n",
      "AUC: 0.5690865647644381\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 16.997797  [    0/12672]\n",
      "loss: 9.282573  [ 1600/12672]\n",
      "loss: 18.622442  [ 3200/12672]\n",
      "loss: 6.129910  [ 4800/12672]\n",
      "loss: 29.837841  [ 6400/12672]\n",
      "loss: 7.064631  [ 8000/12672]\n",
      "loss: 17.381012  [ 9600/12672]\n",
      "loss: 21.811682  [11200/12672]\n",
      "AUC: 0.58591958481373\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.209187  [    0/12672]\n",
      "loss: 9.620127  [ 1600/12672]\n",
      "loss: 13.397108  [ 3200/12672]\n",
      "loss: 17.885771  [ 4800/12672]\n",
      "loss: 20.829048  [ 6400/12672]\n",
      "loss: 19.913660  [ 8000/12672]\n",
      "loss: 20.829662  [ 9600/12672]\n",
      "loss: 23.952044  [11200/12672]\n",
      "AUC: 0.5840409472175538\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 19.450933  [    0/12672]\n",
      "loss: 10.616345  [ 1600/12672]\n",
      "loss: 19.555618  [ 3200/12672]\n",
      "loss: 5.662347  [ 4800/12672]\n",
      "loss: 11.169668  [ 6400/12672]\n",
      "loss: 21.842846  [ 8000/12672]\n",
      "loss: 12.158241  [ 9600/12672]\n",
      "loss: 8.637652  [11200/12672]\n",
      "AUC: 0.5035723756718953\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 21.067978  [    0/12672]\n",
      "loss: 8.144690  [ 1600/12672]\n",
      "loss: 7.602041  [ 3200/12672]\n",
      "loss: 20.038752  [ 4800/12672]\n",
      "loss: 14.130633  [ 6400/12672]\n",
      "loss: 21.133223  [ 8000/12672]\n",
      "loss: 14.757772  [ 9600/12672]\n",
      "loss: 11.853228  [11200/12672]\n",
      "AUC: 0.6193273239299222\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.121220  [    0/12672]\n",
      "loss: 9.948357  [ 1600/12672]\n",
      "loss: 14.067739  [ 3200/12672]\n",
      "loss: 16.383335  [ 4800/12672]\n",
      "loss: 13.661660  [ 6400/12672]\n",
      "loss: 11.882517  [ 8000/12672]\n",
      "loss: 21.115826  [ 9600/12672]\n",
      "loss: 7.670186  [11200/12672]\n",
      "AUC: 0.5343713341243765\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 25.138954  [    0/12672]\n",
      "loss: 8.803300  [ 1600/12672]\n",
      "loss: 16.438641  [ 3200/12672]\n",
      "loss: 9.472639  [ 4800/12672]\n",
      "loss: 9.018229  [ 6400/12672]\n",
      "loss: 16.288738  [ 8000/12672]\n",
      "loss: 16.717049  [ 9600/12672]\n",
      "loss: 18.677431  [11200/12672]\n",
      "AUC: 0.625649212490806\n",
      "sklearn AUC: 0.5797328484509408\n",
      "Done!\n",
      "##############################\n",
      "SESSION 17\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.747965  [    0/12672]\n",
      "loss: 3.424525  [ 1600/12672]\n",
      "loss: 8.091778  [ 3200/12672]\n",
      "loss: 22.404522  [ 4800/12672]\n",
      "loss: 22.918373  [ 6400/12672]\n",
      "loss: 17.116188  [ 8000/12672]\n",
      "loss: 25.469202  [ 9600/12672]\n",
      "loss: 6.653963  [11200/12672]\n",
      "AUC: 0.5355156818465755\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 33.099499  [    0/12672]\n",
      "loss: 26.750654  [ 1600/12672]\n",
      "loss: 6.229357  [ 3200/12672]\n",
      "loss: 11.933768  [ 4800/12672]\n",
      "loss: 16.272322  [ 6400/12672]\n",
      "loss: 17.216591  [ 8000/12672]\n",
      "loss: 22.139727  [ 9600/12672]\n",
      "loss: 17.421572  [11200/12672]\n",
      "AUC: 0.5751944206778568\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 19.584164  [    0/12672]\n",
      "loss: 9.479648  [ 1600/12672]\n",
      "loss: 20.274935  [ 3200/12672]\n",
      "loss: 21.918171  [ 4800/12672]\n",
      "loss: 16.966238  [ 6400/12672]\n",
      "loss: 18.876375  [ 8000/12672]\n",
      "loss: 16.771227  [ 9600/12672]\n",
      "loss: 17.242714  [11200/12672]\n",
      "AUC: 0.5677603293165697\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.498037  [    0/12672]\n",
      "loss: 15.552201  [ 1600/12672]\n",
      "loss: 13.123116  [ 3200/12672]\n",
      "loss: 22.083271  [ 4800/12672]\n",
      "loss: 11.173553  [ 6400/12672]\n",
      "loss: 27.749199  [ 8000/12672]\n",
      "loss: 17.280222  [ 9600/12672]\n",
      "loss: 19.320278  [11200/12672]\n",
      "AUC: 0.5302115788922414\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 22.304651  [    0/12672]\n",
      "loss: 15.757086  [ 1600/12672]\n",
      "loss: 10.044224  [ 3200/12672]\n",
      "loss: 10.570917  [ 4800/12672]\n",
      "loss: 4.899301  [ 6400/12672]\n",
      "loss: 16.413872  [ 8000/12672]\n",
      "loss: 24.054111  [ 9600/12672]\n",
      "loss: 10.762904  [11200/12672]\n",
      "AUC: 0.5274521702680408\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.085052  [    0/12672]\n",
      "loss: 17.562784  [ 1600/12672]\n",
      "loss: 12.528733  [ 3200/12672]\n",
      "loss: 11.569736  [ 4800/12672]\n",
      "loss: 26.597647  [ 6400/12672]\n",
      "loss: 8.233865  [ 8000/12672]\n",
      "loss: 18.882545  [ 9600/12672]\n",
      "loss: 6.470361  [11200/12672]\n",
      "AUC: 0.5929326817712797\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.613498  [    0/12672]\n",
      "loss: 5.238051  [ 1600/12672]\n",
      "loss: 4.652621  [ 3200/12672]\n",
      "loss: 11.794142  [ 4800/12672]\n",
      "loss: 12.769295  [ 6400/12672]\n",
      "loss: 17.368681  [ 8000/12672]\n",
      "loss: 8.129433  [ 9600/12672]\n",
      "loss: 14.555595  [11200/12672]\n",
      "AUC: 0.556561245024265\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 20.522905  [    0/12672]\n",
      "loss: 14.036764  [ 1600/12672]\n",
      "loss: 9.069431  [ 3200/12672]\n",
      "loss: 11.294171  [ 4800/12672]\n",
      "loss: 9.986812  [ 6400/12672]\n",
      "loss: 17.863008  [ 8000/12672]\n",
      "loss: 8.018394  [ 9600/12672]\n",
      "loss: 12.659355  [11200/12672]\n",
      "AUC: 0.611316781482421\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.406613  [    0/12672]\n",
      "loss: 22.551908  [ 1600/12672]\n",
      "loss: 11.309487  [ 3200/12672]\n",
      "loss: 14.707373  [ 4800/12672]\n",
      "loss: 16.073357  [ 6400/12672]\n",
      "loss: 18.446978  [ 8000/12672]\n",
      "loss: 15.950687  [ 9600/12672]\n",
      "loss: 14.143830  [11200/12672]\n",
      "AUC: 0.6157298214308999\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.873575  [    0/12672]\n",
      "loss: 7.141421  [ 1600/12672]\n",
      "loss: 10.201398  [ 3200/12672]\n",
      "loss: 13.714252  [ 4800/12672]\n",
      "loss: 16.406813  [ 6400/12672]\n",
      "loss: 20.057568  [ 8000/12672]\n",
      "loss: 15.555166  [ 9600/12672]\n",
      "loss: 12.918344  [11200/12672]\n",
      "AUC: 0.6333781296705026\n",
      "sklearn AUC: 0.635273454060929\n",
      "Done!\n",
      "##############################\n",
      "SESSION 18\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.699791  [    0/12672]\n",
      "loss: 15.081879  [ 1600/12672]\n",
      "loss: 13.956064  [ 3200/12672]\n",
      "loss: 13.340436  [ 4800/12672]\n",
      "loss: 29.910975  [ 6400/12672]\n",
      "loss: 9.789010  [ 8000/12672]\n",
      "loss: 21.555420  [ 9600/12672]\n",
      "loss: 23.731817  [11200/12672]\n",
      "AUC: 0.5735809203868447\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.892500  [    0/12672]\n",
      "loss: 9.148643  [ 1600/12672]\n",
      "loss: 15.381108  [ 3200/12672]\n",
      "loss: 21.611971  [ 4800/12672]\n",
      "loss: 15.935391  [ 6400/12672]\n",
      "loss: 18.840862  [ 8000/12672]\n",
      "loss: 12.682886  [ 9600/12672]\n",
      "loss: 11.444788  [11200/12672]\n",
      "AUC: 0.6142876643863135\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 4.383454  [    0/12672]\n",
      "loss: 14.288405  [ 1600/12672]\n",
      "loss: 19.983246  [ 3200/12672]\n",
      "loss: 24.277712  [ 4800/12672]\n",
      "loss: 12.289611  [ 6400/12672]\n",
      "loss: 13.298376  [ 8000/12672]\n",
      "loss: 20.599577  [ 9600/12672]\n",
      "loss: 22.896416  [11200/12672]\n",
      "AUC: 0.579699834752145\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.058678  [    0/12672]\n",
      "loss: 8.425369  [ 1600/12672]\n",
      "loss: 20.413544  [ 3200/12672]\n",
      "loss: 15.865667  [ 4800/12672]\n",
      "loss: 13.492105  [ 6400/12672]\n",
      "loss: 4.813736  [ 8000/12672]\n",
      "loss: 17.920799  [ 9600/12672]\n",
      "loss: 16.331078  [11200/12672]\n",
      "AUC: 0.643214050291327\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 7.608126  [    0/12672]\n",
      "loss: 15.434283  [ 1600/12672]\n",
      "loss: 18.649338  [ 3200/12672]\n",
      "loss: 13.642121  [ 4800/12672]\n",
      "loss: 14.950622  [ 6400/12672]\n",
      "loss: 9.022635  [ 8000/12672]\n",
      "loss: 9.985211  [ 9600/12672]\n",
      "loss: 19.904091  [11200/12672]\n",
      "AUC: 0.6312736897635426\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 5.460303  [    0/12672]\n",
      "loss: 12.732197  [ 1600/12672]\n",
      "loss: 8.856299  [ 3200/12672]\n",
      "loss: 16.882214  [ 4800/12672]\n",
      "loss: 20.074156  [ 6400/12672]\n",
      "loss: 9.744645  [ 8000/12672]\n",
      "loss: 23.598923  [ 9600/12672]\n",
      "loss: 9.353761  [11200/12672]\n",
      "AUC: 0.6436999412891014\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.232074  [    0/12672]\n",
      "loss: 14.721903  [ 1600/12672]\n",
      "loss: 10.221792  [ 3200/12672]\n",
      "loss: 21.269207  [ 4800/12672]\n",
      "loss: 23.130495  [ 6400/12672]\n",
      "loss: 6.334181  [ 8000/12672]\n",
      "loss: 19.133913  [ 9600/12672]\n",
      "loss: 17.806147  [11200/12672]\n",
      "AUC: 0.5985156376079238\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 15.188581  [    0/12672]\n",
      "loss: 14.975966  [ 1600/12672]\n",
      "loss: 14.343554  [ 3200/12672]\n",
      "loss: 15.291156  [ 4800/12672]\n",
      "loss: 18.508308  [ 6400/12672]\n",
      "loss: 20.909365  [ 8000/12672]\n",
      "loss: 15.034113  [ 9600/12672]\n",
      "loss: 13.679252  [11200/12672]\n",
      "AUC: 0.6264019617299079\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 7.341304  [    0/12672]\n",
      "loss: 7.293659  [ 1600/12672]\n",
      "loss: 18.923662  [ 3200/12672]\n",
      "loss: 14.255431  [ 4800/12672]\n",
      "loss: 4.170861  [ 6400/12672]\n",
      "loss: 10.408089  [ 8000/12672]\n",
      "loss: 11.965382  [ 9600/12672]\n",
      "loss: 23.972027  [11200/12672]\n",
      "AUC: 0.6263838903724213\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 12.027957  [    0/12672]\n",
      "loss: 8.937201  [ 1600/12672]\n",
      "loss: 6.866424  [ 3200/12672]\n",
      "loss: 16.180227  [ 4800/12672]\n",
      "loss: 24.870205  [ 6400/12672]\n",
      "loss: 15.438313  [ 8000/12672]\n",
      "loss: 11.518669  [ 9600/12672]\n",
      "loss: 11.242174  [11200/12672]\n",
      "AUC: 0.5874690606802563\n",
      "sklearn AUC: 0.545699267282574\n",
      "Done!\n",
      "##############################\n",
      "SESSION 19\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.757007  [    0/12672]\n",
      "loss: 19.918171  [ 1600/12672]\n",
      "loss: 16.440662  [ 3200/12672]\n",
      "loss: 18.438723  [ 4800/12672]\n",
      "loss: 5.792552  [ 6400/12672]\n",
      "loss: 27.391033  [ 8000/12672]\n",
      "loss: 8.290288  [ 9600/12672]\n",
      "loss: 20.864729  [11200/12672]\n",
      "AUC: 0.5939094458394711\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 7.107557  [    0/12672]\n",
      "loss: 24.456264  [ 1600/12672]\n",
      "loss: 5.616837  [ 3200/12672]\n",
      "loss: 20.411942  [ 4800/12672]\n",
      "loss: 17.796572  [ 6400/12672]\n",
      "loss: 23.095024  [ 8000/12672]\n",
      "loss: 16.479185  [ 9600/12672]\n",
      "loss: 16.412779  [11200/12672]\n",
      "AUC: 0.5449589270049247\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 19.319212  [    0/12672]\n",
      "loss: 8.824503  [ 1600/12672]\n",
      "loss: 4.493415  [ 3200/12672]\n",
      "loss: 31.573277  [ 4800/12672]\n",
      "loss: 5.985581  [ 6400/12672]\n",
      "loss: 22.355474  [ 8000/12672]\n",
      "loss: 9.402124  [ 9600/12672]\n",
      "loss: 15.910559  [11200/12672]\n",
      "AUC: 0.6128244140009376\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 6.985211  [    0/12672]\n",
      "loss: 5.174231  [ 1600/12672]\n",
      "loss: 8.366651  [ 3200/12672]\n",
      "loss: 26.143946  [ 4800/12672]\n",
      "loss: 9.874316  [ 6400/12672]\n",
      "loss: 15.020460  [ 8000/12672]\n",
      "loss: 24.651943  [ 9600/12672]\n",
      "loss: 19.800243  [11200/12672]\n",
      "AUC: 0.5802116083111422\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 16.293129  [    0/12672]\n",
      "loss: 19.359892  [ 1600/12672]\n",
      "loss: 13.098198  [ 3200/12672]\n",
      "loss: 10.142686  [ 4800/12672]\n",
      "loss: 17.850847  [ 6400/12672]\n",
      "loss: 16.965517  [ 8000/12672]\n",
      "loss: 14.500562  [ 9600/12672]\n",
      "loss: 6.322971  [11200/12672]\n",
      "AUC: 0.5206901942549285\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 26.735619  [    0/12672]\n",
      "loss: 7.433164  [ 1600/12672]\n",
      "loss: 3.484652  [ 3200/12672]\n",
      "loss: 7.848645  [ 4800/12672]\n",
      "loss: 11.200237  [ 6400/12672]\n",
      "loss: 10.062584  [ 8000/12672]\n",
      "loss: 23.646809  [ 9600/12672]\n",
      "loss: 9.614874  [11200/12672]\n",
      "AUC: 0.6353900088483738\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 5.694057  [    0/12672]\n",
      "loss: 16.178995  [ 1600/12672]\n",
      "loss: 12.881453  [ 3200/12672]\n",
      "loss: 19.092514  [ 4800/12672]\n",
      "loss: 16.926142  [ 6400/12672]\n",
      "loss: 21.576418  [ 8000/12672]\n",
      "loss: 10.637090  [ 9600/12672]\n",
      "loss: 10.598244  [11200/12672]\n",
      "AUC: 0.5451057933430017\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.234221  [    0/12672]\n",
      "loss: 24.063736  [ 1600/12672]\n",
      "loss: 8.269053  [ 3200/12672]\n",
      "loss: 3.528171  [ 4800/12672]\n",
      "loss: 18.000822  [ 6400/12672]\n",
      "loss: 14.849436  [ 8000/12672]\n",
      "loss: 22.657393  [ 9600/12672]\n",
      "loss: 17.454742  [11200/12672]\n",
      "AUC: 0.5354178762679188\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 20.970390  [    0/12672]\n",
      "loss: 13.254111  [ 1600/12672]\n",
      "loss: 17.274715  [ 3200/12672]\n",
      "loss: 7.887078  [ 4800/12672]\n",
      "loss: 14.364001  [ 6400/12672]\n",
      "loss: 9.396379  [ 8000/12672]\n",
      "loss: 6.734177  [ 9600/12672]\n",
      "loss: 13.383113  [11200/12672]\n",
      "AUC: 0.5439357282945846\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 19.386005  [    0/12672]\n",
      "loss: 19.984179  [ 1600/12672]\n",
      "loss: 16.215122  [ 3200/12672]\n",
      "loss: 10.023355  [ 4800/12672]\n",
      "loss: 16.811987  [ 6400/12672]\n",
      "loss: 21.130922  [ 8000/12672]\n",
      "loss: 5.799153  [ 9600/12672]\n",
      "loss: 10.667345  [11200/12672]\n",
      "AUC: 0.5924551816363963\n",
      "sklearn AUC: 0.6122200844423067\n",
      "Done!\n",
      "##############################\n",
      "SESSION 20\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.723903  [    0/12672]\n",
      "loss: 8.579504  [ 1600/12672]\n",
      "loss: 5.457921  [ 3200/12672]\n",
      "loss: 18.821865  [ 4800/12672]\n",
      "loss: 10.727404  [ 6400/12672]\n",
      "loss: 30.335285  [ 8000/12672]\n",
      "loss: 26.408398  [ 9600/12672]\n",
      "loss: 10.914210  [11200/12672]\n",
      "AUC: 0.5247074915849864\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 27.876484  [    0/12672]\n",
      "loss: 16.653358  [ 1600/12672]\n",
      "loss: 17.143606  [ 3200/12672]\n",
      "loss: 13.513647  [ 4800/12672]\n",
      "loss: 17.962282  [ 6400/12672]\n",
      "loss: 28.948811  [ 8000/12672]\n",
      "loss: 22.067572  [ 9600/12672]\n",
      "loss: 18.742661  [11200/12672]\n",
      "AUC: 0.5427224638743562\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.050179  [    0/12672]\n",
      "loss: 22.407368  [ 1600/12672]\n",
      "loss: 13.497411  [ 3200/12672]\n",
      "loss: 11.490495  [ 4800/12672]\n",
      "loss: 11.619346  [ 6400/12672]\n",
      "loss: 4.709288  [ 8000/12672]\n",
      "loss: 21.116690  [ 9600/12672]\n",
      "loss: 13.636007  [11200/12672]\n",
      "AUC: 0.6193632844983943\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 10.846641  [    0/12672]\n",
      "loss: 22.367205  [ 1600/12672]\n",
      "loss: 16.640295  [ 3200/12672]\n",
      "loss: 15.447340  [ 4800/12672]\n",
      "loss: 20.805708  [ 6400/12672]\n",
      "loss: 20.879944  [ 8000/12672]\n",
      "loss: 21.292818  [ 9600/12672]\n",
      "loss: 14.370728  [11200/12672]\n",
      "AUC: 0.6218314952886066\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 6.359824  [    0/12672]\n",
      "loss: 10.808536  [ 1600/12672]\n",
      "loss: 16.849192  [ 3200/12672]\n",
      "loss: 18.026272  [ 4800/12672]\n",
      "loss: 14.076564  [ 6400/12672]\n",
      "loss: 19.993780  [ 8000/12672]\n",
      "loss: 10.416132  [ 9600/12672]\n",
      "loss: 16.832193  [11200/12672]\n",
      "AUC: 0.560699525297468\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 19.960230  [    0/12672]\n",
      "loss: 12.597827  [ 1600/12672]\n",
      "loss: 13.673230  [ 3200/12672]\n",
      "loss: 7.777695  [ 4800/12672]\n",
      "loss: 7.400980  [ 6400/12672]\n",
      "loss: 15.295063  [ 8000/12672]\n",
      "loss: 19.585783  [ 9600/12672]\n",
      "loss: 12.769836  [11200/12672]\n",
      "AUC: 0.5291274401826993\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 20.929743  [    0/12672]\n",
      "loss: 10.666913  [ 1600/12672]\n",
      "loss: 16.235222  [ 3200/12672]\n",
      "loss: 15.194500  [ 4800/12672]\n",
      "loss: 11.594779  [ 6400/12672]\n",
      "loss: 5.854448  [ 8000/12672]\n",
      "loss: 15.435652  [ 9600/12672]\n",
      "loss: 13.920863  [11200/12672]\n",
      "AUC: 0.603198857773789\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 19.520552  [    0/12672]\n",
      "loss: 10.012654  [ 1600/12672]\n",
      "loss: 12.951919  [ 3200/12672]\n",
      "loss: 15.685369  [ 4800/12672]\n",
      "loss: 5.987846  [ 6400/12672]\n",
      "loss: 19.494190  [ 8000/12672]\n",
      "loss: 11.281778  [ 9600/12672]\n",
      "loss: 13.400200  [11200/12672]\n",
      "AUC: 0.6464067071936198\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.247612  [    0/12672]\n",
      "loss: 9.110036  [ 1600/12672]\n",
      "loss: 12.545291  [ 3200/12672]\n",
      "loss: 12.017771  [ 4800/12672]\n",
      "loss: 7.534744  [ 6400/12672]\n",
      "loss: 12.639334  [ 8000/12672]\n",
      "loss: 6.042943  [ 9600/12672]\n",
      "loss: 4.509046  [11200/12672]\n",
      "AUC: 0.5202850401851231\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 25.859829  [    0/12672]\n",
      "loss: 8.394729  [ 1600/12672]\n",
      "loss: 12.407276  [ 3200/12672]\n",
      "loss: 12.024661  [ 4800/12672]\n",
      "loss: 11.947610  [ 6400/12672]\n",
      "loss: 24.348442  [ 8000/12672]\n",
      "loss: 11.730378  [ 9600/12672]\n",
      "loss: 9.399372  [11200/12672]\n",
      "AUC: 0.5532645431575958\n",
      "sklearn AUC: 0.5946913665583743\n",
      "Done!\n",
      "##############################\n",
      "SESSION 21\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.773975  [    0/12672]\n",
      "loss: 22.551655  [ 1600/12672]\n",
      "loss: 9.245339  [ 3200/12672]\n",
      "loss: 28.014809  [ 4800/12672]\n",
      "loss: 16.115999  [ 6400/12672]\n",
      "loss: 6.298082  [ 8000/12672]\n",
      "loss: 14.767112  [ 9600/12672]\n",
      "loss: 6.433066  [11200/12672]\n",
      "AUC: 0.5297194600452945\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.781473  [    0/12672]\n",
      "loss: 21.188972  [ 1600/12672]\n",
      "loss: 20.722412  [ 3200/12672]\n",
      "loss: 10.994416  [ 4800/12672]\n",
      "loss: 3.017396  [ 6400/12672]\n",
      "loss: 10.528286  [ 8000/12672]\n",
      "loss: 11.397026  [ 9600/12672]\n",
      "loss: 23.213556  [11200/12672]\n",
      "AUC: 0.5990270638026364\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 5.815384  [    0/12672]\n",
      "loss: 13.969739  [ 1600/12672]\n",
      "loss: 9.385937  [ 3200/12672]\n",
      "loss: 14.356415  [ 4800/12672]\n",
      "loss: 10.516901  [ 6400/12672]\n",
      "loss: 19.326014  [ 8000/12672]\n",
      "loss: 16.843880  [ 9600/12672]\n",
      "loss: 12.455470  [11200/12672]\n",
      "AUC: 0.5371072021680012\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.755333  [    0/12672]\n",
      "loss: 12.416578  [ 1600/12672]\n",
      "loss: 25.400156  [ 3200/12672]\n",
      "loss: 28.492699  [ 4800/12672]\n",
      "loss: 16.753052  [ 6400/12672]\n",
      "loss: 22.179388  [ 8000/12672]\n",
      "loss: 17.704136  [ 9600/12672]\n",
      "loss: 17.242256  [11200/12672]\n",
      "AUC: 0.6048610096986624\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 8.243212  [    0/12672]\n",
      "loss: 9.891590  [ 1600/12672]\n",
      "loss: 21.354599  [ 3200/12672]\n",
      "loss: 16.044876  [ 4800/12672]\n",
      "loss: 10.705533  [ 6400/12672]\n",
      "loss: 24.181074  [ 8000/12672]\n",
      "loss: 11.849720  [ 9600/12672]\n",
      "loss: 17.854710  [11200/12672]\n",
      "AUC: 0.5690596071874644\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 26.817181  [    0/12672]\n",
      "loss: 13.767743  [ 1600/12672]\n",
      "loss: 5.273962  [ 3200/12672]\n",
      "loss: 10.113286  [ 4800/12672]\n",
      "loss: 9.266470  [ 6400/12672]\n",
      "loss: 6.454073  [ 8000/12672]\n",
      "loss: 15.358890  [ 9600/12672]\n",
      "loss: 16.709959  [11200/12672]\n",
      "AUC: 0.5116077849981829\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 29.284836  [    0/12672]\n",
      "loss: 23.651440  [ 1600/12672]\n",
      "loss: 7.811034  [ 3200/12672]\n",
      "loss: 8.512115  [ 4800/12672]\n",
      "loss: 7.258577  [ 6400/12672]\n",
      "loss: 12.624157  [ 8000/12672]\n",
      "loss: 7.930473  [ 9600/12672]\n",
      "loss: 15.279963  [11200/12672]\n",
      "AUC: 0.5814452331765133\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 18.155424  [    0/12672]\n",
      "loss: 18.821892  [ 1600/12672]\n",
      "loss: 24.286209  [ 3200/12672]\n",
      "loss: 8.249099  [ 4800/12672]\n",
      "loss: 22.180941  [ 6400/12672]\n",
      "loss: 19.036163  [ 8000/12672]\n",
      "loss: 14.986416  [ 9600/12672]\n",
      "loss: 11.226034  [11200/12672]\n",
      "AUC: 0.6407893775919076\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 8.302967  [    0/12672]\n",
      "loss: 21.248585  [ 1600/12672]\n",
      "loss: 5.875209  [ 3200/12672]\n",
      "loss: 16.887350  [ 4800/12672]\n",
      "loss: 16.623032  [ 6400/12672]\n",
      "loss: 12.326205  [ 8000/12672]\n",
      "loss: 15.973649  [ 9600/12672]\n",
      "loss: 19.365543  [11200/12672]\n",
      "AUC: 0.6151077410983328\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 11.147881  [    0/12672]\n",
      "loss: 26.316460  [ 1600/12672]\n",
      "loss: 16.283266  [ 3200/12672]\n",
      "loss: 12.931489  [ 4800/12672]\n",
      "loss: 13.203357  [ 6400/12672]\n",
      "loss: 7.931904  [ 8000/12672]\n",
      "loss: 12.406417  [ 9600/12672]\n",
      "loss: 8.491207  [11200/12672]\n",
      "AUC: 0.6020520258983004\n",
      "sklearn AUC: 0.6683150500513175\n",
      "Done!\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.739511  [    0/12672]\n",
      "loss: 20.605413  [ 1600/12672]\n",
      "loss: 12.269506  [ 3200/12672]\n",
      "loss: 26.886724  [ 4800/12672]\n",
      "loss: 17.081362  [ 6400/12672]\n",
      "loss: 14.537307  [ 8000/12672]\n",
      "loss: 18.809790  [ 9600/12672]\n",
      "loss: 8.397475  [11200/12672]\n",
      "AUC: 0.5517255490617168\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 23.375656  [    0/12672]\n",
      "loss: 7.885007  [ 1600/12672]\n",
      "loss: 25.521849  [ 3200/12672]\n",
      "loss: 11.761548  [ 4800/12672]\n",
      "loss: 6.000403  [ 6400/12672]\n",
      "loss: 15.608433  [ 8000/12672]\n",
      "loss: 23.685484  [ 9600/12672]\n",
      "loss: 4.585995  [11200/12672]\n",
      "AUC: 0.5543443702216082\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.590322  [    0/12672]\n",
      "loss: 19.140810  [ 1600/12672]\n",
      "loss: 23.808130  [ 3200/12672]\n",
      "loss: 18.592228  [ 4800/12672]\n",
      "loss: 30.218163  [ 6400/12672]\n",
      "loss: 24.972908  [ 8000/12672]\n",
      "loss: 23.785742  [ 9600/12672]\n",
      "loss: 9.034603  [11200/12672]\n",
      "AUC: 0.5490008535362407\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 14.248378  [    0/12672]\n",
      "loss: 7.001456  [ 1600/12672]\n",
      "loss: 22.971104  [ 3200/12672]\n",
      "loss: 21.364885  [ 4800/12672]\n",
      "loss: 17.607092  [ 6400/12672]\n",
      "loss: 14.792459  [ 8000/12672]\n",
      "loss: 8.217230  [ 9600/12672]\n",
      "loss: 11.015251  [11200/12672]\n",
      "AUC: 0.6218806230920062\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 9.008027  [    0/12672]\n",
      "loss: 24.355181  [ 1600/12672]\n",
      "loss: 17.639500  [ 3200/12672]\n",
      "loss: 9.645941  [ 4800/12672]\n",
      "loss: 15.296521  [ 6400/12672]\n",
      "loss: 11.518813  [ 8000/12672]\n",
      "loss: 19.012392  [ 9600/12672]\n",
      "loss: 11.329838  [11200/12672]\n",
      "AUC: 0.565753533233842\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 17.285603  [    0/12672]\n",
      "loss: 9.422576  [ 1600/12672]\n",
      "loss: 7.154765  [ 3200/12672]\n",
      "loss: 14.537226  [ 4800/12672]\n",
      "loss: 16.980021  [ 6400/12672]\n",
      "loss: 15.312503  [ 8000/12672]\n",
      "loss: 22.352489  [ 9600/12672]\n",
      "loss: 13.954957  [11200/12672]\n",
      "AUC: 0.5874690936004301\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 20.907953  [    0/12672]\n",
      "loss: 6.506123  [ 1600/12672]\n",
      "loss: 11.069824  [ 3200/12672]\n",
      "loss: 10.169417  [ 4800/12672]\n",
      "loss: 9.702079  [ 6400/12672]\n",
      "loss: 6.709302  [ 8000/12672]\n",
      "loss: 11.029011  [ 9600/12672]\n",
      "loss: 7.965961  [11200/12672]\n",
      "AUC: 0.5117590970451714\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 28.022139  [    0/12672]\n",
      "loss: 11.991787  [ 1600/12672]\n",
      "loss: 8.208617  [ 3200/12672]\n",
      "loss: 16.763422  [ 4800/12672]\n",
      "loss: 15.350168  [ 6400/12672]\n",
      "loss: 9.828220  [ 8000/12672]\n",
      "loss: 16.094444  [ 9600/12672]\n",
      "loss: 11.878670  [11200/12672]\n",
      "AUC: 0.5322614147284062\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 20.423613  [    0/12672]\n",
      "loss: 24.591978  [ 1600/12672]\n",
      "loss: 11.268704  [ 3200/12672]\n",
      "loss: 20.305086  [ 4800/12672]\n",
      "loss: 9.197585  [ 6400/12672]\n",
      "loss: 12.612864  [ 8000/12672]\n",
      "loss: 10.771314  [ 9600/12672]\n",
      "loss: 3.714616  [11200/12672]\n",
      "AUC: 0.5758475092620288\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 13.974939  [    0/12672]\n",
      "loss: 14.829138  [ 1600/12672]\n",
      "loss: 10.486351  [ 3200/12672]\n",
      "loss: 13.740087  [ 4800/12672]\n",
      "loss: 5.953163  [ 6400/12672]\n",
      "loss: 16.058413  [ 8000/12672]\n",
      "loss: 12.558070  [ 9600/12672]\n",
      "loss: 16.700096  [11200/12672]\n",
      "AUC: 0.549978806663491\n",
      "sklearn AUC: 0.6187101063829787\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "logo = LeaveOneGroupOut()\n",
    "for i, (train_idx, test_idx) in enumerate(logo.split(X, y, groups=sessions)):\n",
    "    print(f\"{'#'*30}\\nSESSION {i}\\n{'#'*30}\")\n",
    "    ## create model ##\n",
    "    model = LogisticRegressionTorch(X.shape[-1])\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    weight_decay = 1e-4\n",
    "    lr = 5e-1\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    ## data ##\n",
    "    train_set = SimpleDataset(X[train_idx], y[train_idx])\n",
    "    test_set = SimpleDataset(X[test_idx], y[test_idx])\n",
    "    ## class balancing ##\n",
    "    cls_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(train_set.y.detach().numpy()),\n",
    "        y=train_set.y.detach().numpy(),\n",
    "    )\n",
    "    weights = cls_weights[train_set.y.detach().numpy().astype(int)]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights, len(train_set.y.detach().numpy()), replacement=True\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(train_set, batch_size=200, sampler=sampler)\n",
    "\n",
    "    ## training epochs ##\n",
    "    epochs = 10\n",
    "    for t in range(epochs):\n",
    "        print(f\"{'-'*30}\\nEpoch {t+1}\\n{'-'*30}\")\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, print_nth_batch=8)\n",
    "        out = test_auc_score(train_set, model)\n",
    "\n",
    "    lr_sk = LogisticRegression(\n",
    "        C=weight_decay, class_weight=\"balanced\", fit_intercept=True\n",
    "    )\n",
    "    lr_sk.fit(X=X_npy[train_idx], y=y_npy[train_idx])\n",
    "    pred = lr_sk.predict_proba(X_npy[test_idx])[:, 1]\n",
    "    score = roc_auc_score(y_npy[test_idx], pred)\n",
    "    print(\"sklearn AUC:\", score)\n",
    "\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff25666c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
