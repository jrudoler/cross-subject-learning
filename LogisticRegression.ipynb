{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55035abd-0f92-4b00-90a5-90048fe5149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "# import skorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from ptsa.data.timeseries import TimeSeries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_validate\n",
    "\n",
    "#### PYTORCH CONFIGURATION SETTINGS ######\n",
    "device = \"cpu\"\n",
    "#FOR GPU: device = \"mps\" if torch.has_mps else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b18eb46-9cb5-4f79-bd67-59faf8336e78",
   "metadata": {},
   "source": [
    "### Logistic Regression PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5d5cd46b-8998-4508-91e7-d08c09b4d3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "        \n",
    "class LogisticRegressionTorch(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.logistic = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, output_dim, bias=True),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # logits = torch.sigmoid(self.linear(x))\n",
    "        probs = self.logistic(x)\n",
    "        return probs\n",
    "    \n",
    "def train_loop(dataloader, model, loss_fn, optimizer, print_nth_batch=4, l2=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(\"yay training\")\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # print(y)\n",
    "        # Compute prediction and loss\n",
    "        pred = torch.squeeze(model(X))\n",
    "        # print(pred)\n",
    "        # regularization, computing largest singular value\n",
    "        loss = loss_fn(pred, y)\n",
    "        # if l2:\n",
    "        #     loss = loss + l2*l2_reg(model)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % print_nth_batch == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    \n",
    "    # we don't want to track gradients here because we're just doing\n",
    "    # a forward pass to evaluate predictions\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = torch.squeeze(model(X))\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            # round predicted probs to get label prediction, compute n correct\n",
    "            correct += (pred.round() == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "def test_auc_score(dataset, model):\n",
    "    with torch.no_grad():\n",
    "        X, y = dataset[:]\n",
    "        pred = model(X)\n",
    "        pred, y = pred.detach().numpy(), y.detach().numpy()\n",
    "        score = roc_auc_score(y_true=y, y_score=pred)\n",
    "        print(\"AUC:\", score)\n",
    "    return score\n",
    "\n",
    "def l2_reg(model):\n",
    "  \"\"\"\n",
    "  This function calculates the l2 norm of the all the tensors in the model\n",
    "  Args:\n",
    "    model: nn.module\n",
    "      Neural network instance\n",
    "  Returns:\n",
    "    l2: float\n",
    "      L2 norm of the all the tensors in the model\n",
    "  \"\"\"\n",
    "\n",
    "  l2 = 0.0\n",
    "  for param in model.parameters():\n",
    "    l2 += torch.sum(torch.abs(param)**2)\n",
    "\n",
    "  return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "88a7ed46-6f7e-494b-9f74-1571075c8db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = torch.rand(1000, 128) #+ .2\n",
    "y_1 = torch.zeros(1000,)\n",
    "X_2 = torch.rand(1000, 128) #- .1\n",
    "y_2 = torch.ones(1000,)\n",
    "\n",
    "X = torch.cat((X_1, X_2))\n",
    "y = torch.cat((y_1, y_2))\n",
    "\n",
    "dataset = SimpleDataset(X, y)\n",
    "\n",
    "train_prop = .8\n",
    "train_num = int(train_prop * len(dataset))\n",
    "test_num = len(X) - train_num\n",
    "\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset,\n",
    "                                                    [train_num, test_num])\n",
    "\n",
    "\n",
    "target = train_set.dataset.y[train_set.indices]\n",
    "cls_weights = torch.from_numpy(\n",
    "    compute_class_weight(class_weight='balanced',\n",
    "                         classes=np.unique(target.numpy()),\n",
    "                         y=target.numpy())\n",
    ")\n",
    "weights = cls_weights[target.numpy()]\n",
    "sampler = WeightedRandomSampler(weights, len(target.numpy()), replacement=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=100, sampler=sampler)\n",
    "test_dataloader = DataLoader(test_set, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "29b0b7b5-d1b1-44fd-b644-842d55fdd533",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegressionTorch(128)\n",
    "\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "weight_decay = 1\n",
    "lr = 5e-1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d62a1cb0-0e75-4aed-8389-5f90c13bb18c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 56 has been set.\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.708420  [    0/ 1600]\n",
      "loss: 2.634404  [  400/ 1600]\n",
      "loss: 1.886310  [  800/ 1600]\n",
      "loss: 3.355282  [ 1200/ 1600]\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Avg loss: 2.233068 \n",
      "\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.257223  [    0/ 1600]\n",
      "loss: 2.567374  [  400/ 1600]\n",
      "loss: 2.422275  [  800/ 1600]\n",
      "loss: 3.636494  [ 1200/ 1600]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 2.845356 \n",
      "\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.044448  [    0/ 1600]\n",
      "loss: 2.774942  [  400/ 1600]\n",
      "loss: 3.610913  [  800/ 1600]\n",
      "loss: 3.200505  [ 1200/ 1600]\n",
      "Test Error: \n",
      " Accuracy: 49.4%, Avg loss: 2.508357 \n",
      "\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.558735  [    0/ 1600]\n",
      "loss: 3.672528  [  400/ 1600]\n",
      "loss: 2.240831  [  800/ 1600]\n",
      "loss: 2.369738  [ 1200/ 1600]\n",
      "Test Error: \n",
      " Accuracy: 49.8%, Avg loss: 2.688350 \n",
      "\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.106107  [    0/ 1600]\n",
      "loss: 2.108453  [  400/ 1600]\n",
      "loss: 2.663188  [  800/ 1600]\n",
      "loss: 2.808820  [ 1200/ 1600]\n",
      "Test Error: \n",
      " Accuracy: 46.4%, Avg loss: 2.427604 \n",
      "\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.979601  [    0/ 1600]\n",
      "loss: 3.911031  [  400/ 1600]\n",
      "loss: 2.188498  [  800/ 1600]\n",
      "loss: 2.565494  [ 1200/ 1600]\n",
      "Test Error: \n",
      " Accuracy: 50.9%, Avg loss: 2.414783 \n",
      "\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.459810  [    0/ 1600]\n",
      "loss: 2.707870  [  400/ 1600]\n",
      "loss: 1.744800  [  800/ 1600]\n",
      "loss: 4.206394  [ 1200/ 1600]\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Avg loss: 2.561516 \n",
      "\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.690519  [    0/ 1600]\n",
      "loss: 3.623756  [  400/ 1600]\n",
      "loss: 2.977982  [  800/ 1600]\n",
      "loss: 3.448253  [ 1200/ 1600]\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Avg loss: 1.985326 \n",
      "\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.123854  [    0/ 1600]\n",
      "loss: 2.949481  [  400/ 1600]\n",
      "loss: 3.079839  [  800/ 1600]\n",
      "loss: 3.183334  [ 1200/ 1600]\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Avg loss: 2.929711 \n",
      "\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.511956  [    0/ 1600]\n",
      "loss: 2.241677  [  400/ 1600]\n",
      "loss: 3.322734  [  800/ 1600]\n",
      "loss: 2.791874  [ 1200/ 1600]\n",
      "Test Error: \n",
      " Accuracy: 49.3%, Avg loss: 2.542904 \n",
      "\n",
      "Done!\n",
      "CPU times: user 165 ms, sys: 3.49 ms, total: 168 ms\n",
      "Wall time: 170 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "set_seed(56)\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"{'-'*30}\\nEpoch {t+1}\\n{'-'*30}\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(train_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e8834de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.0516, 0.0848, 0.0837, 0.0787, 0.0803, 0.0868, 0.0949, 0.0685, 0.0809,\n",
       "          0.0693, 0.0845, 0.0788, 0.0478, 0.0922, 0.0765, 0.0784, 0.0889, 0.0731,\n",
       "          0.0611, 0.0642, 0.0920, 0.0620, 0.0579, 0.0948, 0.0801, 0.1004, 0.0719,\n",
       "          0.0990, 0.0646, 0.0701, 0.0817, 0.0766, 0.0656, 0.0537, 0.0844, 0.0760,\n",
       "          0.0769, 0.0700, 0.0895, 0.0631, 0.0493, 0.0791, 0.0862, 0.0611, 0.0861,\n",
       "          0.0758, 0.0932, 0.0706, 0.0764, 0.0678, 0.0934, 0.0671, 0.0850, 0.0823,\n",
       "          0.0697, 0.0799, 0.0528, 0.0658, 0.0993, 0.0533, 0.0798, 0.0661, 0.0610,\n",
       "          0.0871, 0.0568, 0.0633, 0.0963, 0.0633, 0.1104, 0.0917, 0.0628, 0.0928,\n",
       "          0.0853, 0.0892, 0.0786, 0.0862, 0.0798, 0.0835, 0.0657, 0.0718, 0.0535,\n",
       "          0.0706, 0.0722, 0.0796, 0.0873, 0.0618, 0.0824, 0.0906, 0.0549, 0.0860,\n",
       "          0.0809, 0.0729, 0.0782, 0.0709, 0.0732, 0.1127, 0.0690, 0.0599, 0.0577,\n",
       "          0.0560, 0.0460, 0.0872, 0.0882, 0.0891, 0.0931, 0.0565, 0.0681, 0.0795,\n",
       "          0.0730, 0.0617, 0.0667, 0.0841, 0.0471, 0.0972, 0.0709, 0.0855, 0.0932,\n",
       "          0.0722, 0.0687, 0.0972, 0.0792, 0.0701, 0.0819, 0.0798, 0.0833, 0.0487,\n",
       "          0.0593, 0.0968]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1517], requires_grad=True)]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ce08d-e451-45d9-b3fc-a28c2bedf854",
   "metadata": {},
   "source": [
    "## Compare to sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "523260ef-dec1-468c-ab02-fe635822b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.6 ms, sys: 2.55 ms, total: 38.2 ms\n",
      "Wall time: 37.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sk_model = LogisticRegression(penalty='l2', C=1e-4, class_weight='balanced', fit_intercept=True, solver='saga')\n",
    "\n",
    "train_idx = train_set.indices\n",
    "test_idx = test_set.indices\n",
    "sk_train_X = train_set.dataset.X[train_idx].detach().numpy()\n",
    "sk_train_y = train_set.dataset.y[train_idx].detach().numpy()\n",
    "sk_test_X = test_set.dataset.X[test_idx].detach().numpy()\n",
    "sk_test_y = test_set.dataset.y[test_idx].detach().numpy()\n",
    "fit = sk_model.fit(sk_train_X, sk_train_y)\n",
    "\n",
    "sk_pred = fit.predict(sk_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "58151025-efbe-4cce-afff-9a5c7e397716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5025"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(sk_test_y, sk_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a1afe66-561c-4162-bf56-5b951363d036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00979521, -0.01041032, -0.01043458, -0.01033281, -0.01070288,\n",
       "        -0.01063507, -0.01136468, -0.01099171, -0.01133921, -0.01007822,\n",
       "        -0.01104162, -0.01064495, -0.01097371, -0.01079975, -0.01099747,\n",
       "        -0.01146429, -0.01066068, -0.0110929 , -0.0106158 , -0.01105246,\n",
       "        -0.00977475, -0.0104137 , -0.01022611, -0.01116386, -0.0105358 ,\n",
       "        -0.01066853, -0.01025571, -0.01081117, -0.01132061, -0.01142405,\n",
       "        -0.01117413, -0.01118021, -0.01104569, -0.01105985, -0.01123603,\n",
       "        -0.01026956, -0.01058908, -0.01129912, -0.00956585, -0.01041917,\n",
       "        -0.01127243, -0.01100743, -0.01120172, -0.01131775, -0.01021639,\n",
       "        -0.01010797, -0.0107777 , -0.01063255, -0.01132434, -0.01146677,\n",
       "        -0.01116416, -0.01115323, -0.00993205, -0.01082593, -0.01145096,\n",
       "        -0.01099095, -0.01160888, -0.01150241, -0.01018634, -0.01087658,\n",
       "        -0.01007172, -0.01082219, -0.01094342, -0.01126222, -0.01157071,\n",
       "        -0.01069679, -0.01101769, -0.00981333, -0.00985882, -0.00998321,\n",
       "        -0.01084413, -0.01024425, -0.01093687, -0.01039739, -0.01042217,\n",
       "        -0.01056994, -0.01129764, -0.01146922, -0.01169576, -0.01045212,\n",
       "        -0.01007781, -0.01072503, -0.0099256 , -0.01183545, -0.01108811,\n",
       "        -0.01134121, -0.01007892, -0.01037475, -0.0103623 , -0.01051581,\n",
       "        -0.01143738, -0.01098496, -0.01085317, -0.01040908, -0.01080115,\n",
       "        -0.01043946, -0.01059909, -0.0111082 , -0.01096098, -0.01063707,\n",
       "        -0.01157838, -0.01157035, -0.01026318, -0.01013502, -0.01164908,\n",
       "        -0.01122986, -0.00959864, -0.01078646, -0.01013704, -0.01044783,\n",
       "        -0.01159921, -0.00981075, -0.01050308, -0.01076146, -0.01011677,\n",
       "        -0.01085857, -0.01082453, -0.01159357, -0.01085533, -0.01066514,\n",
       "        -0.01048233, -0.0109984 , -0.01120654, -0.01054894, -0.01080566,\n",
       "        -0.01044488, -0.0099956 , -0.0109686 ]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e886011f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.75895626])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a23101-0bd3-4961-b112-e36be91814f4",
   "metadata": {},
   "source": [
    "## Using lab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be80b2f1-34c7-458e-88a0-a1c8ae83f825",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = TimeSeries.from_hdf(\n",
    "    \"/Users/jrudoler/rhino_mount/scratch/jrudoler/scalp_features/LTP093_feats.h5\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3edface1-79d9-4147-87e1-79869a0c0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_npy = ts.data\n",
    "y_npy = ts.recalled.data\n",
    "X = torch.tensor(ts.data).float()\n",
    "y = torch.tensor(ts.recalled.data).float()\n",
    "dataset = SimpleDataset(X, y)\n",
    "sessions = ts.session.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5e42c29f-5d38-4187-88cb-36414f76b390",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 56 has been set.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214cc3b22b1c42aba7018d622355a53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.745432  [    0/12672]\n",
      "loss: 1.996796  [ 1600/12672]\n",
      "loss: 2.304744  [ 3200/12672]\n",
      "loss: 2.183406  [ 4800/12672]\n",
      "loss: 1.957629  [ 6400/12672]\n",
      "loss: 1.758837  [ 8000/12672]\n",
      "loss: 1.953741  [ 9600/12672]\n",
      "loss: 2.101884  [11200/12672]\n",
      "AUC: 0.5432395074139231\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.437188  [    0/12672]\n",
      "loss: 2.152052  [ 1600/12672]\n",
      "loss: 1.826231  [ 3200/12672]\n",
      "loss: 2.456435  [ 4800/12672]\n",
      "loss: 1.164517  [ 6400/12672]\n",
      "loss: 2.500187  [ 8000/12672]\n",
      "loss: 2.419441  [ 9600/12672]\n",
      "loss: 1.984975  [11200/12672]\n",
      "AUC: 0.541090726313144\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.699327  [    0/12672]\n",
      "loss: 2.364421  [ 1600/12672]\n",
      "loss: 2.032473  [ 3200/12672]\n",
      "loss: 2.365137  [ 4800/12672]\n",
      "loss: 2.494399  [ 6400/12672]\n",
      "loss: 2.304005  [ 8000/12672]\n",
      "loss: 3.020437  [ 9600/12672]\n",
      "loss: 2.550306  [11200/12672]\n",
      "AUC: 0.5503895451118372\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.449300  [    0/12672]\n",
      "loss: 2.765722  [ 1600/12672]\n",
      "loss: 2.543235  [ 3200/12672]\n",
      "loss: 1.866425  [ 4800/12672]\n",
      "loss: 1.688641  [ 6400/12672]\n",
      "loss: 2.720394  [ 8000/12672]\n",
      "loss: 2.484617  [ 9600/12672]\n",
      "loss: 3.257766  [11200/12672]\n",
      "AUC: 0.5263885398341291\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 6.329540  [    0/12672]\n",
      "loss: 2.524056  [ 1600/12672]\n",
      "loss: 4.279720  [ 3200/12672]\n",
      "loss: 2.096517  [ 4800/12672]\n",
      "loss: 3.524107  [ 6400/12672]\n",
      "loss: 2.260773  [ 8000/12672]\n",
      "loss: 2.853966  [ 9600/12672]\n",
      "loss: 2.582156  [11200/12672]\n",
      "AUC: 0.546695149535059\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.468516  [    0/12672]\n",
      "loss: 0.700708  [ 1600/12672]\n",
      "loss: 0.693528  [ 3200/12672]\n",
      "loss: 0.676130  [ 4800/12672]\n",
      "loss: 0.694968  [ 6400/12672]\n",
      "loss: 0.671512  [ 8000/12672]\n",
      "loss: 0.683493  [ 9600/12672]\n",
      "loss: 0.671536  [11200/12672]\n",
      "AUC: 0.6171525508921839\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.666215  [    0/12672]\n",
      "loss: 0.688777  [ 1600/12672]\n",
      "loss: 0.687132  [ 3200/12672]\n",
      "loss: 0.677501  [ 4800/12672]\n",
      "loss: 0.675480  [ 6400/12672]\n",
      "loss: 0.675956  [ 8000/12672]\n",
      "loss: 0.695382  [ 9600/12672]\n",
      "loss: 0.682303  [11200/12672]\n",
      "AUC: 0.6143000753958281\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.687447  [    0/12672]\n",
      "loss: 0.692019  [ 1600/12672]\n",
      "loss: 0.673352  [ 3200/12672]\n",
      "loss: 0.676433  [ 4800/12672]\n",
      "loss: 0.689189  [ 6400/12672]\n",
      "loss: 0.680461  [ 8000/12672]\n",
      "loss: 0.682965  [ 9600/12672]\n",
      "loss: 0.665656  [11200/12672]\n",
      "AUC: 0.5736365921085701\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.708215  [    0/12672]\n",
      "loss: 0.692008  [ 1600/12672]\n",
      "loss: 0.667009  [ 3200/12672]\n",
      "loss: 0.675288  [ 4800/12672]\n",
      "loss: 0.685143  [ 6400/12672]\n",
      "loss: 0.686183  [ 8000/12672]\n",
      "loss: 0.710645  [ 9600/12672]\n",
      "loss: 0.694424  [11200/12672]\n",
      "AUC: 0.5950741392309625\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.675857  [    0/12672]\n",
      "loss: 0.679456  [ 1600/12672]\n",
      "loss: 0.688708  [ 3200/12672]\n",
      "loss: 0.684154  [ 4800/12672]\n",
      "loss: 0.671024  [ 6400/12672]\n",
      "loss: 0.689410  [ 8000/12672]\n",
      "loss: 0.671227  [ 9600/12672]\n",
      "loss: 0.672702  [11200/12672]\n",
      "AUC: 0.6253706961548128\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.764890  [    0/12672]\n",
      "loss: 1.699613  [ 1600/12672]\n",
      "loss: 2.843538  [ 3200/12672]\n",
      "loss: 2.596693  [ 4800/12672]\n",
      "loss: 2.349087  [ 6400/12672]\n",
      "loss: 2.115015  [ 8000/12672]\n",
      "loss: 3.267020  [ 9600/12672]\n",
      "loss: 2.801669  [11200/12672]\n",
      "AUC: 0.5668518335230366\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 4.048387  [    0/12672]\n",
      "loss: 2.102696  [ 1600/12672]\n",
      "loss: 2.381749  [ 3200/12672]\n",
      "loss: 3.075911  [ 4800/12672]\n",
      "loss: 2.420285  [ 6400/12672]\n",
      "loss: 2.314468  [ 8000/12672]\n",
      "loss: 2.324986  [ 9600/12672]\n",
      "loss: 2.283111  [11200/12672]\n",
      "AUC: 0.5701922601078835\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.972909  [    0/12672]\n",
      "loss: 2.961524  [ 1600/12672]\n",
      "loss: 2.060566  [ 3200/12672]\n",
      "loss: 1.595026  [ 4800/12672]\n",
      "loss: 1.750460  [ 6400/12672]\n",
      "loss: 2.630781  [ 8000/12672]\n",
      "loss: 2.836313  [ 9600/12672]\n",
      "loss: 2.902060  [11200/12672]\n",
      "AUC: 0.576668976097392\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.608702  [    0/12672]\n",
      "loss: 3.112133  [ 1600/12672]\n",
      "loss: 2.329412  [ 3200/12672]\n",
      "loss: 2.626973  [ 4800/12672]\n",
      "loss: 2.032738  [ 6400/12672]\n",
      "loss: 4.033308  [ 8000/12672]\n",
      "loss: 2.890341  [ 9600/12672]\n",
      "loss: 2.568099  [11200/12672]\n",
      "AUC: 0.5688870193497302\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.880482  [    0/12672]\n",
      "loss: 2.171638  [ 1600/12672]\n",
      "loss: 2.713228  [ 3200/12672]\n",
      "loss: 3.809373  [ 4800/12672]\n",
      "loss: 2.326963  [ 6400/12672]\n",
      "loss: 3.175326  [ 8000/12672]\n",
      "loss: 3.685014  [ 9600/12672]\n",
      "loss: 2.164324  [11200/12672]\n",
      "AUC: 0.5632330380561191\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 5.477713  [    0/12672]\n",
      "loss: 0.849035  [ 1600/12672]\n",
      "loss: 0.680370  [ 3200/12672]\n",
      "loss: 0.677624  [ 4800/12672]\n",
      "loss: 0.692802  [ 6400/12672]\n",
      "loss: 0.681994  [ 8000/12672]\n",
      "loss: 0.692542  [ 9600/12672]\n",
      "loss: 0.689597  [11200/12672]\n",
      "AUC: 0.532587717127728\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.676002  [    0/12672]\n",
      "loss: 0.686502  [ 1600/12672]\n",
      "loss: 0.693829  [ 3200/12672]\n",
      "loss: 0.684957  [ 4800/12672]\n",
      "loss: 0.682206  [ 6400/12672]\n",
      "loss: 0.684260  [ 8000/12672]\n",
      "loss: 0.676919  [ 9600/12672]\n",
      "loss: 0.686794  [11200/12672]\n",
      "AUC: 0.6076359677339537\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.685750  [    0/12672]\n",
      "loss: 0.685233  [ 1600/12672]\n",
      "loss: 0.675762  [ 3200/12672]\n",
      "loss: 0.669533  [ 4800/12672]\n",
      "loss: 0.684903  [ 6400/12672]\n",
      "loss: 0.675842  [ 8000/12672]\n",
      "loss: 0.681250  [ 9600/12672]\n",
      "loss: 0.677913  [11200/12672]\n",
      "AUC: 0.6129311624684516\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.689328  [    0/12672]\n",
      "loss: 0.679829  [ 1600/12672]\n",
      "loss: 0.682610  [ 3200/12672]\n",
      "loss: 0.682198  [ 4800/12672]\n",
      "loss: 0.673650  [ 6400/12672]\n",
      "loss: 0.689382  [ 8000/12672]\n",
      "loss: 0.684419  [ 9600/12672]\n",
      "loss: 0.684210  [11200/12672]\n",
      "AUC: 0.5908224872568912\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.676694  [    0/12672]\n",
      "loss: 0.677989  [ 1600/12672]\n",
      "loss: 0.673105  [ 3200/12672]\n",
      "loss: 0.682267  [ 4800/12672]\n",
      "loss: 0.668386  [ 6400/12672]\n",
      "loss: 0.684440  [ 8000/12672]\n",
      "loss: 0.675687  [ 9600/12672]\n",
      "loss: 0.676692  [11200/12672]\n",
      "AUC: 0.5876923838273866\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.733132  [    0/12672]\n",
      "loss: 1.918195  [ 1600/12672]\n",
      "loss: 3.047584  [ 3200/12672]\n",
      "loss: 3.054600  [ 4800/12672]\n",
      "loss: 3.412297  [ 6400/12672]\n",
      "loss: 3.049307  [ 8000/12672]\n",
      "loss: 2.413154  [ 9600/12672]\n",
      "loss: 2.048525  [11200/12672]\n",
      "AUC: 0.4878721859114016\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.471427  [    0/12672]\n",
      "loss: 2.409298  [ 1600/12672]\n",
      "loss: 2.165976  [ 3200/12672]\n",
      "loss: 1.732591  [ 4800/12672]\n",
      "loss: 3.022880  [ 6400/12672]\n",
      "loss: 2.046564  [ 8000/12672]\n",
      "loss: 1.746551  [ 9600/12672]\n",
      "loss: 1.880523  [11200/12672]\n",
      "AUC: 0.4860566448801743\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.700447  [    0/12672]\n",
      "loss: 2.972633  [ 1600/12672]\n",
      "loss: 2.266039  [ 3200/12672]\n",
      "loss: 2.208823  [ 4800/12672]\n",
      "loss: 3.655629  [ 6400/12672]\n",
      "loss: 2.606248  [ 8000/12672]\n",
      "loss: 3.318285  [ 9600/12672]\n",
      "loss: 2.228842  [11200/12672]\n",
      "AUC: 0.48592350520455097\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.304882  [    0/12672]\n",
      "loss: 2.200992  [ 1600/12672]\n",
      "loss: 3.365231  [ 3200/12672]\n",
      "loss: 2.145385  [ 4800/12672]\n",
      "loss: 3.006649  [ 6400/12672]\n",
      "loss: 1.878996  [ 8000/12672]\n",
      "loss: 2.880288  [ 9600/12672]\n",
      "loss: 2.489501  [11200/12672]\n",
      "AUC: 0.4774086177680949\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.978452  [    0/12672]\n",
      "loss: 2.072610  [ 1600/12672]\n",
      "loss: 4.882101  [ 3200/12672]\n",
      "loss: 2.138558  [ 4800/12672]\n",
      "loss: 1.765347  [ 6400/12672]\n",
      "loss: 1.620879  [ 8000/12672]\n",
      "loss: 2.584701  [ 9600/12672]\n",
      "loss: 2.903093  [11200/12672]\n",
      "AUC: 0.4807007988380537\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.199480  [    0/12672]\n",
      "loss: 0.690443  [ 1600/12672]\n",
      "loss: 0.695056  [ 3200/12672]\n",
      "loss: 0.674293  [ 4800/12672]\n",
      "loss: 0.676909  [ 6400/12672]\n",
      "loss: 0.676827  [ 8000/12672]\n",
      "loss: 0.704472  [ 9600/12672]\n",
      "loss: 0.666424  [11200/12672]\n",
      "AUC: 0.5320624546114742\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.690030  [    0/12672]\n",
      "loss: 0.683837  [ 1600/12672]\n",
      "loss: 0.680126  [ 3200/12672]\n",
      "loss: 0.708632  [ 4800/12672]\n",
      "loss: 0.675494  [ 6400/12672]\n",
      "loss: 0.676944  [ 8000/12672]\n",
      "loss: 0.684855  [ 9600/12672]\n",
      "loss: 0.670525  [11200/12672]\n",
      "AUC: 0.5330549503752118\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.671289  [    0/12672]\n",
      "loss: 0.688272  [ 1600/12672]\n",
      "loss: 0.673063  [ 3200/12672]\n",
      "loss: 0.676546  [ 4800/12672]\n",
      "loss: 0.687089  [ 6400/12672]\n",
      "loss: 0.690334  [ 8000/12672]\n",
      "loss: 0.693224  [ 9600/12672]\n",
      "loss: 0.682850  [11200/12672]\n",
      "AUC: 0.49094650205761314\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.703500  [    0/12672]\n",
      "loss: 0.678979  [ 1600/12672]\n",
      "loss: 0.688859  [ 3200/12672]\n",
      "loss: 0.673834  [ 4800/12672]\n",
      "loss: 0.684518  [ 6400/12672]\n",
      "loss: 0.687478  [ 8000/12672]\n",
      "loss: 0.681975  [ 9600/12672]\n",
      "loss: 0.685575  [11200/12672]\n",
      "AUC: 0.5204429920116194\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.687446  [    0/12672]\n",
      "loss: 0.680335  [ 1600/12672]\n",
      "loss: 0.691870  [ 3200/12672]\n",
      "loss: 0.683889  [ 4800/12672]\n",
      "loss: 0.682253  [ 6400/12672]\n",
      "loss: 0.680740  [ 8000/12672]\n",
      "loss: 0.674920  [ 9600/12672]\n",
      "loss: 0.680277  [11200/12672]\n",
      "AUC: 0.529520697167756\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.753539  [    0/12672]\n",
      "loss: 2.460051  [ 1600/12672]\n",
      "loss: 1.977108  [ 3200/12672]\n",
      "loss: 2.592093  [ 4800/12672]\n",
      "loss: 2.019946  [ 6400/12672]\n",
      "loss: 2.326581  [ 8000/12672]\n",
      "loss: 2.854949  [ 9600/12672]\n",
      "loss: 2.872136  [11200/12672]\n",
      "AUC: 0.5102905273437499\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.920854  [    0/12672]\n",
      "loss: 3.044145  [ 1600/12672]\n",
      "loss: 2.334139  [ 3200/12672]\n",
      "loss: 1.680641  [ 4800/12672]\n",
      "loss: 1.828277  [ 6400/12672]\n",
      "loss: 1.674152  [ 8000/12672]\n",
      "loss: 2.132528  [ 9600/12672]\n",
      "loss: 2.028266  [11200/12672]\n",
      "AUC: 0.504833984375\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.707968  [    0/12672]\n",
      "loss: 2.353799  [ 1600/12672]\n",
      "loss: 1.415029  [ 3200/12672]\n",
      "loss: 1.519116  [ 4800/12672]\n",
      "loss: 3.204559  [ 6400/12672]\n",
      "loss: 1.984975  [ 8000/12672]\n",
      "loss: 2.920211  [ 9600/12672]\n",
      "loss: 2.600893  [11200/12672]\n",
      "AUC: 0.51971435546875\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.499947  [    0/12672]\n",
      "loss: 2.287203  [ 1600/12672]\n",
      "loss: 2.093974  [ 3200/12672]\n",
      "loss: 2.035944  [ 4800/12672]\n",
      "loss: 2.086892  [ 6400/12672]\n",
      "loss: 2.824918  [ 8000/12672]\n",
      "loss: 2.359528  [ 9600/12672]\n",
      "loss: 2.909451  [11200/12672]\n",
      "AUC: 0.5173583984375001\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.701330  [    0/12672]\n",
      "loss: 2.254435  [ 1600/12672]\n",
      "loss: 1.905162  [ 3200/12672]\n",
      "loss: 2.256475  [ 4800/12672]\n",
      "loss: 2.370613  [ 6400/12672]\n",
      "loss: 2.625518  [ 8000/12672]\n",
      "loss: 2.395669  [ 9600/12672]\n",
      "loss: 1.945458  [11200/12672]\n",
      "AUC: 0.5186767578125\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.287862  [    0/12672]\n",
      "loss: 0.701449  [ 1600/12672]\n",
      "loss: 0.693141  [ 3200/12672]\n",
      "loss: 0.690194  [ 4800/12672]\n",
      "loss: 0.690271  [ 6400/12672]\n",
      "loss: 0.689042  [ 8000/12672]\n",
      "loss: 0.679337  [ 9600/12672]\n",
      "loss: 0.679634  [11200/12672]\n",
      "AUC: 0.57864990234375\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.687339  [    0/12672]\n",
      "loss: 0.691344  [ 1600/12672]\n",
      "loss: 0.665609  [ 3200/12672]\n",
      "loss: 0.686292  [ 4800/12672]\n",
      "loss: 0.662482  [ 6400/12672]\n",
      "loss: 0.682026  [ 8000/12672]\n",
      "loss: 0.677222  [ 9600/12672]\n",
      "loss: 0.672027  [11200/12672]\n",
      "AUC: 0.5573974609375\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.700365  [    0/12672]\n",
      "loss: 0.673137  [ 1600/12672]\n",
      "loss: 0.673426  [ 3200/12672]\n",
      "loss: 0.671762  [ 4800/12672]\n",
      "loss: 0.673527  [ 6400/12672]\n",
      "loss: 0.688960  [ 8000/12672]\n",
      "loss: 0.677180  [ 9600/12672]\n",
      "loss: 0.685111  [11200/12672]\n",
      "AUC: 0.61005859375\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.680067  [    0/12672]\n",
      "loss: 0.691378  [ 1600/12672]\n",
      "loss: 0.677288  [ 3200/12672]\n",
      "loss: 0.686132  [ 4800/12672]\n",
      "loss: 0.687478  [ 6400/12672]\n",
      "loss: 0.678615  [ 8000/12672]\n",
      "loss: 0.670937  [ 9600/12672]\n",
      "loss: 0.692898  [11200/12672]\n",
      "AUC: 0.5777587890625\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.681829  [    0/12672]\n",
      "loss: 0.689682  [ 1600/12672]\n",
      "loss: 0.675714  [ 3200/12672]\n",
      "loss: 0.677650  [ 4800/12672]\n",
      "loss: 0.681353  [ 6400/12672]\n",
      "loss: 0.671684  [ 8000/12672]\n",
      "loss: 0.674690  [ 9600/12672]\n",
      "loss: 0.687045  [11200/12672]\n",
      "AUC: 0.597314453125\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.760653  [    0/12672]\n",
      "loss: 1.781553  [ 1600/12672]\n",
      "loss: 2.110374  [ 3200/12672]\n",
      "loss: 3.208347  [ 4800/12672]\n",
      "loss: 2.892700  [ 6400/12672]\n",
      "loss: 2.162598  [ 8000/12672]\n",
      "loss: 2.221037  [ 9600/12672]\n",
      "loss: 3.053047  [11200/12672]\n",
      "AUC: 0.42619332909473273\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.114813  [    0/12672]\n",
      "loss: 1.901425  [ 1600/12672]\n",
      "loss: 1.993603  [ 3200/12672]\n",
      "loss: 2.900810  [ 4800/12672]\n",
      "loss: 3.299647  [ 6400/12672]\n",
      "loss: 2.352026  [ 8000/12672]\n",
      "loss: 2.076504  [ 9600/12672]\n",
      "loss: 2.723894  [11200/12672]\n",
      "AUC: 0.42245806230742894\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.309948  [    0/12672]\n",
      "loss: 1.898982  [ 1600/12672]\n",
      "loss: 2.402706  [ 3200/12672]\n",
      "loss: 3.723909  [ 4800/12672]\n",
      "loss: 2.418113  [ 6400/12672]\n",
      "loss: 2.136762  [ 8000/12672]\n",
      "loss: 5.975776  [ 9600/12672]\n",
      "loss: 2.204949  [11200/12672]\n",
      "AUC: 0.42617498899594075\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.612441  [    0/12672]\n",
      "loss: 2.925276  [ 1600/12672]\n",
      "loss: 3.340799  [ 3200/12672]\n",
      "loss: 1.863398  [ 4800/12672]\n",
      "loss: 4.549307  [ 6400/12672]\n",
      "loss: 2.451219  [ 8000/12672]\n",
      "loss: 2.303280  [ 9600/12672]\n",
      "loss: 2.295763  [11200/12672]\n",
      "AUC: 0.4251479434635888\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.668925  [    0/12672]\n",
      "loss: 2.282473  [ 1600/12672]\n",
      "loss: 2.609217  [ 3200/12672]\n",
      "loss: 4.600268  [ 4800/12672]\n",
      "loss: 3.171481  [ 6400/12672]\n",
      "loss: 2.540161  [ 8000/12672]\n",
      "loss: 2.973692  [ 9600/12672]\n",
      "loss: 2.739086  [11200/12672]\n",
      "AUC: 0.44440504719518753\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.388601  [    0/12672]\n",
      "loss: 0.731104  [ 1600/12672]\n",
      "loss: 0.709865  [ 3200/12672]\n",
      "loss: 0.682690  [ 4800/12672]\n",
      "loss: 0.688603  [ 6400/12672]\n",
      "loss: 0.683230  [ 8000/12672]\n",
      "loss: 0.694795  [ 9600/12672]\n",
      "loss: 0.679983  [11200/12672]\n",
      "AUC: 0.5276690957108623\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.686509  [    0/12672]\n",
      "loss: 0.675255  [ 1600/12672]\n",
      "loss: 0.688427  [ 3200/12672]\n",
      "loss: 0.694038  [ 4800/12672]\n",
      "loss: 0.675750  [ 6400/12672]\n",
      "loss: 0.688103  [ 8000/12672]\n",
      "loss: 0.682250  [ 9600/12672]\n",
      "loss: 0.670761  [11200/12672]\n",
      "AUC: 0.5368024649092776\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.692459  [    0/12672]\n",
      "loss: 0.687330  [ 1600/12672]\n",
      "loss: 0.678787  [ 3200/12672]\n",
      "loss: 0.685304  [ 4800/12672]\n",
      "loss: 0.678705  [ 6400/12672]\n",
      "loss: 0.698699  [ 8000/12672]\n",
      "loss: 0.676828  [ 9600/12672]\n",
      "loss: 0.674504  [11200/12672]\n",
      "AUC: 0.5269966254218222\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.668795  [    0/12672]\n",
      "loss: 0.706875  [ 1600/12672]\n",
      "loss: 0.677813  [ 3200/12672]\n",
      "loss: 0.669793  [ 4800/12672]\n",
      "loss: 0.686868  [ 6400/12672]\n",
      "loss: 0.680091  [ 8000/12672]\n",
      "loss: 0.682871  [ 9600/12672]\n",
      "loss: 0.695258  [11200/12672]\n",
      "AUC: 0.5442363182863011\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.678372  [    0/12672]\n",
      "loss: 0.679214  [ 1600/12672]\n",
      "loss: 0.680534  [ 3200/12672]\n",
      "loss: 0.687452  [ 4800/12672]\n",
      "loss: 0.683290  [ 6400/12672]\n",
      "loss: 0.666775  [ 8000/12672]\n",
      "loss: 0.681309  [ 9600/12672]\n",
      "loss: 0.690107  [11200/12672]\n",
      "AUC: 0.5247346799041425\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.704859  [    0/12672]\n",
      "loss: 3.383421  [ 1600/12672]\n",
      "loss: 1.614233  [ 3200/12672]\n",
      "loss: 2.171172  [ 4800/12672]\n",
      "loss: 2.248076  [ 6400/12672]\n",
      "loss: 1.700836  [ 8000/12672]\n",
      "loss: 2.518138  [ 9600/12672]\n",
      "loss: 1.911641  [11200/12672]\n",
      "AUC: 0.510408856900085\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.545510  [    0/12672]\n",
      "loss: 1.998750  [ 1600/12672]\n",
      "loss: 1.866885  [ 3200/12672]\n",
      "loss: 4.068071  [ 4800/12672]\n",
      "loss: 1.966159  [ 6400/12672]\n",
      "loss: 2.277544  [ 8000/12672]\n",
      "loss: 2.529150  [ 9600/12672]\n",
      "loss: 2.289493  [11200/12672]\n",
      "AUC: 0.48473034437946716\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.188185  [    0/12672]\n",
      "loss: 3.683742  [ 1600/12672]\n",
      "loss: 3.898065  [ 3200/12672]\n",
      "loss: 1.889963  [ 4800/12672]\n",
      "loss: 2.699161  [ 6400/12672]\n",
      "loss: 1.376885  [ 8000/12672]\n",
      "loss: 4.347106  [ 9600/12672]\n",
      "loss: 3.142018  [11200/12672]\n",
      "AUC: 0.5059854051082121\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.070385  [    0/12672]\n",
      "loss: 2.473736  [ 1600/12672]\n",
      "loss: 2.452034  [ 3200/12672]\n",
      "loss: 3.454297  [ 4800/12672]\n",
      "loss: 1.695019  [ 6400/12672]\n",
      "loss: 3.251040  [ 8000/12672]\n",
      "loss: 2.852823  [ 9600/12672]\n",
      "loss: 3.939052  [11200/12672]\n",
      "AUC: 0.47542735042735046\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 5.637913  [    0/12672]\n",
      "loss: 2.840918  [ 1600/12672]\n",
      "loss: 3.388091  [ 3200/12672]\n",
      "loss: 2.023268  [ 4800/12672]\n",
      "loss: 1.670656  [ 6400/12672]\n",
      "loss: 2.765919  [ 8000/12672]\n",
      "loss: 2.868331  [ 9600/12672]\n",
      "loss: 3.057916  [11200/12672]\n",
      "AUC: 0.47850752236717153\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 4.455945  [    0/12672]\n",
      "loss: 0.716985  [ 1600/12672]\n",
      "loss: 0.664755  [ 3200/12672]\n",
      "loss: 0.681020  [ 4800/12672]\n",
      "loss: 0.684838  [ 6400/12672]\n",
      "loss: 0.671914  [ 8000/12672]\n",
      "loss: 0.685748  [ 9600/12672]\n",
      "loss: 0.694226  [11200/12672]\n",
      "AUC: 0.5583170890188435\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.672903  [    0/12672]\n",
      "loss: 0.676897  [ 1600/12672]\n",
      "loss: 0.677917  [ 3200/12672]\n",
      "loss: 0.683701  [ 4800/12672]\n",
      "loss: 0.679582  [ 6400/12672]\n",
      "loss: 0.692583  [ 8000/12672]\n",
      "loss: 0.678020  [ 9600/12672]\n",
      "loss: 0.697557  [11200/12672]\n",
      "AUC: 0.5613910131453991\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.691479  [    0/12672]\n",
      "loss: 0.676927  [ 1600/12672]\n",
      "loss: 0.671722  [ 3200/12672]\n",
      "loss: 0.690084  [ 4800/12672]\n",
      "loss: 0.677052  [ 6400/12672]\n",
      "loss: 0.681956  [ 8000/12672]\n",
      "loss: 0.692742  [ 9600/12672]\n",
      "loss: 0.688956  [11200/12672]\n",
      "AUC: 0.5741740390863197\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.688335  [    0/12672]\n",
      "loss: 0.688997  [ 1600/12672]\n",
      "loss: 0.669995  [ 3200/12672]\n",
      "loss: 0.686704  [ 4800/12672]\n",
      "loss: 0.678379  [ 6400/12672]\n",
      "loss: 0.683174  [ 8000/12672]\n",
      "loss: 0.695545  [ 9600/12672]\n",
      "loss: 0.680485  [11200/12672]\n",
      "AUC: 0.5497076023391814\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.682228  [    0/12672]\n",
      "loss: 0.681754  [ 1600/12672]\n",
      "loss: 0.665759  [ 3200/12672]\n",
      "loss: 0.699880  [ 4800/12672]\n",
      "loss: 0.680377  [ 6400/12672]\n",
      "loss: 0.672390  [ 8000/12672]\n",
      "loss: 0.680447  [ 9600/12672]\n",
      "loss: 0.683104  [11200/12672]\n",
      "AUC: 0.5580546808616984\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.733163  [    0/12672]\n",
      "loss: 1.410591  [ 1600/12672]\n",
      "loss: 2.152969  [ 3200/12672]\n",
      "loss: 2.699235  [ 4800/12672]\n",
      "loss: 3.983548  [ 6400/12672]\n",
      "loss: 2.514612  [ 8000/12672]\n",
      "loss: 3.189413  [ 9600/12672]\n",
      "loss: 2.309278  [11200/12672]\n",
      "AUC: 0.48241245417711753\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.802881  [    0/12672]\n",
      "loss: 4.057206  [ 1600/12672]\n",
      "loss: 3.129144  [ 3200/12672]\n",
      "loss: 2.740271  [ 4800/12672]\n",
      "loss: 1.995211  [ 6400/12672]\n",
      "loss: 2.248213  [ 8000/12672]\n",
      "loss: 2.138072  [ 9600/12672]\n",
      "loss: 2.445448  [11200/12672]\n",
      "AUC: 0.4854693227860313\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.546207  [    0/12672]\n",
      "loss: 3.091577  [ 1600/12672]\n",
      "loss: 3.579604  [ 3200/12672]\n",
      "loss: 2.852892  [ 4800/12672]\n",
      "loss: 3.119250  [ 6400/12672]\n",
      "loss: 1.806231  [ 8000/12672]\n",
      "loss: 2.377734  [ 9600/12672]\n",
      "loss: 3.098856  [11200/12672]\n",
      "AUC: 0.47180686860891374\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.237097  [    0/12672]\n",
      "loss: 2.790263  [ 1600/12672]\n",
      "loss: 3.941064  [ 3200/12672]\n",
      "loss: 3.054617  [ 4800/12672]\n",
      "loss: 2.000910  [ 6400/12672]\n",
      "loss: 1.908430  [ 8000/12672]\n",
      "loss: 3.308382  [ 9600/12672]\n",
      "loss: 2.202774  [11200/12672]\n",
      "AUC: 0.4884839861084314\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.560836  [    0/12672]\n",
      "loss: 3.477830  [ 1600/12672]\n",
      "loss: 2.681397  [ 3200/12672]\n",
      "loss: 2.346396  [ 4800/12672]\n",
      "loss: 2.487808  [ 6400/12672]\n",
      "loss: 5.215377  [ 8000/12672]\n",
      "loss: 2.258996  [ 9600/12672]\n",
      "loss: 2.197780  [11200/12672]\n",
      "AUC: 0.48020572062512057\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.999564  [    0/12672]\n",
      "loss: 0.686576  [ 1600/12672]\n",
      "loss: 0.681865  [ 3200/12672]\n",
      "loss: 0.692258  [ 4800/12672]\n",
      "loss: 0.685926  [ 6400/12672]\n",
      "loss: 0.674179  [ 8000/12672]\n",
      "loss: 0.674641  [ 9600/12672]\n",
      "loss: 0.681835  [11200/12672]\n",
      "AUC: 0.5247443565502605\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.689862  [    0/12672]\n",
      "loss: 0.686598  [ 1600/12672]\n",
      "loss: 0.679347  [ 3200/12672]\n",
      "loss: 0.686002  [ 4800/12672]\n",
      "loss: 0.681109  [ 6400/12672]\n",
      "loss: 0.683662  [ 8000/12672]\n",
      "loss: 0.690145  [ 9600/12672]\n",
      "loss: 0.675623  [11200/12672]\n",
      "AUC: 0.5326065984950801\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.691334  [    0/12672]\n",
      "loss: 0.685405  [ 1600/12672]\n",
      "loss: 0.683896  [ 3200/12672]\n",
      "loss: 0.687221  [ 4800/12672]\n",
      "loss: 0.685962  [ 6400/12672]\n",
      "loss: 0.678914  [ 8000/12672]\n",
      "loss: 0.702324  [ 9600/12672]\n",
      "loss: 0.683321  [11200/12672]\n",
      "AUC: 0.5108648466139302\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.674302  [    0/12672]\n",
      "loss: 0.675454  [ 1600/12672]\n",
      "loss: 0.689133  [ 3200/12672]\n",
      "loss: 0.697062  [ 4800/12672]\n",
      "loss: 0.683867  [ 6400/12672]\n",
      "loss: 0.684207  [ 8000/12672]\n",
      "loss: 0.686358  [ 9600/12672]\n",
      "loss: 0.674610  [11200/12672]\n",
      "AUC: 0.500663225930928\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.674615  [    0/12672]\n",
      "loss: 0.677679  [ 1600/12672]\n",
      "loss: 0.689792  [ 3200/12672]\n",
      "loss: 0.685798  [ 4800/12672]\n",
      "loss: 0.675329  [ 6400/12672]\n",
      "loss: 0.670393  [ 8000/12672]\n",
      "loss: 0.687519  [ 9600/12672]\n",
      "loss: 0.681233  [11200/12672]\n",
      "AUC: 0.5064754968165155\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.725233  [    0/12672]\n",
      "loss: 2.179183  [ 1600/12672]\n",
      "loss: 1.910993  [ 3200/12672]\n",
      "loss: 2.758301  [ 4800/12672]\n",
      "loss: 2.739532  [ 6400/12672]\n",
      "loss: 2.005902  [ 8000/12672]\n",
      "loss: 2.576463  [ 9600/12672]\n",
      "loss: 2.818312  [11200/12672]\n",
      "AUC: 0.5352272727272727\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.082984  [    0/12672]\n",
      "loss: 2.186602  [ 1600/12672]\n",
      "loss: 2.459157  [ 3200/12672]\n",
      "loss: 2.289084  [ 4800/12672]\n",
      "loss: 1.856130  [ 6400/12672]\n",
      "loss: 2.676562  [ 8000/12672]\n",
      "loss: 3.064370  [ 9600/12672]\n",
      "loss: 2.209818  [11200/12672]\n",
      "AUC: 0.5287538304392236\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.334239  [    0/12672]\n",
      "loss: 2.999068  [ 1600/12672]\n",
      "loss: 2.430852  [ 3200/12672]\n",
      "loss: 2.703245  [ 4800/12672]\n",
      "loss: 2.071851  [ 6400/12672]\n",
      "loss: 1.408194  [ 8000/12672]\n",
      "loss: 1.863741  [ 9600/12672]\n",
      "loss: 1.558187  [11200/12672]\n",
      "AUC: 0.5289964249233912\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.781833  [    0/12672]\n",
      "loss: 2.115753  [ 1600/12672]\n",
      "loss: 1.916286  [ 3200/12672]\n",
      "loss: 2.549547  [ 4800/12672]\n",
      "loss: 2.328056  [ 6400/12672]\n",
      "loss: 2.304892  [ 8000/12672]\n",
      "loss: 2.191816  [ 9600/12672]\n",
      "loss: 2.359732  [11200/12672]\n",
      "AUC: 0.5230911644535239\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 4.322006  [    0/12672]\n",
      "loss: 2.685873  [ 1600/12672]\n",
      "loss: 2.027153  [ 3200/12672]\n",
      "loss: 1.868324  [ 4800/12672]\n",
      "loss: 2.963211  [ 6400/12672]\n",
      "loss: 2.206433  [ 8000/12672]\n",
      "loss: 2.014700  [ 9600/12672]\n",
      "loss: 2.586468  [11200/12672]\n",
      "AUC: 0.5500510725229827\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.438398  [    0/12672]\n",
      "loss: 0.730509  [ 1600/12672]\n",
      "loss: 0.698475  [ 3200/12672]\n",
      "loss: 0.679074  [ 4800/12672]\n",
      "loss: 0.684416  [ 6400/12672]\n",
      "loss: 0.657973  [ 8000/12672]\n",
      "loss: 0.681244  [ 9600/12672]\n",
      "loss: 0.690390  [11200/12672]\n",
      "AUC: 0.6000893769152196\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.684831  [    0/12672]\n",
      "loss: 0.698224  [ 1600/12672]\n",
      "loss: 0.681979  [ 3200/12672]\n",
      "loss: 0.690175  [ 4800/12672]\n",
      "loss: 0.683202  [ 6400/12672]\n",
      "loss: 0.688474  [ 8000/12672]\n",
      "loss: 0.678620  [ 9600/12672]\n",
      "loss: 0.675764  [11200/12672]\n",
      "AUC: 0.5915858018386108\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.681911  [    0/12672]\n",
      "loss: 0.681082  [ 1600/12672]\n",
      "loss: 0.681785  [ 3200/12672]\n",
      "loss: 0.677454  [ 4800/12672]\n",
      "loss: 0.685825  [ 6400/12672]\n",
      "loss: 0.671923  [ 8000/12672]\n",
      "loss: 0.673142  [ 9600/12672]\n",
      "loss: 0.675797  [11200/12672]\n",
      "AUC: 0.5911389172625128\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.686539  [    0/12672]\n",
      "loss: 0.691242  [ 1600/12672]\n",
      "loss: 0.679256  [ 3200/12672]\n",
      "loss: 0.666348  [ 4800/12672]\n",
      "loss: 0.683919  [ 6400/12672]\n",
      "loss: 0.700214  [ 8000/12672]\n",
      "loss: 0.700574  [ 9600/12672]\n",
      "loss: 0.689614  [11200/12672]\n",
      "AUC: 0.5973059244126661\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.672103  [    0/12672]\n",
      "loss: 0.686832  [ 1600/12672]\n",
      "loss: 0.693130  [ 3200/12672]\n",
      "loss: 0.686147  [ 4800/12672]\n",
      "loss: 0.689511  [ 6400/12672]\n",
      "loss: 0.685865  [ 8000/12672]\n",
      "loss: 0.683599  [ 9600/12672]\n",
      "loss: 0.685900  [11200/12672]\n",
      "AUC: 0.6026430030643514\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.754130  [    0/12672]\n",
      "loss: 2.541938  [ 1600/12672]\n",
      "loss: 2.402936  [ 3200/12672]\n",
      "loss: 2.746822  [ 4800/12672]\n",
      "loss: 2.332322  [ 6400/12672]\n",
      "loss: 1.426862  [ 8000/12672]\n",
      "loss: 4.481506  [ 9600/12672]\n",
      "loss: 2.180653  [11200/12672]\n",
      "AUC: 0.5359172482552342\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.594851  [    0/12672]\n",
      "loss: 2.657850  [ 1600/12672]\n",
      "loss: 2.898392  [ 3200/12672]\n",
      "loss: 2.311502  [ 4800/12672]\n",
      "loss: 2.142169  [ 6400/12672]\n",
      "loss: 1.681278  [ 8000/12672]\n",
      "loss: 2.287830  [ 9600/12672]\n",
      "loss: 2.845898  [11200/12672]\n",
      "AUC: 0.5350760219341975\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.740413  [    0/12672]\n",
      "loss: 3.469613  [ 1600/12672]\n",
      "loss: 1.845939  [ 3200/12672]\n",
      "loss: 2.899312  [ 4800/12672]\n",
      "loss: 2.321338  [ 6400/12672]\n",
      "loss: 2.412082  [ 8000/12672]\n",
      "loss: 2.153044  [ 9600/12672]\n",
      "loss: 2.018315  [11200/12672]\n",
      "AUC: 0.5310319042871385\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.394260  [    0/12672]\n",
      "loss: 1.594343  [ 1600/12672]\n",
      "loss: 2.372900  [ 3200/12672]\n",
      "loss: 2.009075  [ 4800/12672]\n",
      "loss: 3.095984  [ 6400/12672]\n",
      "loss: 2.571765  [ 8000/12672]\n",
      "loss: 2.510613  [ 9600/12672]\n",
      "loss: 2.210886  [11200/12672]\n",
      "AUC: 0.5322781655034896\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.891382  [    0/12672]\n",
      "loss: 2.285049  [ 1600/12672]\n",
      "loss: 2.072846  [ 3200/12672]\n",
      "loss: 1.897807  [ 4800/12672]\n",
      "loss: 2.236358  [ 6400/12672]\n",
      "loss: 2.987932  [ 8000/12672]\n",
      "loss: 2.308033  [ 9600/12672]\n",
      "loss: 2.433686  [11200/12672]\n",
      "AUC: 0.5520937188434697\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.155593  [    0/12672]\n",
      "loss: 0.704308  [ 1600/12672]\n",
      "loss: 0.695576  [ 3200/12672]\n",
      "loss: 0.671326  [ 4800/12672]\n",
      "loss: 0.678287  [ 6400/12672]\n",
      "loss: 0.682750  [ 8000/12672]\n",
      "loss: 0.682807  [ 9600/12672]\n",
      "loss: 0.684154  [11200/12672]\n",
      "AUC: 0.5629860418743768\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.672543  [    0/12672]\n",
      "loss: 0.679302  [ 1600/12672]\n",
      "loss: 0.665876  [ 3200/12672]\n",
      "loss: 0.677718  [ 4800/12672]\n",
      "loss: 0.674640  [ 6400/12672]\n",
      "loss: 0.687558  [ 8000/12672]\n",
      "loss: 0.677791  [ 9600/12672]\n",
      "loss: 0.686172  [11200/12672]\n",
      "AUC: 0.5828015952143569\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.687678  [    0/12672]\n",
      "loss: 0.693279  [ 1600/12672]\n",
      "loss: 0.684091  [ 3200/12672]\n",
      "loss: 0.678419  [ 4800/12672]\n",
      "loss: 0.670793  [ 6400/12672]\n",
      "loss: 0.684272  [ 8000/12672]\n",
      "loss: 0.677276  [ 9600/12672]\n",
      "loss: 0.694585  [11200/12672]\n",
      "AUC: 0.5601071784646062\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.694842  [    0/12672]\n",
      "loss: 0.693567  [ 1600/12672]\n",
      "loss: 0.673220  [ 3200/12672]\n",
      "loss: 0.685269  [ 4800/12672]\n",
      "loss: 0.680524  [ 6400/12672]\n",
      "loss: 0.666894  [ 8000/12672]\n",
      "loss: 0.672156  [ 9600/12672]\n",
      "loss: 0.671763  [11200/12672]\n",
      "AUC: 0.5728439680957128\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.680717  [    0/12672]\n",
      "loss: 0.675672  [ 1600/12672]\n",
      "loss: 0.684815  [ 3200/12672]\n",
      "loss: 0.688122  [ 4800/12672]\n",
      "loss: 0.691838  [ 6400/12672]\n",
      "loss: 0.678032  [ 8000/12672]\n",
      "loss: 0.682927  [ 9600/12672]\n",
      "loss: 0.673408  [11200/12672]\n",
      "AUC: 0.5719092721834497\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.736697  [    0/12672]\n",
      "loss: 1.455632  [ 1600/12672]\n",
      "loss: 2.755514  [ 3200/12672]\n",
      "loss: 1.929578  [ 4800/12672]\n",
      "loss: 3.355920  [ 6400/12672]\n",
      "loss: 2.739186  [ 8000/12672]\n",
      "loss: 3.190937  [ 9600/12672]\n",
      "loss: 2.091282  [11200/12672]\n",
      "AUC: 0.44295199182839634\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.720748  [    0/12672]\n",
      "loss: 3.390157  [ 1600/12672]\n",
      "loss: 2.311866  [ 3200/12672]\n",
      "loss: 1.790423  [ 4800/12672]\n",
      "loss: 2.092278  [ 6400/12672]\n",
      "loss: 1.969910  [ 8000/12672]\n",
      "loss: 3.498100  [ 9600/12672]\n",
      "loss: 2.452907  [11200/12672]\n",
      "AUC: 0.45124489274770163\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 8.293344  [    0/12672]\n",
      "loss: 1.928675  [ 1600/12672]\n",
      "loss: 2.890795  [ 3200/12672]\n",
      "loss: 3.076265  [ 4800/12672]\n",
      "loss: 1.564316  [ 6400/12672]\n",
      "loss: 4.658322  [ 8000/12672]\n",
      "loss: 3.717896  [ 9600/12672]\n",
      "loss: 3.194772  [11200/12672]\n",
      "AUC: 0.4392620020429009\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 4.358557  [    0/12672]\n",
      "loss: 2.625195  [ 1600/12672]\n",
      "loss: 1.538989  [ 3200/12672]\n",
      "loss: 1.708174  [ 4800/12672]\n",
      "loss: 2.467146  [ 6400/12672]\n",
      "loss: 3.856287  [ 8000/12672]\n",
      "loss: 2.485885  [ 9600/12672]\n",
      "loss: 1.081167  [11200/12672]\n",
      "AUC: 0.4465653728294177\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.626724  [    0/12672]\n",
      "loss: 1.926644  [ 1600/12672]\n",
      "loss: 2.824393  [ 3200/12672]\n",
      "loss: 2.185645  [ 4800/12672]\n",
      "loss: 1.831344  [ 6400/12672]\n",
      "loss: 5.649817  [ 8000/12672]\n",
      "loss: 1.932839  [ 9600/12672]\n",
      "loss: 3.826273  [11200/12672]\n",
      "AUC: 0.45574565883554646\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.739473  [    0/12672]\n",
      "loss: 0.730412  [ 1600/12672]\n",
      "loss: 0.680586  [ 3200/12672]\n",
      "loss: 0.683407  [ 4800/12672]\n",
      "loss: 0.683093  [ 6400/12672]\n",
      "loss: 0.670515  [ 8000/12672]\n",
      "loss: 0.680596  [ 9600/12672]\n",
      "loss: 0.673580  [11200/12672]\n",
      "AUC: 0.5561159346271706\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.680497  [    0/12672]\n",
      "loss: 0.683900  [ 1600/12672]\n",
      "loss: 0.681862  [ 3200/12672]\n",
      "loss: 0.677299  [ 4800/12672]\n",
      "loss: 0.686357  [ 6400/12672]\n",
      "loss: 0.697472  [ 8000/12672]\n",
      "loss: 0.679971  [ 9600/12672]\n",
      "loss: 0.677025  [11200/12672]\n",
      "AUC: 0.5609295199182839\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.690576  [    0/12672]\n",
      "loss: 0.685525  [ 1600/12672]\n",
      "loss: 0.682388  [ 3200/12672]\n",
      "loss: 0.667714  [ 4800/12672]\n",
      "loss: 0.683412  [ 6400/12672]\n",
      "loss: 0.686263  [ 8000/12672]\n",
      "loss: 0.663379  [ 9600/12672]\n",
      "loss: 0.678121  [11200/12672]\n",
      "AUC: 0.5696246169560777\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.667578  [    0/12672]\n",
      "loss: 0.703735  [ 1600/12672]\n",
      "loss: 0.680035  [ 3200/12672]\n",
      "loss: 0.677198  [ 4800/12672]\n",
      "loss: 0.674167  [ 6400/12672]\n",
      "loss: 0.669263  [ 8000/12672]\n",
      "loss: 0.685612  [ 9600/12672]\n",
      "loss: 0.677715  [11200/12672]\n",
      "AUC: 0.5576225740551584\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.662701  [    0/12672]\n",
      "loss: 0.654063  [ 1600/12672]\n",
      "loss: 0.675571  [ 3200/12672]\n",
      "loss: 0.690289  [ 4800/12672]\n",
      "loss: 0.666266  [ 6400/12672]\n",
      "loss: 0.688185  [ 8000/12672]\n",
      "loss: 0.674111  [ 9600/12672]\n",
      "loss: 0.682021  [11200/12672]\n",
      "AUC: 0.5517492339121552\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.747092  [    0/12672]\n",
      "loss: 2.134240  [ 1600/12672]\n",
      "loss: 2.443061  [ 3200/12672]\n",
      "loss: 1.730206  [ 4800/12672]\n",
      "loss: 2.806847  [ 6400/12672]\n",
      "loss: 1.903388  [ 8000/12672]\n",
      "loss: 2.010165  [ 9600/12672]\n",
      "loss: 2.205505  [11200/12672]\n",
      "AUC: 0.48868654553409074\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.857345  [    0/12672]\n",
      "loss: 2.565056  [ 1600/12672]\n",
      "loss: 2.319701  [ 3200/12672]\n",
      "loss: 2.371959  [ 4800/12672]\n",
      "loss: 1.830356  [ 6400/12672]\n",
      "loss: 1.957260  [ 8000/12672]\n",
      "loss: 2.944755  [ 9600/12672]\n",
      "loss: 2.404234  [11200/12672]\n",
      "AUC: 0.464760811014041\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 4.926994  [    0/12672]\n",
      "loss: 1.950847  [ 1600/12672]\n",
      "loss: 1.821035  [ 3200/12672]\n",
      "loss: 2.953534  [ 4800/12672]\n",
      "loss: 2.656129  [ 6400/12672]\n",
      "loss: 1.419718  [ 8000/12672]\n",
      "loss: 4.149124  [ 9600/12672]\n",
      "loss: 2.255082  [11200/12672]\n",
      "AUC: 0.45230575721531796\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.842299  [    0/12672]\n",
      "loss: 3.020386  [ 1600/12672]\n",
      "loss: 2.680107  [ 3200/12672]\n",
      "loss: 2.355110  [ 4800/12672]\n",
      "loss: 2.069086  [ 6400/12672]\n",
      "loss: 2.094866  [ 8000/12672]\n",
      "loss: 2.017154  [ 9600/12672]\n",
      "loss: 2.069103  [11200/12672]\n",
      "AUC: 0.4611514430635877\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.357656  [    0/12672]\n",
      "loss: 2.260125  [ 1600/12672]\n",
      "loss: 1.751516  [ 3200/12672]\n",
      "loss: 2.624945  [ 4800/12672]\n",
      "loss: 4.657305  [ 6400/12672]\n",
      "loss: 1.384911  [ 8000/12672]\n",
      "loss: 1.991750  [ 9600/12672]\n",
      "loss: 1.801338  [11200/12672]\n",
      "AUC: 0.4585332841146794\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.550550  [    0/12672]\n",
      "loss: 0.734039  [ 1600/12672]\n",
      "loss: 0.671116  [ 3200/12672]\n",
      "loss: 0.682967  [ 4800/12672]\n",
      "loss: 0.672268  [ 6400/12672]\n",
      "loss: 0.678532  [ 8000/12672]\n",
      "loss: 0.673705  [ 9600/12672]\n",
      "loss: 0.680717  [11200/12672]\n",
      "AUC: 0.5988953146575886\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.679924  [    0/12672]\n",
      "loss: 0.683611  [ 1600/12672]\n",
      "loss: 0.678849  [ 3200/12672]\n",
      "loss: 0.681704  [ 4800/12672]\n",
      "loss: 0.688862  [ 6400/12672]\n",
      "loss: 0.678283  [ 8000/12672]\n",
      "loss: 0.690421  [ 9600/12672]\n",
      "loss: 0.687956  [11200/12672]\n",
      "AUC: 0.5816414421065583\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.690609  [    0/12672]\n",
      "loss: 0.692962  [ 1600/12672]\n",
      "loss: 0.684245  [ 3200/12672]\n",
      "loss: 0.685172  [ 4800/12672]\n",
      "loss: 0.665831  [ 6400/12672]\n",
      "loss: 0.691624  [ 8000/12672]\n",
      "loss: 0.692538  [ 9600/12672]\n",
      "loss: 0.682412  [11200/12672]\n",
      "AUC: 0.574832861654567\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.680462  [    0/12672]\n",
      "loss: 0.689853  [ 1600/12672]\n",
      "loss: 0.698322  [ 3200/12672]\n",
      "loss: 0.702228  [ 4800/12672]\n",
      "loss: 0.675039  [ 6400/12672]\n",
      "loss: 0.674736  [ 8000/12672]\n",
      "loss: 0.692106  [ 9600/12672]\n",
      "loss: 0.680961  [11200/12672]\n",
      "AUC: 0.5889832246421394\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.695569  [    0/12672]\n",
      "loss: 0.660752  [ 1600/12672]\n",
      "loss: 0.682156  [ 3200/12672]\n",
      "loss: 0.675261  [ 4800/12672]\n",
      "loss: 0.678380  [ 6400/12672]\n",
      "loss: 0.681897  [ 8000/12672]\n",
      "loss: 0.672772  [ 9600/12672]\n",
      "loss: 0.682676  [11200/12672]\n",
      "AUC: 0.5868094007628891\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.762599  [    0/12672]\n",
      "loss: 3.109328  [ 1600/12672]\n",
      "loss: 2.203877  [ 3200/12672]\n",
      "loss: 2.072303  [ 4800/12672]\n",
      "loss: 3.624677  [ 6400/12672]\n",
      "loss: 3.923590  [ 8000/12672]\n",
      "loss: 1.967873  [ 9600/12672]\n",
      "loss: 3.514510  [11200/12672]\n",
      "AUC: 0.5344982078853047\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.102321  [    0/12672]\n",
      "loss: 1.907258  [ 1600/12672]\n",
      "loss: 1.556818  [ 3200/12672]\n",
      "loss: 2.280433  [ 4800/12672]\n",
      "loss: 2.334133  [ 6400/12672]\n",
      "loss: 3.210814  [ 8000/12672]\n",
      "loss: 3.180779  [ 9600/12672]\n",
      "loss: 2.865125  [11200/12672]\n",
      "AUC: 0.5500672043010754\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.912372  [    0/12672]\n",
      "loss: 1.780535  [ 1600/12672]\n",
      "loss: 1.779524  [ 3200/12672]\n",
      "loss: 2.340295  [ 4800/12672]\n",
      "loss: 2.340515  [ 6400/12672]\n",
      "loss: 2.004230  [ 8000/12672]\n",
      "loss: 2.034508  [ 9600/12672]\n",
      "loss: 2.752493  [11200/12672]\n",
      "AUC: 0.5422860004216741\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.625462  [    0/12672]\n",
      "loss: 3.492640  [ 1600/12672]\n",
      "loss: 1.486504  [ 3200/12672]\n",
      "loss: 1.835562  [ 4800/12672]\n",
      "loss: 3.050161  [ 6400/12672]\n",
      "loss: 2.866147  [ 8000/12672]\n",
      "loss: 2.145503  [ 9600/12672]\n",
      "loss: 2.096657  [11200/12672]\n",
      "AUC: 0.5515101201771031\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.030783  [    0/12672]\n",
      "loss: 3.332538  [ 1600/12672]\n",
      "loss: 4.459032  [ 3200/12672]\n",
      "loss: 2.288139  [ 4800/12672]\n",
      "loss: 2.425411  [ 6400/12672]\n",
      "loss: 1.900237  [ 8000/12672]\n",
      "loss: 1.952973  [ 9600/12672]\n",
      "loss: 1.955346  [11200/12672]\n",
      "AUC: 0.5284695867594349\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.256820  [    0/12672]\n",
      "loss: 0.703625  [ 1600/12672]\n",
      "loss: 0.693433  [ 3200/12672]\n",
      "loss: 0.671510  [ 4800/12672]\n",
      "loss: 0.687053  [ 6400/12672]\n",
      "loss: 0.670038  [ 8000/12672]\n",
      "loss: 0.682926  [ 9600/12672]\n",
      "loss: 0.681993  [11200/12672]\n",
      "AUC: 0.5704590976175417\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.667290  [    0/12672]\n",
      "loss: 0.686338  [ 1600/12672]\n",
      "loss: 0.675053  [ 3200/12672]\n",
      "loss: 0.679569  [ 4800/12672]\n",
      "loss: 0.676187  [ 6400/12672]\n",
      "loss: 0.680209  [ 8000/12672]\n",
      "loss: 0.693559  [ 9600/12672]\n",
      "loss: 0.687559  [11200/12672]\n",
      "AUC: 0.567652329749104\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.681580  [    0/12672]\n",
      "loss: 0.682389  [ 1600/12672]\n",
      "loss: 0.695546  [ 3200/12672]\n",
      "loss: 0.699278  [ 4800/12672]\n",
      "loss: 0.672103  [ 6400/12672]\n",
      "loss: 0.682924  [ 8000/12672]\n",
      "loss: 0.688692  [ 9600/12672]\n",
      "loss: 0.680273  [11200/12672]\n",
      "AUC: 0.5961285051655072\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.667543  [    0/12672]\n",
      "loss: 0.689716  [ 1600/12672]\n",
      "loss: 0.669873  [ 3200/12672]\n",
      "loss: 0.687924  [ 4800/12672]\n",
      "loss: 0.681795  [ 6400/12672]\n",
      "loss: 0.673977  [ 8000/12672]\n",
      "loss: 0.692535  [ 9600/12672]\n",
      "loss: 0.664317  [11200/12672]\n",
      "AUC: 0.5121494834492937\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.690346  [    0/12672]\n",
      "loss: 0.694836  [ 1600/12672]\n",
      "loss: 0.690349  [ 3200/12672]\n",
      "loss: 0.694001  [ 4800/12672]\n",
      "loss: 0.684490  [ 6400/12672]\n",
      "loss: 0.691564  [ 8000/12672]\n",
      "loss: 0.690221  [ 9600/12672]\n",
      "loss: 0.678353  [11200/12672]\n",
      "AUC: 0.6086469534050181\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.738194  [    0/12672]\n",
      "loss: 1.466269  [ 1600/12672]\n",
      "loss: 3.942967  [ 3200/12672]\n",
      "loss: 4.024822  [ 4800/12672]\n",
      "loss: 3.496381  [ 6400/12672]\n",
      "loss: 2.692997  [ 8000/12672]\n",
      "loss: 2.243608  [ 9600/12672]\n",
      "loss: 2.783739  [11200/12672]\n",
      "AUC: 0.4416491734452899\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.140113  [    0/12672]\n",
      "loss: 3.570752  [ 1600/12672]\n",
      "loss: 2.469289  [ 3200/12672]\n",
      "loss: 3.522411  [ 4800/12672]\n",
      "loss: 2.029549  [ 6400/12672]\n",
      "loss: 1.472578  [ 8000/12672]\n",
      "loss: 1.760391  [ 9600/12672]\n",
      "loss: 1.851732  [11200/12672]\n",
      "AUC: 0.493676200472317\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.122225  [    0/12672]\n",
      "loss: 2.313643  [ 1600/12672]\n",
      "loss: 2.683136  [ 3200/12672]\n",
      "loss: 3.463620  [ 4800/12672]\n",
      "loss: 5.374908  [ 6400/12672]\n",
      "loss: 2.428052  [ 8000/12672]\n",
      "loss: 2.381446  [ 9600/12672]\n",
      "loss: 3.304861  [11200/12672]\n",
      "AUC: 0.4755772763054317\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 8.153814  [    0/12672]\n",
      "loss: 3.065778  [ 1600/12672]\n",
      "loss: 4.163140  [ 3200/12672]\n",
      "loss: 1.921801  [ 4800/12672]\n",
      "loss: 3.977578  [ 6400/12672]\n",
      "loss: 2.792474  [ 8000/12672]\n",
      "loss: 2.365632  [ 9600/12672]\n",
      "loss: 4.625769  [11200/12672]\n",
      "AUC: 0.42595119391235897\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.489566  [    0/12672]\n",
      "loss: 2.695372  [ 1600/12672]\n",
      "loss: 2.220921  [ 3200/12672]\n",
      "loss: 2.500350  [ 4800/12672]\n",
      "loss: 2.393288  [ 6400/12672]\n",
      "loss: 3.099534  [ 8000/12672]\n",
      "loss: 3.627948  [ 9600/12672]\n",
      "loss: 4.574123  [11200/12672]\n",
      "AUC: 0.4778535817370768\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.585301  [    0/12672]\n",
      "loss: 0.695841  [ 1600/12672]\n",
      "loss: 0.689105  [ 3200/12672]\n",
      "loss: 0.689821  [ 4800/12672]\n",
      "loss: 0.680234  [ 6400/12672]\n",
      "loss: 0.719065  [ 8000/12672]\n",
      "loss: 0.676634  [ 9600/12672]\n",
      "loss: 0.675501  [11200/12672]\n",
      "AUC: 0.535791130936762\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.695557  [    0/12672]\n",
      "loss: 0.688449  [ 1600/12672]\n",
      "loss: 0.690228  [ 3200/12672]\n",
      "loss: 0.678137  [ 4800/12672]\n",
      "loss: 0.687922  [ 6400/12672]\n",
      "loss: 0.672715  [ 8000/12672]\n",
      "loss: 0.681325  [ 9600/12672]\n",
      "loss: 0.680306  [11200/12672]\n",
      "AUC: 0.5799265284702178\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.697209  [    0/12672]\n",
      "loss: 0.700787  [ 1600/12672]\n",
      "loss: 0.685080  [ 3200/12672]\n",
      "loss: 0.692405  [ 4800/12672]\n",
      "loss: 0.679585  [ 6400/12672]\n",
      "loss: 0.680955  [ 8000/12672]\n",
      "loss: 0.675289  [ 9600/12672]\n",
      "loss: 0.685908  [11200/12672]\n",
      "AUC: 0.5714248228811335\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.670016  [    0/12672]\n",
      "loss: 0.706470  [ 1600/12672]\n",
      "loss: 0.688753  [ 3200/12672]\n",
      "loss: 0.678554  [ 4800/12672]\n",
      "loss: 0.670466  [ 6400/12672]\n",
      "loss: 0.684439  [ 8000/12672]\n",
      "loss: 0.689719  [ 9600/12672]\n",
      "loss: 0.685558  [11200/12672]\n",
      "AUC: 0.542993964838625\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.688742  [    0/12672]\n",
      "loss: 0.676803  [ 1600/12672]\n",
      "loss: 0.676423  [ 3200/12672]\n",
      "loss: 0.672672  [ 4800/12672]\n",
      "loss: 0.681714  [ 6400/12672]\n",
      "loss: 0.681996  [ 8000/12672]\n",
      "loss: 0.688755  [ 9600/12672]\n",
      "loss: 0.683331  [11200/12672]\n",
      "AUC: 0.5977827341905011\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.729079  [    0/12672]\n",
      "loss: 2.676516  [ 1600/12672]\n",
      "loss: 2.265612  [ 3200/12672]\n",
      "loss: 2.512825  [ 4800/12672]\n",
      "loss: 1.749467  [ 6400/12672]\n",
      "loss: 2.265284  [ 8000/12672]\n",
      "loss: 1.808334  [ 9600/12672]\n",
      "loss: 1.615770  [11200/12672]\n",
      "AUC: 0.42583263000714155\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.848793  [    0/12672]\n",
      "loss: 3.489927  [ 1600/12672]\n",
      "loss: 1.932978  [ 3200/12672]\n",
      "loss: 2.387268  [ 4800/12672]\n",
      "loss: 2.795722  [ 6400/12672]\n",
      "loss: 2.027156  [ 8000/12672]\n",
      "loss: 2.092449  [ 9600/12672]\n",
      "loss: 2.104631  [11200/12672]\n",
      "AUC: 0.41795104849704606\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.055832  [    0/12672]\n",
      "loss: 2.892212  [ 1600/12672]\n",
      "loss: 4.889968  [ 3200/12672]\n",
      "loss: 4.523908  [ 4800/12672]\n",
      "loss: 3.508878  [ 6400/12672]\n",
      "loss: 2.125604  [ 8000/12672]\n",
      "loss: 2.063299  [ 9600/12672]\n",
      "loss: 5.422813  [11200/12672]\n",
      "AUC: 0.4375511264039472\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.353680  [    0/12672]\n",
      "loss: 1.867363  [ 1600/12672]\n",
      "loss: 2.593066  [ 3200/12672]\n",
      "loss: 1.735095  [ 4800/12672]\n",
      "loss: 1.893931  [ 6400/12672]\n",
      "loss: 2.551069  [ 8000/12672]\n",
      "loss: 3.347936  [ 9600/12672]\n",
      "loss: 2.338571  [11200/12672]\n",
      "AUC: 0.4178796338375641\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.940500  [    0/12672]\n",
      "loss: 2.228509  [ 1600/12672]\n",
      "loss: 2.324033  [ 3200/12672]\n",
      "loss: 2.141820  [ 4800/12672]\n",
      "loss: 3.219278  [ 6400/12672]\n",
      "loss: 4.110704  [ 8000/12672]\n",
      "loss: 2.894490  [ 9600/12672]\n",
      "loss: 4.045878  [11200/12672]\n",
      "AUC: 0.4212296305914432\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.183760  [    0/12672]\n",
      "loss: 0.690547  [ 1600/12672]\n",
      "loss: 0.678249  [ 3200/12672]\n",
      "loss: 0.683295  [ 4800/12672]\n",
      "loss: 0.677424  [ 6400/12672]\n",
      "loss: 0.681177  [ 8000/12672]\n",
      "loss: 0.684569  [ 9600/12672]\n",
      "loss: 0.685507  [11200/12672]\n",
      "AUC: 0.5968317860157112\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.676206  [    0/12672]\n",
      "loss: 0.680153  [ 1600/12672]\n",
      "loss: 0.681882  [ 3200/12672]\n",
      "loss: 0.692016  [ 4800/12672]\n",
      "loss: 0.681344  [ 6400/12672]\n",
      "loss: 0.673956  [ 8000/12672]\n",
      "loss: 0.685388  [ 9600/12672]\n",
      "loss: 0.696771  [11200/12672]\n",
      "AUC: 0.5901837304421217\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.678855  [    0/12672]\n",
      "loss: 0.677346  [ 1600/12672]\n",
      "loss: 0.677279  [ 3200/12672]\n",
      "loss: 0.679110  [ 4800/12672]\n",
      "loss: 0.687491  [ 6400/12672]\n",
      "loss: 0.693744  [ 8000/12672]\n",
      "loss: 0.670598  [ 9600/12672]\n",
      "loss: 0.672687  [11200/12672]\n",
      "AUC: 0.5731480880347984\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.682646  [    0/12672]\n",
      "loss: 0.695746  [ 1600/12672]\n",
      "loss: 0.680387  [ 3200/12672]\n",
      "loss: 0.685550  [ 4800/12672]\n",
      "loss: 0.665549  [ 6400/12672]\n",
      "loss: 0.694576  [ 8000/12672]\n",
      "loss: 0.680194  [ 9600/12672]\n",
      "loss: 0.678831  [11200/12672]\n",
      "AUC: 0.5940011686035188\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.663053  [    0/12672]\n",
      "loss: 0.684233  [ 1600/12672]\n",
      "loss: 0.676217  [ 3200/12672]\n",
      "loss: 0.675576  [ 4800/12672]\n",
      "loss: 0.688053  [ 6400/12672]\n",
      "loss: 0.674012  [ 8000/12672]\n",
      "loss: 0.695288  [ 9600/12672]\n",
      "loss: 0.680188  [11200/12672]\n",
      "AUC: 0.605985846912939\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.739548  [    0/12672]\n",
      "loss: 4.397935  [ 1600/12672]\n",
      "loss: 3.163212  [ 3200/12672]\n",
      "loss: 2.395800  [ 4800/12672]\n",
      "loss: 1.920827  [ 6400/12672]\n",
      "loss: 1.800089  [ 8000/12672]\n",
      "loss: 2.166207  [ 9600/12672]\n",
      "loss: 1.843088  [11200/12672]\n",
      "AUC: 0.48968253968253966\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.324282  [    0/12672]\n",
      "loss: 1.814659  [ 1600/12672]\n",
      "loss: 1.849815  [ 3200/12672]\n",
      "loss: 2.797643  [ 4800/12672]\n",
      "loss: 2.468374  [ 6400/12672]\n",
      "loss: 2.178290  [ 8000/12672]\n",
      "loss: 3.059396  [ 9600/12672]\n",
      "loss: 3.081861  [11200/12672]\n",
      "AUC: 0.5008977361436378\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.256624  [    0/12672]\n",
      "loss: 2.787451  [ 1600/12672]\n",
      "loss: 2.209462  [ 3200/12672]\n",
      "loss: 2.275886  [ 4800/12672]\n",
      "loss: 2.500704  [ 6400/12672]\n",
      "loss: 3.341318  [ 8000/12672]\n",
      "loss: 3.315872  [ 9600/12672]\n",
      "loss: 2.922096  [11200/12672]\n",
      "AUC: 0.4925904241478012\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.099173  [    0/12672]\n",
      "loss: 1.845399  [ 1600/12672]\n",
      "loss: 2.257328  [ 3200/12672]\n",
      "loss: 3.158187  [ 4800/12672]\n",
      "loss: 2.612956  [ 6400/12672]\n",
      "loss: 3.015518  [ 8000/12672]\n",
      "loss: 3.457673  [ 9600/12672]\n",
      "loss: 1.523747  [11200/12672]\n",
      "AUC: 0.5106297163674214\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.002831  [    0/12672]\n",
      "loss: 5.052539  [ 1600/12672]\n",
      "loss: 2.904924  [ 3200/12672]\n",
      "loss: 1.806144  [ 4800/12672]\n",
      "loss: 2.122176  [ 6400/12672]\n",
      "loss: 2.292627  [ 8000/12672]\n",
      "loss: 2.951799  [ 9600/12672]\n",
      "loss: 2.683019  [11200/12672]\n",
      "AUC: 0.4865339578454333\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.421885  [    0/12672]\n",
      "loss: 0.726295  [ 1600/12672]\n",
      "loss: 0.689392  [ 3200/12672]\n",
      "loss: 0.677771  [ 4800/12672]\n",
      "loss: 0.678988  [ 6400/12672]\n",
      "loss: 0.694328  [ 8000/12672]\n",
      "loss: 0.697574  [ 9600/12672]\n",
      "loss: 0.674955  [11200/12672]\n",
      "AUC: 0.5458300806661462\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.685829  [    0/12672]\n",
      "loss: 0.684686  [ 1600/12672]\n",
      "loss: 0.679504  [ 3200/12672]\n",
      "loss: 0.684489  [ 4800/12672]\n",
      "loss: 0.691374  [ 6400/12672]\n",
      "loss: 0.678635  [ 8000/12672]\n",
      "loss: 0.682223  [ 9600/12672]\n",
      "loss: 0.665719  [11200/12672]\n",
      "AUC: 0.5800546448087431\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.692102  [    0/12672]\n",
      "loss: 0.676134  [ 1600/12672]\n",
      "loss: 0.675925  [ 3200/12672]\n",
      "loss: 0.672278  [ 4800/12672]\n",
      "loss: 0.688083  [ 6400/12672]\n",
      "loss: 0.669448  [ 8000/12672]\n",
      "loss: 0.680272  [ 9600/12672]\n",
      "loss: 0.682971  [11200/12672]\n",
      "AUC: 0.5656648451730419\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.701527  [    0/12672]\n",
      "loss: 0.666766  [ 1600/12672]\n",
      "loss: 0.674607  [ 3200/12672]\n",
      "loss: 0.681228  [ 4800/12672]\n",
      "loss: 0.686649  [ 6400/12672]\n",
      "loss: 0.685203  [ 8000/12672]\n",
      "loss: 0.679304  [ 9600/12672]\n",
      "loss: 0.694687  [11200/12672]\n",
      "AUC: 0.5562060889929743\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.693281  [    0/12672]\n",
      "loss: 0.670543  [ 1600/12672]\n",
      "loss: 0.664994  [ 3200/12672]\n",
      "loss: 0.679473  [ 4800/12672]\n",
      "loss: 0.687872  [ 6400/12672]\n",
      "loss: 0.675039  [ 8000/12672]\n",
      "loss: 0.702095  [ 9600/12672]\n",
      "loss: 0.683176  [11200/12672]\n",
      "AUC: 0.5637652875357794\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.759813  [    0/12672]\n",
      "loss: 3.220672  [ 1600/12672]\n",
      "loss: 1.775179  [ 3200/12672]\n",
      "loss: 2.445640  [ 4800/12672]\n",
      "loss: 3.450627  [ 6400/12672]\n",
      "loss: 2.280015  [ 8000/12672]\n",
      "loss: 1.555914  [ 9600/12672]\n",
      "loss: 2.367079  [11200/12672]\n",
      "AUC: 0.4859033256097397\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.944921  [    0/12672]\n",
      "loss: 2.013685  [ 1600/12672]\n",
      "loss: 2.964486  [ 3200/12672]\n",
      "loss: 3.037543  [ 4800/12672]\n",
      "loss: 1.911123  [ 6400/12672]\n",
      "loss: 3.571277  [ 8000/12672]\n",
      "loss: 3.612543  [ 9600/12672]\n",
      "loss: 2.203883  [11200/12672]\n",
      "AUC: 0.49523860546723275\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.869785  [    0/12672]\n",
      "loss: 2.046695  [ 1600/12672]\n",
      "loss: 1.609549  [ 3200/12672]\n",
      "loss: 2.294414  [ 4800/12672]\n",
      "loss: 3.209327  [ 6400/12672]\n",
      "loss: 1.685070  [ 8000/12672]\n",
      "loss: 2.822006  [ 9600/12672]\n",
      "loss: 2.111614  [11200/12672]\n",
      "AUC: 0.4678756546080388\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 5.508928  [    0/12672]\n",
      "loss: 2.240489  [ 1600/12672]\n",
      "loss: 3.310798  [ 3200/12672]\n",
      "loss: 2.593498  [ 4800/12672]\n",
      "loss: 2.002105  [ 6400/12672]\n",
      "loss: 2.903615  [ 8000/12672]\n",
      "loss: 3.311952  [ 9600/12672]\n",
      "loss: 2.105855  [11200/12672]\n",
      "AUC: 0.48260852095415396\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.244143  [    0/12672]\n",
      "loss: 3.538940  [ 1600/12672]\n",
      "loss: 1.699932  [ 3200/12672]\n",
      "loss: 2.169680  [ 4800/12672]\n",
      "loss: 2.689878  [ 6400/12672]\n",
      "loss: 4.566320  [ 8000/12672]\n",
      "loss: 2.068831  [ 9600/12672]\n",
      "loss: 2.738147  [11200/12672]\n",
      "AUC: 0.4654648219332199\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.444581  [    0/12672]\n",
      "loss: 0.694917  [ 1600/12672]\n",
      "loss: 0.684730  [ 3200/12672]\n",
      "loss: 0.693329  [ 4800/12672]\n",
      "loss: 0.688795  [ 6400/12672]\n",
      "loss: 0.688096  [ 8000/12672]\n",
      "loss: 0.682551  [ 9600/12672]\n",
      "loss: 0.685970  [11200/12672]\n",
      "AUC: 0.6002303684555937\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.694192  [    0/12672]\n",
      "loss: 0.679932  [ 1600/12672]\n",
      "loss: 0.692423  [ 3200/12672]\n",
      "loss: 0.685836  [ 4800/12672]\n",
      "loss: 0.688534  [ 6400/12672]\n",
      "loss: 0.675427  [ 8000/12672]\n",
      "loss: 0.689832  [ 9600/12672]\n",
      "loss: 0.686277  [11200/12672]\n",
      "AUC: 0.5496698498586984\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.684939  [    0/12672]\n",
      "loss: 0.685579  [ 1600/12672]\n",
      "loss: 0.681997  [ 3200/12672]\n",
      "loss: 0.693527  [ 4800/12672]\n",
      "loss: 0.686974  [ 6400/12672]\n",
      "loss: 0.697929  [ 8000/12672]\n",
      "loss: 0.680448  [ 9600/12672]\n",
      "loss: 0.671641  [11200/12672]\n",
      "AUC: 0.6136640638602788\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.692440  [    0/12672]\n",
      "loss: 0.681553  [ 1600/12672]\n",
      "loss: 0.671333  [ 3200/12672]\n",
      "loss: 0.692777  [ 4800/12672]\n",
      "loss: 0.686425  [ 6400/12672]\n",
      "loss: 0.678824  [ 8000/12672]\n",
      "loss: 0.677283  [ 9600/12672]\n",
      "loss: 0.688317  [11200/12672]\n",
      "AUC: 0.6127934853943721\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.685573  [    0/12672]\n",
      "loss: 0.682410  [ 1600/12672]\n",
      "loss: 0.679001  [ 3200/12672]\n",
      "loss: 0.675830  [ 4800/12672]\n",
      "loss: 0.674610  [ 6400/12672]\n",
      "loss: 0.677923  [ 8000/12672]\n",
      "loss: 0.681892  [ 9600/12672]\n",
      "loss: 0.678232  [11200/12672]\n",
      "AUC: 0.6331784150114513\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.735430  [    0/12672]\n",
      "loss: 2.838127  [ 1600/12672]\n",
      "loss: 2.751049  [ 3200/12672]\n",
      "loss: 2.446831  [ 4800/12672]\n",
      "loss: 2.232464  [ 6400/12672]\n",
      "loss: 2.415772  [ 8000/12672]\n",
      "loss: 1.848413  [ 9600/12672]\n",
      "loss: 3.264230  [11200/12672]\n",
      "AUC: 0.4469962934470942\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.051270  [    0/12672]\n",
      "loss: 2.620385  [ 1600/12672]\n",
      "loss: 2.211732  [ 3200/12672]\n",
      "loss: 1.955793  [ 4800/12672]\n",
      "loss: 3.104563  [ 6400/12672]\n",
      "loss: 3.682507  [ 8000/12672]\n",
      "loss: 3.202774  [ 9600/12672]\n",
      "loss: 3.281071  [11200/12672]\n",
      "AUC: 0.4522833764598923\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.143919  [    0/12672]\n",
      "loss: 1.827868  [ 1600/12672]\n",
      "loss: 4.297067  [ 3200/12672]\n",
      "loss: 2.099509  [ 4800/12672]\n",
      "loss: 2.821026  [ 6400/12672]\n",
      "loss: 3.658374  [ 8000/12672]\n",
      "loss: 2.800951  [ 9600/12672]\n",
      "loss: 3.701567  [11200/12672]\n",
      "AUC: 0.4613749213231695\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.749113  [    0/12672]\n",
      "loss: 2.310800  [ 1600/12672]\n",
      "loss: 4.583682  [ 3200/12672]\n",
      "loss: 2.572155  [ 4800/12672]\n",
      "loss: 3.240839  [ 6400/12672]\n",
      "loss: 2.942173  [ 8000/12672]\n",
      "loss: 3.466476  [ 9600/12672]\n",
      "loss: 2.768083  [11200/12672]\n",
      "AUC: 0.4636827750192321\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.978648  [    0/12672]\n",
      "loss: 2.595292  [ 1600/12672]\n",
      "loss: 2.201259  [ 3200/12672]\n",
      "loss: 2.733938  [ 4800/12672]\n",
      "loss: 1.445520  [ 6400/12672]\n",
      "loss: 1.993433  [ 8000/12672]\n",
      "loss: 2.461506  [ 9600/12672]\n",
      "loss: 3.077107  [11200/12672]\n",
      "AUC: 0.4778655850059445\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.702239  [    0/12672]\n",
      "loss: 0.703645  [ 1600/12672]\n",
      "loss: 0.670143  [ 3200/12672]\n",
      "loss: 0.688895  [ 4800/12672]\n",
      "loss: 0.686414  [ 6400/12672]\n",
      "loss: 0.681777  [ 8000/12672]\n",
      "loss: 0.688908  [ 9600/12672]\n",
      "loss: 0.666241  [11200/12672]\n",
      "AUC: 0.5078536960626616\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.680743  [    0/12672]\n",
      "loss: 0.672399  [ 1600/12672]\n",
      "loss: 0.691141  [ 3200/12672]\n",
      "loss: 0.679341  [ 4800/12672]\n",
      "loss: 0.660641  [ 6400/12672]\n",
      "loss: 0.691343  [ 8000/12672]\n",
      "loss: 0.691691  [ 9600/12672]\n",
      "loss: 0.683518  [11200/12672]\n",
      "AUC: 0.6056927057836212\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.682803  [    0/12672]\n",
      "loss: 0.688872  [ 1600/12672]\n",
      "loss: 0.679443  [ 3200/12672]\n",
      "loss: 0.661869  [ 4800/12672]\n",
      "loss: 0.691308  [ 6400/12672]\n",
      "loss: 0.659964  [ 8000/12672]\n",
      "loss: 0.685538  [ 9600/12672]\n",
      "loss: 0.685591  [11200/12672]\n",
      "AUC: 0.504902440730121\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.692262  [    0/12672]\n",
      "loss: 0.694021  [ 1600/12672]\n",
      "loss: 0.674608  [ 3200/12672]\n",
      "loss: 0.705108  [ 4800/12672]\n",
      "loss: 0.685422  [ 6400/12672]\n",
      "loss: 0.679096  [ 8000/12672]\n",
      "loss: 0.692669  [ 9600/12672]\n",
      "loss: 0.677408  [11200/12672]\n",
      "AUC: 0.5041331561647668\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.676350  [    0/12672]\n",
      "loss: 0.683976  [ 1600/12672]\n",
      "loss: 0.682667  [ 3200/12672]\n",
      "loss: 0.689248  [ 4800/12672]\n",
      "loss: 0.678288  [ 6400/12672]\n",
      "loss: 0.687454  [ 8000/12672]\n",
      "loss: 0.682655  [ 9600/12672]\n",
      "loss: 0.668698  [11200/12672]\n",
      "AUC: 0.5520945520665781\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.736050  [    0/12672]\n",
      "loss: 2.088752  [ 1600/12672]\n",
      "loss: 3.152326  [ 3200/12672]\n",
      "loss: 2.293357  [ 4800/12672]\n",
      "loss: 2.859040  [ 6400/12672]\n",
      "loss: 2.038592  [ 8000/12672]\n",
      "loss: 2.351391  [ 9600/12672]\n",
      "loss: 2.079311  [11200/12672]\n",
      "AUC: 0.4797256374538186\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.489482  [    0/12672]\n",
      "loss: 1.429050  [ 1600/12672]\n",
      "loss: 2.271574  [ 3200/12672]\n",
      "loss: 2.436468  [ 4800/12672]\n",
      "loss: 2.136309  [ 6400/12672]\n",
      "loss: 2.962842  [ 8000/12672]\n",
      "loss: 1.844472  [ 9600/12672]\n",
      "loss: 3.272613  [11200/12672]\n",
      "AUC: 0.5030055720428805\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.123265  [    0/12672]\n",
      "loss: 2.170239  [ 1600/12672]\n",
      "loss: 3.588382  [ 3200/12672]\n",
      "loss: 2.662967  [ 4800/12672]\n",
      "loss: 1.752591  [ 6400/12672]\n",
      "loss: 2.404034  [ 8000/12672]\n",
      "loss: 2.114634  [ 9600/12672]\n",
      "loss: 1.749907  [11200/12672]\n",
      "AUC: 0.49444309854036705\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.870870  [    0/12672]\n",
      "loss: 1.996877  [ 1600/12672]\n",
      "loss: 2.320002  [ 3200/12672]\n",
      "loss: 2.439291  [ 4800/12672]\n",
      "loss: 2.157243  [ 6400/12672]\n",
      "loss: 2.257076  [ 8000/12672]\n",
      "loss: 2.454126  [ 9600/12672]\n",
      "loss: 2.172261  [11200/12672]\n",
      "AUC: 0.5118103082793289\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.604640  [    0/12672]\n",
      "loss: 2.770449  [ 1600/12672]\n",
      "loss: 1.991159  [ 3200/12672]\n",
      "loss: 2.557115  [ 4800/12672]\n",
      "loss: 2.631202  [ 6400/12672]\n",
      "loss: 2.328942  [ 8000/12672]\n",
      "loss: 2.454305  [ 9600/12672]\n",
      "loss: 1.274651  [11200/12672]\n",
      "AUC: 0.4943068257525286\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.427524  [    0/12672]\n",
      "loss: 0.685907  [ 1600/12672]\n",
      "loss: 0.674183  [ 3200/12672]\n",
      "loss: 0.682256  [ 4800/12672]\n",
      "loss: 0.686873  [ 6400/12672]\n",
      "loss: 0.685260  [ 8000/12672]\n",
      "loss: 0.685393  [ 9600/12672]\n",
      "loss: 0.689264  [11200/12672]\n",
      "AUC: 0.5909090909090909\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.685793  [    0/12672]\n",
      "loss: 0.693830  [ 1600/12672]\n",
      "loss: 0.687261  [ 3200/12672]\n",
      "loss: 0.670470  [ 4800/12672]\n",
      "loss: 0.651571  [ 6400/12672]\n",
      "loss: 0.689455  [ 8000/12672]\n",
      "loss: 0.661150  [ 9600/12672]\n",
      "loss: 0.680582  [11200/12672]\n",
      "AUC: 0.5657591908424687\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.696530  [    0/12672]\n",
      "loss: 0.684177  [ 1600/12672]\n",
      "loss: 0.682714  [ 3200/12672]\n",
      "loss: 0.674200  [ 4800/12672]\n",
      "loss: 0.685527  [ 6400/12672]\n",
      "loss: 0.674818  [ 8000/12672]\n",
      "loss: 0.669220  [ 9600/12672]\n",
      "loss: 0.686839  [11200/12672]\n",
      "AUC: 0.5354763491005996\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.702089  [    0/12672]\n",
      "loss: 0.682571  [ 1600/12672]\n",
      "loss: 0.685261  [ 3200/12672]\n",
      "loss: 0.690994  [ 4800/12672]\n",
      "loss: 0.692149  [ 6400/12672]\n",
      "loss: 0.701805  [ 8000/12672]\n",
      "loss: 0.688822  [ 9600/12672]\n",
      "loss: 0.691749  [11200/12672]\n",
      "AUC: 0.6500060565683484\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.689264  [    0/12672]\n",
      "loss: 0.687054  [ 1600/12672]\n",
      "loss: 0.677005  [ 3200/12672]\n",
      "loss: 0.676864  [ 4800/12672]\n",
      "loss: 0.688343  [ 6400/12672]\n",
      "loss: 0.684228  [ 8000/12672]\n",
      "loss: 0.680970  [ 9600/12672]\n",
      "loss: 0.671257  [11200/12672]\n",
      "AUC: 0.6578644540003634\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.714256  [    0/12672]\n",
      "loss: 2.197756  [ 1600/12672]\n",
      "loss: 2.624432  [ 3200/12672]\n",
      "loss: 2.033420  [ 4800/12672]\n",
      "loss: 2.204699  [ 6400/12672]\n",
      "loss: 2.941818  [ 8000/12672]\n",
      "loss: 3.922864  [ 9600/12672]\n",
      "loss: 3.408853  [11200/12672]\n",
      "AUC: 0.46538706594456836\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.680237  [    0/12672]\n",
      "loss: 4.737845  [ 1600/12672]\n",
      "loss: 2.362283  [ 3200/12672]\n",
      "loss: 2.424253  [ 4800/12672]\n",
      "loss: 2.484995  [ 6400/12672]\n",
      "loss: 2.681975  [ 8000/12672]\n",
      "loss: 1.881108  [ 9600/12672]\n",
      "loss: 2.657786  [11200/12672]\n",
      "AUC: 0.5008601465434852\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.122901  [    0/12672]\n",
      "loss: 2.868094  [ 1600/12672]\n",
      "loss: 1.621241  [ 3200/12672]\n",
      "loss: 2.486060  [ 4800/12672]\n",
      "loss: 4.126281  [ 6400/12672]\n",
      "loss: 2.319328  [ 8000/12672]\n",
      "loss: 2.171933  [ 9600/12672]\n",
      "loss: 2.435604  [11200/12672]\n",
      "AUC: 0.47245938196877985\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.521461  [    0/12672]\n",
      "loss: 2.489874  [ 1600/12672]\n",
      "loss: 2.189711  [ 3200/12672]\n",
      "loss: 2.451132  [ 4800/12672]\n",
      "loss: 2.679935  [ 6400/12672]\n",
      "loss: 2.398019  [ 8000/12672]\n",
      "loss: 2.396462  [ 9600/12672]\n",
      "loss: 3.005938  [11200/12672]\n",
      "AUC: 0.480997132844855\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.903631  [    0/12672]\n",
      "loss: 2.054945  [ 1600/12672]\n",
      "loss: 2.815967  [ 3200/12672]\n",
      "loss: 2.529251  [ 4800/12672]\n",
      "loss: 2.113548  [ 6400/12672]\n",
      "loss: 2.326265  [ 8000/12672]\n",
      "loss: 3.006508  [ 9600/12672]\n",
      "loss: 2.345843  [11200/12672]\n",
      "AUC: 0.4682701497292131\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.770880  [    0/12672]\n",
      "loss: 0.676675  [ 1600/12672]\n",
      "loss: 0.683038  [ 3200/12672]\n",
      "loss: 0.684580  [ 4800/12672]\n",
      "loss: 0.692554  [ 6400/12672]\n",
      "loss: 0.691893  [ 8000/12672]\n",
      "loss: 0.695274  [ 9600/12672]\n",
      "loss: 0.680737  [11200/12672]\n",
      "AUC: 0.5980726345970055\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.678689  [    0/12672]\n",
      "loss: 0.688156  [ 1600/12672]\n",
      "loss: 0.689386  [ 3200/12672]\n",
      "loss: 0.685231  [ 4800/12672]\n",
      "loss: 0.672840  [ 6400/12672]\n",
      "loss: 0.689464  [ 8000/12672]\n",
      "loss: 0.681826  [ 9600/12672]\n",
      "loss: 0.674815  [11200/12672]\n",
      "AUC: 0.5614367633004141\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.688051  [    0/12672]\n",
      "loss: 0.669498  [ 1600/12672]\n",
      "loss: 0.691990  [ 3200/12672]\n",
      "loss: 0.675442  [ 4800/12672]\n",
      "loss: 0.682130  [ 6400/12672]\n",
      "loss: 0.677084  [ 8000/12672]\n",
      "loss: 0.692572  [ 9600/12672]\n",
      "loss: 0.688246  [11200/12672]\n",
      "AUC: 0.4920516087926091\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.680781  [    0/12672]\n",
      "loss: 0.681914  [ 1600/12672]\n",
      "loss: 0.674660  [ 3200/12672]\n",
      "loss: 0.663783  [ 4800/12672]\n",
      "loss: 0.685029  [ 6400/12672]\n",
      "loss: 0.668676  [ 8000/12672]\n",
      "loss: 0.692244  [ 9600/12672]\n",
      "loss: 0.678537  [11200/12672]\n",
      "AUC: 0.555431666135712\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.666190  [    0/12672]\n",
      "loss: 0.688053  [ 1600/12672]\n",
      "loss: 0.671838  [ 3200/12672]\n",
      "loss: 0.683486  [ 4800/12672]\n",
      "loss: 0.678877  [ 6400/12672]\n",
      "loss: 0.678541  [ 8000/12672]\n",
      "loss: 0.678595  [ 9600/12672]\n",
      "loss: 0.685850  [11200/12672]\n",
      "AUC: 0.5726983115641925\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.715129  [    0/12672]\n",
      "loss: 2.889676  [ 1600/12672]\n",
      "loss: 1.939539  [ 3200/12672]\n",
      "loss: 3.669700  [ 4800/12672]\n",
      "loss: 2.486570  [ 6400/12672]\n",
      "loss: 2.546403  [ 8000/12672]\n",
      "loss: 4.323350  [ 9600/12672]\n",
      "loss: 2.780507  [11200/12672]\n",
      "AUC: 0.5095598845598845\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.440092  [    0/12672]\n",
      "loss: 1.551057  [ 1600/12672]\n",
      "loss: 3.909305  [ 3200/12672]\n",
      "loss: 2.462126  [ 4800/12672]\n",
      "loss: 1.994476  [ 6400/12672]\n",
      "loss: 1.801202  [ 8000/12672]\n",
      "loss: 2.875960  [ 9600/12672]\n",
      "loss: 1.984364  [11200/12672]\n",
      "AUC: 0.5151581957137512\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.624249  [    0/12672]\n",
      "loss: 2.940230  [ 1600/12672]\n",
      "loss: 1.813518  [ 3200/12672]\n",
      "loss: 3.935255  [ 4800/12672]\n",
      "loss: 1.991977  [ 6400/12672]\n",
      "loss: 2.241724  [ 8000/12672]\n",
      "loss: 3.942940  [ 9600/12672]\n",
      "loss: 4.094548  [11200/12672]\n",
      "AUC: 0.5077427716316605\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.209002  [    0/12672]\n",
      "loss: 2.537838  [ 1600/12672]\n",
      "loss: 2.197761  [ 3200/12672]\n",
      "loss: 2.042081  [ 4800/12672]\n",
      "loss: 2.699936  [ 6400/12672]\n",
      "loss: 2.467415  [ 8000/12672]\n",
      "loss: 1.908080  [ 9600/12672]\n",
      "loss: 1.371629  [11200/12672]\n",
      "AUC: 0.4939607717385495\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.053705  [    0/12672]\n",
      "loss: 2.935391  [ 1600/12672]\n",
      "loss: 2.325471  [ 3200/12672]\n",
      "loss: 1.860800  [ 4800/12672]\n",
      "loss: 3.089714  [ 6400/12672]\n",
      "loss: 4.862830  [ 8000/12672]\n",
      "loss: 2.161721  [ 9600/12672]\n",
      "loss: 1.982526  [11200/12672]\n",
      "AUC: 0.5088918283362728\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.658251  [    0/12672]\n",
      "loss: 0.720088  [ 1600/12672]\n",
      "loss: 0.689951  [ 3200/12672]\n",
      "loss: 0.693554  [ 4800/12672]\n",
      "loss: 0.674846  [ 6400/12672]\n",
      "loss: 0.673743  [ 8000/12672]\n",
      "loss: 0.675153  [ 9600/12672]\n",
      "loss: 0.686010  [11200/12672]\n",
      "AUC: 0.5682219015552349\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.663059  [    0/12672]\n",
      "loss: 0.675173  [ 1600/12672]\n",
      "loss: 0.689129  [ 3200/12672]\n",
      "loss: 0.697137  [ 4800/12672]\n",
      "loss: 0.697459  [ 6400/12672]\n",
      "loss: 0.669332  [ 8000/12672]\n",
      "loss: 0.686999  [ 9600/12672]\n",
      "loss: 0.697504  [11200/12672]\n",
      "AUC: 0.5537918871252204\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.684821  [    0/12672]\n",
      "loss: 0.678001  [ 1600/12672]\n",
      "loss: 0.696692  [ 3200/12672]\n",
      "loss: 0.667731  [ 4800/12672]\n",
      "loss: 0.673737  [ 6400/12672]\n",
      "loss: 0.675153  [ 8000/12672]\n",
      "loss: 0.679258  [ 9600/12672]\n",
      "loss: 0.689854  [11200/12672]\n",
      "AUC: 0.5924857035968147\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.695010  [    0/12672]\n",
      "loss: 0.672241  [ 1600/12672]\n",
      "loss: 0.678059  [ 3200/12672]\n",
      "loss: 0.665600  [ 4800/12672]\n",
      "loss: 0.673200  [ 6400/12672]\n",
      "loss: 0.688206  [ 8000/12672]\n",
      "loss: 0.679216  [ 9600/12672]\n",
      "loss: 0.687800  [11200/12672]\n",
      "AUC: 0.5726577948800171\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.684628  [    0/12672]\n",
      "loss: 0.677850  [ 1600/12672]\n",
      "loss: 0.688881  [ 3200/12672]\n",
      "loss: 0.684375  [ 4800/12672]\n",
      "loss: 0.672639  [ 6400/12672]\n",
      "loss: 0.679602  [ 8000/12672]\n",
      "loss: 0.688382  [ 9600/12672]\n",
      "loss: 0.686261  [11200/12672]\n",
      "AUC: 0.5918978141200363\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.692018  [    0/12672]\n",
      "loss: 2.320632  [ 1600/12672]\n",
      "loss: 3.192759  [ 3200/12672]\n",
      "loss: 3.028933  [ 4800/12672]\n",
      "loss: 2.866452  [ 6400/12672]\n",
      "loss: 2.078606  [ 8000/12672]\n",
      "loss: 3.364530  [ 9600/12672]\n",
      "loss: 2.817702  [11200/12672]\n",
      "AUC: 0.557655353563282\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.546638  [    0/12672]\n",
      "loss: 3.752533  [ 1600/12672]\n",
      "loss: 2.938506  [ 3200/12672]\n",
      "loss: 4.001348  [ 4800/12672]\n",
      "loss: 2.213612  [ 6400/12672]\n",
      "loss: 3.132068  [ 8000/12672]\n",
      "loss: 4.104292  [ 9600/12672]\n",
      "loss: 3.818867  [11200/12672]\n",
      "AUC: 0.5446256998686666\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.330368  [    0/12672]\n",
      "loss: 2.386615  [ 1600/12672]\n",
      "loss: 2.369336  [ 3200/12672]\n",
      "loss: 2.603643  [ 4800/12672]\n",
      "loss: 2.168054  [ 6400/12672]\n",
      "loss: 1.732311  [ 8000/12672]\n",
      "loss: 3.881737  [ 9600/12672]\n",
      "loss: 3.270695  [11200/12672]\n",
      "AUC: 0.5202253404299441\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 4.139471  [    0/12672]\n",
      "loss: 1.583118  [ 1600/12672]\n",
      "loss: 2.479975  [ 3200/12672]\n",
      "loss: 3.166481  [ 4800/12672]\n",
      "loss: 2.929420  [ 6400/12672]\n",
      "loss: 3.090156  [ 8000/12672]\n",
      "loss: 1.493343  [ 9600/12672]\n",
      "loss: 3.076351  [11200/12672]\n",
      "AUC: 0.5640008294739752\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 1.487053  [    0/12672]\n",
      "loss: 4.007208  [ 1600/12672]\n",
      "loss: 2.344365  [ 3200/12672]\n",
      "loss: 1.829977  [ 4800/12672]\n",
      "loss: 1.610554  [ 6400/12672]\n",
      "loss: 2.579582  [ 8000/12672]\n",
      "loss: 1.807701  [ 9600/12672]\n",
      "loss: 2.602454  [11200/12672]\n",
      "AUC: 0.5515448952789106\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.062537  [    0/12672]\n",
      "loss: 0.692065  [ 1600/12672]\n",
      "loss: 0.680904  [ 3200/12672]\n",
      "loss: 0.682419  [ 4800/12672]\n",
      "loss: 0.689745  [ 6400/12672]\n",
      "loss: 0.683176  [ 8000/12672]\n",
      "loss: 0.681661  [ 9600/12672]\n",
      "loss: 0.695914  [11200/12672]\n",
      "AUC: 0.6051842123453377\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.682266  [    0/12672]\n",
      "loss: 0.700439  [ 1600/12672]\n",
      "loss: 0.682971  [ 3200/12672]\n",
      "loss: 0.685910  [ 4800/12672]\n",
      "loss: 0.679165  [ 6400/12672]\n",
      "loss: 0.688155  [ 8000/12672]\n",
      "loss: 0.691149  [ 9600/12672]\n",
      "loss: 0.677651  [11200/12672]\n",
      "AUC: 0.508633441625769\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.671887  [    0/12672]\n",
      "loss: 0.685077  [ 1600/12672]\n",
      "loss: 0.668957  [ 3200/12672]\n",
      "loss: 0.678495  [ 4800/12672]\n",
      "loss: 0.672105  [ 6400/12672]\n",
      "loss: 0.673642  [ 8000/12672]\n",
      "loss: 0.665417  [ 9600/12672]\n",
      "loss: 0.687139  [11200/12672]\n",
      "AUC: 0.5910140319347481\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.684767  [    0/12672]\n",
      "loss: 0.694747  [ 1600/12672]\n",
      "loss: 0.683784  [ 3200/12672]\n",
      "loss: 0.675152  [ 4800/12672]\n",
      "loss: 0.697004  [ 6400/12672]\n",
      "loss: 0.675944  [ 8000/12672]\n",
      "loss: 0.690245  [ 9600/12672]\n",
      "loss: 0.680549  [11200/12672]\n",
      "AUC: 0.5819451164719708\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.692440  [    0/12672]\n",
      "loss: 0.679743  [ 1600/12672]\n",
      "loss: 0.684133  [ 3200/12672]\n",
      "loss: 0.692221  [ 4800/12672]\n",
      "loss: 0.677856  [ 6400/12672]\n",
      "loss: 0.688971  [ 8000/12672]\n",
      "loss: 0.682946  [ 9600/12672]\n",
      "loss: 0.693186  [11200/12672]\n",
      "AUC: 0.5807976774728694\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.713094  [    0/12672]\n",
      "loss: 2.331102  [ 1600/12672]\n",
      "loss: 1.969620  [ 3200/12672]\n",
      "loss: 1.764616  [ 4800/12672]\n",
      "loss: 2.593667  [ 6400/12672]\n",
      "loss: 1.857554  [ 8000/12672]\n",
      "loss: 2.716481  [ 9600/12672]\n",
      "loss: 2.214127  [11200/12672]\n",
      "AUC: 0.5476787118616958\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.636798  [    0/12672]\n",
      "loss: 1.951909  [ 1600/12672]\n",
      "loss: 1.996588  [ 3200/12672]\n",
      "loss: 2.316812  [ 4800/12672]\n",
      "loss: 2.682057  [ 6400/12672]\n",
      "loss: 2.023044  [ 8000/12672]\n",
      "loss: 2.866484  [ 9600/12672]\n",
      "loss: 3.071386  [11200/12672]\n",
      "AUC: 0.5629340335630406\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.368998  [    0/12672]\n",
      "loss: 1.842938  [ 1600/12672]\n",
      "loss: 3.301487  [ 3200/12672]\n",
      "loss: 2.642236  [ 4800/12672]\n",
      "loss: 2.632497  [ 6400/12672]\n",
      "loss: 2.046366  [ 8000/12672]\n",
      "loss: 1.737863  [ 9600/12672]\n",
      "loss: 2.571307  [11200/12672]\n",
      "AUC: 0.5558761979659572\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.985845  [    0/12672]\n",
      "loss: 2.170233  [ 1600/12672]\n",
      "loss: 2.771004  [ 3200/12672]\n",
      "loss: 1.887218  [ 4800/12672]\n",
      "loss: 2.735175  [ 6400/12672]\n",
      "loss: 2.929930  [ 8000/12672]\n",
      "loss: 2.223438  [ 9600/12672]\n",
      "loss: 2.742944  [11200/12672]\n",
      "AUC: 0.5554496621036216\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.250362  [    0/12672]\n",
      "loss: 1.766489  [ 1600/12672]\n",
      "loss: 1.455203  [ 3200/12672]\n",
      "loss: 2.002547  [ 4800/12672]\n",
      "loss: 2.360222  [ 6400/12672]\n",
      "loss: 2.490049  [ 8000/12672]\n",
      "loss: 2.191804  [ 9600/12672]\n",
      "loss: 2.510198  [11200/12672]\n",
      "AUC: 0.5637204590592219\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.190182  [    0/12672]\n",
      "loss: 0.663259  [ 1600/12672]\n",
      "loss: 0.695727  [ 3200/12672]\n",
      "loss: 0.679846  [ 4800/12672]\n",
      "loss: 0.691828  [ 6400/12672]\n",
      "loss: 0.676404  [ 8000/12672]\n",
      "loss: 0.682745  [ 9600/12672]\n",
      "loss: 0.680410  [11200/12672]\n",
      "AUC: 0.6428295322767685\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.684632  [    0/12672]\n",
      "loss: 0.682760  [ 1600/12672]\n",
      "loss: 0.675227  [ 3200/12672]\n",
      "loss: 0.679986  [ 4800/12672]\n",
      "loss: 0.682640  [ 6400/12672]\n",
      "loss: 0.687567  [ 8000/12672]\n",
      "loss: 0.699230  [ 9600/12672]\n",
      "loss: 0.686149  [11200/12672]\n",
      "AUC: 0.6456153446276475\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.688989  [    0/12672]\n",
      "loss: 0.671116  [ 1600/12672]\n",
      "loss: 0.674263  [ 3200/12672]\n",
      "loss: 0.675965  [ 4800/12672]\n",
      "loss: 0.689132  [ 6400/12672]\n",
      "loss: 0.676634  [ 8000/12672]\n",
      "loss: 0.691437  [ 9600/12672]\n",
      "loss: 0.691541  [11200/12672]\n",
      "AUC: 0.5883529051090999\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.678362  [    0/12672]\n",
      "loss: 0.677285  [ 1600/12672]\n",
      "loss: 0.679138  [ 3200/12672]\n",
      "loss: 0.679306  [ 4800/12672]\n",
      "loss: 0.679692  [ 6400/12672]\n",
      "loss: 0.676835  [ 8000/12672]\n",
      "loss: 0.689376  [ 9600/12672]\n",
      "loss: 0.681961  [11200/12672]\n",
      "AUC: 0.5843941191367981\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.692424  [    0/12672]\n",
      "loss: 0.692297  [ 1600/12672]\n",
      "loss: 0.683736  [ 3200/12672]\n",
      "loss: 0.657024  [ 4800/12672]\n",
      "loss: 0.681818  [ 6400/12672]\n",
      "loss: 0.685529  [ 8000/12672]\n",
      "loss: 0.688836  [ 9600/12672]\n",
      "loss: 0.676586  [11200/12672]\n",
      "AUC: 0.6580781893552644\n",
      "##############################\n",
      "SESSION 22\n",
      "##############################\n",
      "------------------------------\n",
      "Epoch 1\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.742729  [    0/12672]\n",
      "loss: 3.656982  [ 1600/12672]\n",
      "loss: 2.298016  [ 3200/12672]\n",
      "loss: 2.080737  [ 4800/12672]\n",
      "loss: 4.347044  [ 6400/12672]\n",
      "loss: 3.607296  [ 8000/12672]\n",
      "loss: 2.062227  [ 9600/12672]\n",
      "loss: 1.400101  [11200/12672]\n",
      "AUC: 0.5382446808510638\n",
      "------------------------------\n",
      "Epoch 2\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.978812  [    0/12672]\n",
      "loss: 2.149679  [ 1600/12672]\n",
      "loss: 2.666604  [ 3200/12672]\n",
      "loss: 2.415994  [ 4800/12672]\n",
      "loss: 2.796404  [ 6400/12672]\n",
      "loss: 3.595449  [ 8000/12672]\n",
      "loss: 2.246372  [ 9600/12672]\n",
      "loss: 2.269348  [11200/12672]\n",
      "AUC: 0.5235970744680851\n",
      "------------------------------\n",
      "Epoch 3\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.850715  [    0/12672]\n",
      "loss: 2.087531  [ 1600/12672]\n",
      "loss: 2.204139  [ 3200/12672]\n",
      "loss: 2.336557  [ 4800/12672]\n",
      "loss: 3.279998  [ 6400/12672]\n",
      "loss: 1.565400  [ 8000/12672]\n",
      "loss: 2.125862  [ 9600/12672]\n",
      "loss: 2.060899  [11200/12672]\n",
      "AUC: 0.5895412234042553\n",
      "------------------------------\n",
      "Epoch 4\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.120697  [    0/12672]\n",
      "loss: 3.851297  [ 1600/12672]\n",
      "loss: 2.672990  [ 3200/12672]\n",
      "loss: 2.694325  [ 4800/12672]\n",
      "loss: 3.050966  [ 6400/12672]\n",
      "loss: 2.052548  [ 8000/12672]\n",
      "loss: 1.805514  [ 9600/12672]\n",
      "loss: 2.457981  [11200/12672]\n",
      "AUC: 0.5355186170212766\n",
      "------------------------------\n",
      "Epoch 5\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 2.008493  [    0/12672]\n",
      "loss: 2.585362  [ 1600/12672]\n",
      "loss: 2.236583  [ 3200/12672]\n",
      "loss: 4.125707  [ 4800/12672]\n",
      "loss: 2.674059  [ 6400/12672]\n",
      "loss: 1.852885  [ 8000/12672]\n",
      "loss: 1.788577  [ 9600/12672]\n",
      "loss: 2.345049  [11200/12672]\n",
      "AUC: 0.547905585106383\n",
      "------------------------------\n",
      "Epoch 6\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 3.470215  [    0/12672]\n",
      "loss: 0.710257  [ 1600/12672]\n",
      "loss: 0.676277  [ 3200/12672]\n",
      "loss: 0.698459  [ 4800/12672]\n",
      "loss: 0.699829  [ 6400/12672]\n",
      "loss: 0.681899  [ 8000/12672]\n",
      "loss: 0.685893  [ 9600/12672]\n",
      "loss: 0.679296  [11200/12672]\n",
      "AUC: 0.6155452127659574\n",
      "------------------------------\n",
      "Epoch 7\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.672613  [    0/12672]\n",
      "loss: 0.683428  [ 1600/12672]\n",
      "loss: 0.682610  [ 3200/12672]\n",
      "loss: 0.694168  [ 4800/12672]\n",
      "loss: 0.689666  [ 6400/12672]\n",
      "loss: 0.679371  [ 8000/12672]\n",
      "loss: 0.685189  [ 9600/12672]\n",
      "loss: 0.658655  [11200/12672]\n",
      "AUC: 0.6146941489361702\n",
      "------------------------------\n",
      "Epoch 8\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.682451  [    0/12672]\n",
      "loss: 0.687326  [ 1600/12672]\n",
      "loss: 0.657178  [ 3200/12672]\n",
      "loss: 0.705857  [ 4800/12672]\n",
      "loss: 0.688537  [ 6400/12672]\n",
      "loss: 0.684868  [ 8000/12672]\n",
      "loss: 0.685822  [ 9600/12672]\n",
      "loss: 0.684129  [11200/12672]\n",
      "AUC: 0.5496010638297872\n",
      "------------------------------\n",
      "Epoch 9\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.691196  [    0/12672]\n",
      "loss: 0.683717  [ 1600/12672]\n",
      "loss: 0.693030  [ 3200/12672]\n",
      "loss: 0.689962  [ 4800/12672]\n",
      "loss: 0.657925  [ 6400/12672]\n",
      "loss: 0.681417  [ 8000/12672]\n",
      "loss: 0.690088  [ 9600/12672]\n",
      "loss: 0.692924  [11200/12672]\n",
      "AUC: 0.5513829787234042\n",
      "------------------------------\n",
      "Epoch 10\n",
      "------------------------------\n",
      "yay training\n",
      "loss: 0.689983  [    0/12672]\n",
      "loss: 0.672518  [ 1600/12672]\n",
      "loss: 0.680610  [ 3200/12672]\n",
      "loss: 0.699203  [ 4800/12672]\n",
      "loss: 0.676150  [ 6400/12672]\n",
      "loss: 0.671008  [ 8000/12672]\n",
      "loss: 0.681290  [ 9600/12672]\n",
      "loss: 0.688851  [11200/12672]\n",
      "AUC: 0.6151063829787233\n"
     ]
    }
   ],
   "source": [
    "set_seed(56)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "torch_auc_list = []\n",
    "sk_auc_list = []\n",
    "logo = LeaveOneGroupOut()\n",
    "for train_idx, test_idx in tqdm(list(logo.split(X, y, groups=sessions))):\n",
    "    print(f\"{'#'*30}\\nSESSION {i}\\n{'#'*30}\")\n",
    "    ## create model ##\n",
    "    model = LogisticRegressionTorch(X.shape[-1])\n",
    "    loss_fn = torch.nn.BCELoss()\n",
    "    alpha = 1\n",
    "    lr = 5e-2\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=10)\n",
    "    ## data ##\n",
    "    train_set = SimpleDataset(X[train_idx], y[train_idx])\n",
    "    test_set = SimpleDataset(X[test_idx], y[test_idx])\n",
    "    ## class balancing ##\n",
    "    cls_weights = compute_class_weight(\n",
    "        class_weight=\"balanced\",\n",
    "        classes=np.unique(train_set.y.detach().numpy()),\n",
    "        y=train_set.y.detach().numpy(),\n",
    "    )\n",
    "    weights = cls_weights[train_set.y.detach().numpy().astype(int)]\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights, len(train_set.y.detach().numpy()), replacement=True\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(train_set, batch_size=200, sampler=sampler)\n",
    "\n",
    "    ## training epochs ##\n",
    "    EPOCHS = 10\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer=optimizer, milestones=[2, 4, 6, 8])\n",
    "    for t in range(EPOCHS):\n",
    "        print(f\"{'-'*30}\\nEpoch {t+1}\\n{'-'*30}\")\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, print_nth_batch=8)\n",
    "        out = test_auc_score(test_set, model)\n",
    "        if t in scheduler.milestones:\n",
    "            scheduler.step()\n",
    "    torch_auc_list.append(out)\n",
    "\n",
    "    ### STANDARD SKLEARN LOGISTIC REGRESSION ###\n",
    "    # lr_sk = LogisticRegression(\n",
    "    #     # penalty=\"none\", class_weight=\"balanced\", fit_intercept=True, max_iter=1000\n",
    "    #     C=1e-4, class_weight=\"balanced\", fit_intercept=True, max_iter=1000, solver='saga'\n",
    "    # )\n",
    "    # lr_sk.fit(X=X_npy[train_idx], y=y_npy[train_idx])\n",
    "    # pred = lr_sk.predict_proba(X_npy[test_idx])[:, 1]\n",
    "    # score = roc_auc_score(y_true=y_npy[test_idx], y_score=pred)\n",
    "    # sk_auc_list.append(score)\n",
    "    # print(\"sklearn AUC:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ff25666c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pytorch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.625371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.587692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.529521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.597314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.524735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.558055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.506475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.602643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.571909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.551749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.586809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.608647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.597783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.605986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.563765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.633178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.552095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.657864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.572698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.591898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.580798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.658078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.615106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pytorch\n",
       "0   0.625371\n",
       "1   0.587692\n",
       "2   0.529521\n",
       "3   0.597314\n",
       "4   0.524735\n",
       "5   0.558055\n",
       "6   0.506475\n",
       "7   0.602643\n",
       "8   0.571909\n",
       "9   0.551749\n",
       "10  0.586809\n",
       "11  0.608647\n",
       "12  0.597783\n",
       "13  0.605986\n",
       "14  0.563765\n",
       "15  0.633178\n",
       "16  0.552095\n",
       "17  0.657864\n",
       "18  0.572698\n",
       "19  0.591898\n",
       "20  0.580798\n",
       "21  0.658078\n",
       "22  0.615106"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.DataFrame(data={\"sklearn\":sk_auc_list, \"pytorch\":torch_auc_list})\n",
    "df = pd.DataFrame(data={\"pytorch\":torch_auc_list})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b35b7872",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"LTP093_loso_sgd_l2_unit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ac107a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x29208d330>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['sklearn'], bins=10, label='sklearn', alpha=.5)\n",
    "plt.hist(df['pytorch'], bins=10, label='pytorch', alpha=.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e75ed9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
